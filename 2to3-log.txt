WARNING: --write-unchanged-files/-W implies -w.
lib2to3.main: Output in 'planet.archlinux.org3' will mirror the input directory 'planet.archlinux.org' layout.
RefactoringTool: Skipping optional fixer: buffer
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: set_literal
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: Refactored planet.archlinux.org/admin_cb.py
RefactoringTool: Writing converted planet.archlinux.org/admin_cb.py to planet.archlinux.org3/admin_cb.py.
RefactoringTool: Refactored planet.archlinux.org/expunge.py
RefactoringTool: Writing converted planet.archlinux.org/expunge.py to planet.archlinux.org3/expunge.py.
RefactoringTool: Refactored planet.archlinux.org/favicon.py
RefactoringTool: Writing converted planet.archlinux.org/favicon.py to planet.archlinux.org3/favicon.py.
RefactoringTool: Refactored planet.archlinux.org/planet.py
RefactoringTool: Writing converted planet.archlinux.org/planet.py to planet.archlinux.org3/planet.py.
RefactoringTool: Refactored planet.archlinux.org/publish.py
RefactoringTool: Writing converted planet.archlinux.org/publish.py to planet.archlinux.org3/publish.py.
RefactoringTool: Refactored planet.archlinux.org/runtests.py
--- planet.archlinux.org/admin_cb.py	(original)
+++ planet.archlinux.org/admin_cb.py	(refactored)
@@ -5,7 +5,7 @@
 import cgitb
 cgitb.enable()
 
-from urllib import unquote
+from urllib.parse import unquote
 import sys, os
 
 # Modify this to point to where you usually run planet.
@@ -43,13 +43,13 @@
 
 
 # Start HTML output at once
-print "Content-Type: text/html;charset=utf-8"     # HTML is following
-print                                             # blank line, end of headers
+print("Content-Type: text/html;charset=utf-8")     # HTML is following
+print()                                             # blank line, end of headers
 
 
-print '<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">'
-print '<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="sv"><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><title>Admin results</title></head><body>'
-print '<div>'
+print('<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">')
+print('<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="sv"><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><title>Admin results</title></head><body>')
+print('<div>')
 
 # Cache and blacklist dirs
 
@@ -58,7 +58,7 @@
 
 # Must have command parameter
 if not "command" in form:
-  print "<p>Unknown command</p>"
+  print("<p>Unknown command</p>")
 
 elif form['command'].value == "blacklist":
 
@@ -66,11 +66,11 @@
   # Create the blacklist dir if it does not exist
   if not os.path.exists(blacklist):
     os.mkdir(blacklist)
-    print "<p>Created directory %s</p>" % blacklist
+    print("<p>Created directory %s</p>" % blacklist)
   
   # find list of urls, in the form bl[n]=url
 
-  for key in form.keys():
+  for key in list(form.keys()):
 
     if not key.startswith("bl"): continue
 
@@ -85,17 +85,17 @@
 
       os.rename(cache_file, blacklist_file)
 
-      print "<p>Blacklisted <a href='%s'>%s</a></p>" % (url, url)
+      print("<p>Blacklisted <a href='%s'>%s</a></p>" % (url, url))
 
     else:
 
-      print "<p>Unknown file: %s</p>" % cache_file
+      print("<p>Unknown file: %s</p>" % cache_file)
 
-    print """
+    print("""
 <p>Note that blacklisting does not automatically 
 refresh the planet. You will need to either wait for
 a scheduled planet run, or refresh manually from the admin interface.</p>
-"""
+""")
 
 
 elif form['command'].value == "run":
@@ -105,9 +105,9 @@
   from planet import spider, splice
   try:
      spider.spiderPlanet(only_if_new=False)
-     print "<p>Successfully ran spider</p>"
-  except Exception, e:
-     print e
+     print("<p>Successfully ran spider</p>")
+  except Exception as e:
+     print(e)
 
   doc = splice.splice()
   splice.apply(doc.toxml('utf-8'))
@@ -121,7 +121,7 @@
   doc = splice.splice()
   splice.apply(doc.toxml('utf-8'))
 
-  print "<p>Successfully refreshed</p>"
+  print("<p>Successfully refreshed</p>")
 
 elif form['command'].value == "expunge":
 
@@ -129,13 +129,13 @@
   from planet import expunge
   expunge.expungeCache()
 
-  print "<p>Successfully expunged</p>"
+  print("<p>Successfully expunged</p>")
 
 
 
 
-print "<p><strong><a href='" + ADMIN_URL + "'>Return</a> to admin interface</strong></p>"
+print("<p><strong><a href='" + ADMIN_URL + "'>Return</a> to admin interface</strong></p>")
 
 
 
-print "</body></html>"
+print("</body></html>")
--- planet.archlinux.org/expunge.py	(original)
+++ planet.archlinux.org/expunge.py	(refactored)
@@ -13,5 +13,5 @@
         config.load(sys.argv[1])
         expunge.expungeCache()
     else:
-        print "Usage:"
-        print "  python %s config.ini" % sys.argv[0]
+        print("Usage:")
+        print("  python %s config.ini" % sys.argv[0])
--- planet.archlinux.org/favicon.py	(original)
+++ planet.archlinux.org/favicon.py	(refactored)
@@ -1,10 +1,10 @@
 import sys, socket
 from planet import config, feedparser
 from planet.spider import filename
-from urllib2 import urlopen
-from urlparse import urljoin
+from urllib.request import urlopen
+from urllib.parse import urljoin
 from html5lib import html5parser, treebuilders
-from ConfigParser import ConfigParser
+from configparser import ConfigParser
 
 # load config files (default: config.ini)
 for arg in sys.argv[1:]:
@@ -12,7 +12,7 @@
 if len(sys.argv) == 1:
   config.load('config.ini')
 
-from Queue import Queue
+from queue import Queue
 from threading import Thread
 
 # determine which subscriptions have no icon but do have a html page
@@ -73,7 +73,7 @@
 
 # produce config file
 config = ConfigParser()
-for sub, icon in icons.items():
+for sub, icon in list(icons.items()):
   config.add_section(sub)
   config.set(sub, 'favicon', icon)
 config.write(sys.stdout)
--- planet.archlinux.org/planet.py	(original)
+++ planet.archlinux.org/planet.py	(refactored)
@@ -27,16 +27,16 @@
 
     for arg in sys.argv[1:]:
         if arg == "-h" or arg == "--help":
-            print "Usage: planet [options] [CONFIGFILE]"
-            print
-            print "Options:"
-            print " -v, --verbose       DEBUG level logging during update"
-            print " -o, --offline       Update the Planet from the cache only"
-            print " -h, --help          Display this help message and exit"
-            print " -n, --only-if-new   Only spider new feeds"
-            print " -x, --expunge       Expunge old entries from cache"
-            print " --no-publish        Do not publish feeds using PubSubHubbub"
-            print
+            print("Usage: planet [options] [CONFIGFILE]")
+            print()
+            print("Options:")
+            print(" -v, --verbose       DEBUG level logging during update")
+            print(" -o, --offline       Update the Planet from the cache only")
+            print(" -h, --help          Display this help message and exit")
+            print(" -n, --only-if-new   Only spider new feeds")
+            print(" -x, --expunge       Expunge old entries from cache")
+            print(" --no-publish        Do not publish feeds using PubSubHubbub")
+            print()
             sys.exit(0)
         elif arg == "-v" or arg == "--verbose":
             verbose = 1
@@ -51,7 +51,7 @@
         elif arg == "--no-publish":
             no_publish = 1
         elif arg.startswith("-"):
-            print >>sys.stderr, "Unknown option:", arg
+            print("Unknown option:", arg, file=sys.stderr)
             sys.exit(1)
         else:
             config_file.append(arg)
@@ -67,8 +67,8 @@
         from planet import spider
         try:
             spider.spiderPlanet(only_if_new=only_if_new)
-        except Exception, e:
-            print e
+        except Exception as e:
+            print(e)
 
     from planet import splice
     doc = splice.splice()
@@ -79,7 +79,7 @@
         debug=open('debug.atom','w')
         try:
             from lxml import etree
-            from StringIO import StringIO
+            from io import StringIO
             tree = etree.tostring(etree.parse(StringIO(doc.toxml())))
             debug.write(etree.tostring(tree, pretty_print=True))
         except:
--- planet.archlinux.org/publish.py	(original)
+++ planet.archlinux.org/publish.py	(refactored)
@@ -13,5 +13,5 @@
         config.load(sys.argv[1])
         publish.publish(config)
     else:
-        print "Usage:"
-        print "  python %s config.ini" % sys.argv[0]
+        print("Usage:")
+        print("  python %s config.ini" % sys.argv[0])
--- planet.archlinux.org/runtests.py	(original)
+++ planet.archlinux.org/runtests.py	(refactored)
@@ -10,9 +10,9 @@
 
 # more python 2.2 accomodations
 if not hasattr(unittest.TestCase, 'assertTrue'):
-    unittest.TestCase.assertTrue = unittest.TestCase.assert_
+    unittest.TestCase.assertTrue = unittest.TestCase.assertTrue
 if not hasattr(unittest.TestCase, 'assertFalse'):
-    unittest.TestCase.assertFalse = unittest.TestCase.failIf
+    unittest.TestCase.assertFalse = unittest.TestCase.assertFalse
 
 # try to start in a consistent, predictable location
 if sys.path[0]: os.chdir(sys.path[0])
@@ -28,7 +28,7 @@
 # find all of the planet test modulesRefactoringTool: Writing converted planet.archlinux.org/runtests.py to planet.archlinux.org3/runtests.py.
RefactoringTool: Refactored planet.archlinux.org/spider.py
RefactoringTool: Writing converted planet.archlinux.org/spider.py to planet.archlinux.org3/spider.py.
RefactoringTool: Refactored planet.archlinux.org/splice.py
RefactoringTool: Writing converted planet.archlinux.org/splice.py to planet.archlinux.org3/splice.py.
RefactoringTool: Refactored planet.archlinux.org/examples/filters/guess-language/guess-language.py
RefactoringTool: Writing converted planet.archlinux.org/examples/filters/guess-language/guess-language.py to planet.archlinux.org3/examples/filters/guess-language/guess-language.py.
RefactoringTool: Refactored planet.archlinux.org/examples/filters/guess-language/learn-language.py
RefactoringTool: Writing converted planet.archlinux.org/examples/filters/guess-language/learn-language.py to planet.archlinux.org3/examples/filters/guess-language/learn-language.py.
RefactoringTool: Refactored planet.archlinux.org/examples/filters/guess-language/trigram.py
RefactoringTool: Writing converted planet.archlinux.org/examples/filters/guess-language/trigram.py to planet.archlinux.org3/examples/filters/guess-language/trigram.py.
RefactoringTool: Refactored planet.archlinux.org/filters/coral_cdn_filter.py
RefactoringTool: Writing converted planet.archlinux.org/filters/coral_cdn_filter.py to planet.archlinux.org3/filters/coral_cdn_filter.py.
RefactoringTool: Refactored planet.archlinux.org/filters/excerpt.py
RefactoringTool: Writing converted planet.archlinux.org/filters/excerpt.py to planet.archlinux.org3/filters/excerpt.py.
RefactoringTool: Refactored planet.archlinux.org/filters/minhead.py

 modules = []
 for pattern in sys.argv[1:] or ['test_*.py']:
-    modules += map(fullmodname, glob.glob(os.path.join('tests', pattern)))
+    modules += list(map(fullmodname, glob.glob(os.path.join('tests', pattern))))
 
 # enable logging
 import planet
@@ -39,7 +39,7 @@
 # load all of the tests into a suite
 try:
     suite = unittest.TestLoader().loadTestsFromNames(modules)
-except Exception, exception:
+except Exception as exception:
     # attempt to produce a more specific message
     for module in modules: __import__(module)
     raise
--- planet.archlinux.org/spider.py	(original)
+++ planet.archlinux.org/spider.py	(refactored)
@@ -18,5 +18,5 @@
         for feed in sys.argv[2:]:
             spider.spiderFeed(feed)
     else:
-        print "Usage:"
-        print "  python %s config.ini [URI URI ...]" % sys.argv[0]
+        print("Usage:")
+        print("  python %s config.ini [URI URI ...]" % sys.argv[0])
--- planet.archlinux.org/splice.py	(original)
+++ planet.archlinux.org/splice.py	(refactored)
@@ -14,5 +14,5 @@
         doc = splice.splice()
         splice.apply(doc.toxml('utf-8'))
     else:
-        print "Usage:"
-        print "  python %s config.ini" % sys.argv[0]
+        print("Usage:")
+        print("  python %s config.ini" % sys.argv[0])
--- planet.archlinux.org/examples/filters/guess-language/guess-language.py	(original)
+++ planet.archlinux.org/examples/filters/guess-language/guess-language.py	(refactored)
@@ -18,39 +18,39 @@
 from sys import stdin, stdout
 from trigram import Trigram
 from xml.dom import XML_NAMESPACE as XML_NS
-import cPickle
+import pickle
 
 ATOM_NSS = {
-    u'atom': u'http://www.w3.org/2005/Atom',
-    u'xml': XML_NS
+    'atom': 'http://www.w3.org/2005/Atom',
+    'xml': XML_NS
 }
 
 langs = {}
 
 def tri(lang):
-    if not langs.has_key(lang):
+    if lang not in langs:
 	f = open('filters/guess-language/%s.data' % lang, 'r')
-	t = cPickle.load(f)
+	t = pickle.load(f)
 	f.close()
 	langs[lang] = t
     return langs[lang]
     
 
 def guess_language(entry):
-    text = u'';
-    for child in entry.xml_xpath(u'atom:title|atom:summary|atom:content'):
-	text = text + u' '+ child.__unicode__()
+    text = '';
+    for child in entry.xml_xpath('atom:title|atom:summary|atom:content'):
+	text = text + ' '+ child.__unicode__()
     t = Trigram()
     t.parseString(text)
     if tri('fr') - t > tri('en') - t:
-	lang=u'en'
+	lang='en'
     else:
-	lang=u'fr'
-    entry.xml_set_attribute((u'xml:lang', XML_NS), lang)
+	lang='fr'
+    entry.xml_set_attribute(('xml:lang', XML_NS), lang)
 
 def main():
     feed = amara.parse(stdin, prefixes=ATOM_NSS)
-    for entry in feed.xml_xpath(u'//atom:entry[not(@xml:lang)]'):
+    for entry in feed.xml_xpath('//atom:entry[not(@xml:lang)]'):
 	guess_language(entry)
     feed.xml(stdout)
 
--- planet.archlinux.org/examples/filters/guess-language/learn-language.py	(original)
+++ planet.archlinux.org/examples/filters/guess-language/learn-language.py	(refactored)
@@ -12,7 +12,7 @@
 
 from trigram import Trigram
 from sys import argv
-from cPickle import dump
+from pickle import dump
 
 
 def main():
--- planet.archlinux.org/examples/filters/guess-language/trigram.py	(original)
+++ planet.archlinux.org/examples/filters/guess-language/trigram.py	(refactored)
@@ -12,7 +12,7 @@
 __license__ = "Python"
 
 import random
-from urllib import urlopen
+from urllib.request import urlopen
 
 class Trigram:
     """
@@ -98,8 +98,8 @@
         """calculates the scalar length of the trigram vector and
         stores it in self.length."""
         total = 0
-        for y in self.lut.values():
-            total += sum([ x * x for x in y.values() ])
+        for y in list(self.lut.values()):
+            total += sum([ x * x for x in list(y.values()) ])
         self.length = total ** 0.5
 
     def similarity(self, other):
@@ -112,7 +112,7 @@
         lut1 = self.lut
         lut2 = other.lut
         total = 0
-        for k in lut1.keys():
+        for k in list(lut1.keys()):
             if k in lut2:
                 a = lut1[k]
                 b = lut2[k]
@@ -148,7 +148,7 @@
             return ' '
         # if you were using this a lot, caching would a good idea.
         letters = []
-        for k, v in self.lut[k].items():
+        for k, v in list(self.lut[k].items()):
             letters.append(k * v)
         letters = ''.join(letters)
         return random.choice(letters)
@@ -165,23 +165,23 @@
     no2 = Trigram('http://gutenberg.net/dirs/1/3/0/4/13041/13041-8.txt')
     en2 = Trigram('http://gutenberg.net/dirs/etext05/cfgsh10.txt')
     fr2 = Trigram('http://gutenberg.net/dirs/1/3/7/0/13704/13704-8.txt')
-    print "calculating difference:"
-    print "en - fr is %s" % (en - fr)
-    print "fr - en is %s" % (fr - en)
-    print "en - en2 is %s" % (en - en2)
-    print "en - fr2 is %s" % (en - fr2)
-    print "fr - en2 is %s" % (fr - en2)
-    print "fr - fr2 is %s" % (fr - fr2)
-    print "fr2 - en2 is %s" % (fr2 - en2)
-    print "fi - fr  is %s" % (fi - fr)
-    print "fi - en  is %s" % (fi - en)
-    print "fi - se  is %s" % (fi - se)
-    print "no - se  is %s" % (no - se)
-    print "en - no  is %s" % (en - no)
-    print "no - no2  is %s" % (no - no2)
-    print "se - no2  is %s" % (se - no2)
-    print "en - no2  is %s" % (en - no2)
-    print "fr - no2  is %s" % (fr - no2)
+    print("calculating difference:")
+    print("en - fr is %s" % (en - fr))
+    print("fr - en is %s" % (fr - en))
+    print("en - en2 is %s" % (en - en2))
+    print("en - fr2 is %s" % (en - fr2))
+    print("fr - en2 is %s" % (fr - en2))
+    print("fr - fr2 is %s" % (fr - fr2))
+    print("fr2 - en2 is %s" % (fr2 - en2))
+    print("fi - fr  is %s" % (fi - fr))
+    print("fi - en  is %s" % (fi - en))
+    print("fi - se  is %s" % (fi - se))
+    print("no - se  is %s" % (no - se))
+    print("en - no  is %s" % (en - no))
+    print("no - no2  is %s" % (no - no2))
+    print("se - no2  is %s" % (se - no2))
+    print("en - no2  is %s" % (en - no2))
+    print("fr - no2  is %s" % (fr - no2))
 
 
 if __name__ == '__main__':
--- planet.archlinux.org/filters/coral_cdn_filter.py	(original)
+++ planet.archlinux.org/filters/coral_cdn_filter.py	(refactored)
@@ -3,16 +3,16 @@
 Network <http://www.coralcdn.org/>.
 """
 
-import re, sys, urlparse, xml.dom.minidom
+import re, sys, urllib.parse, xml.dom.minidom
 
 entry = xml.dom.minidom.parse(sys.stdin).documentElement
 
 for node in entry.getElementsByTagName('img'):
     if node.hasAttribute('src'):
-        component = list(urlparse.urlparse(node.getAttribute('src')))
+        component = list(urllib.parse.urlparse(node.getAttribute('src')))
         if component[0] == 'http':
             component[1] = re.sub(r':(\d+)$', r'.\1', component[1])
             component[1] += '.nyud.net:8080'
-            node.setAttribute('src', urlparse.urlunparse(component))
+            node.setAttribute('src', urllib.parse.urlunparse(component))
 
-print entry.toxml('utf-8')
+print(entry.toxml('utf-8'))
--- planet.archlinux.org/filters/excerpt.py	(original)
+++ planet.archlinux.org/filters/excerpt.py	(refactored)
@@ -20,7 +20,7 @@
 atomNS = 'http://www.w3.org/2005/Atom'
 planetNS = 'http://planet.intertwingly.net/'
 
-args = dict(zip([name.lstrip('-') for name in sys.argv[1::2]], sys.argv[2::2]))
+args = dict(list(zip([name.lstrip('-') for name in sys.argv[1::2]], sys.argv[2::2])))
 
 wrapper = textwrap.TextWrapper(width=int(args.get('width','500')))
 omit = args.get('omit', '').split()
@@ -84,7 +84,7 @@
             target.appendChild(self.dom.createTextNode(source))
             self.textlen = len(lines[0])
         elif lines:
-            excerpt = source[:len(lines[0])-self.textlen] + u' \u2026'
+            excerpt = source[:len(lines[0])-self.textlen] + ' \u2026'
             target.appendChild(dom.createTextNode(excerpt))
             self.full = True
 
@@ -106,4 +106,4 @@
         source[0].parentNode.removeChild(source[0])
 
 # print out results
-print dom.toxml('utf-8')
+print(dom.toxml('utf-8'))
--- planet.archlinux.org/filters/minhead.py	(original)
+++ planet.archlinux.org/filters/minhead.py	(refactored)
@@ -33,4 +33,4 @@
       oldhead.parentNode.replaceChild(newhead, oldhead)RefactoringTool: Writing converted planet.archlinux.org/filters/minhead.py to planet.archlinux.org3/filters/minhead.py.
RefactoringTool: No changes to planet.archlinux.org/filters/notweets.py
RefactoringTool: Writing converted planet.archlinux.org/filters/notweets.py to planet.archlinux.org3/filters/notweets.py.
RefactoringTool: Refactored planet.archlinux.org/filters/regexp_sifter.py
RefactoringTool: Writing converted planet.archlinux.org/filters/regexp_sifter.py to planet.archlinux.org3/filters/regexp_sifter.py.
RefactoringTool: Refactored planet.archlinux.org/filters/xpath_sifter.py
RefactoringTool: Writing converted planet.archlinux.org/filters/xpath_sifter.py to planet.archlinux.org3/filters/xpath_sifter.py.
RefactoringTool: Refactored planet.archlinux.org/planet/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/__init__.py to planet.archlinux.org3/planet/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/config.py
RefactoringTool: Writing converted planet.archlinux.org/planet/config.py to planet.archlinux.org3/planet/config.py.
RefactoringTool: Refactored planet.archlinux.org/planet/csv_config.py

 
 # return (possibly modified) document
-print doc.toxml('utf-8')
+print(doc.toxml('utf-8'))
--- planet.archlinux.org/filters/regexp_sifter.py	(original)
+++ planet.archlinux.org/filters/regexp_sifter.py	(refactored)
@@ -1,7 +1,7 @@
 import sys, re
 
 # parse options
-options = dict(zip(sys.argv[1::2],sys.argv[2::2]))
+options = dict(list(zip(sys.argv[1::2],sys.argv[2::2])))
 
 # read entry
 doc = data = sys.stdin.read()
@@ -31,14 +31,14 @@
   data=pattern.sub(replacement,data)
 
 # process requirements
-if options.has_key('--require'):
+if '--require' in options:
   for regexp in options['--require'].split('\n'):
      if regexp and not re.search(regexp,data): sys.exit(1)
 
 # process exclusions
-if options.has_key('--exclude'):
+if '--exclude' in options:
   for regexp in options['--exclude'].split('\n'):
      if regexp and re.search(regexp,data): sys.exit(1)
 
 # if we get this far, the feed is to be included
-print doc
+print(doc)
--- planet.archlinux.org/filters/xpath_sifter.py	(original)
+++ planet.archlinux.org/filters/xpath_sifter.py	(refactored)
@@ -1,7 +1,7 @@
 import sys, libxml2
 
 # parse options
-options = dict(zip(sys.argv[1::2],sys.argv[2::2]))
+options = dict(list(zip(sys.argv[1::2],sys.argv[2::2])))
 
 # parse entry
 doc = libxml2.parseDoc(sys.stdin.read())
@@ -10,14 +10,14 @@
 ctxt.xpathRegisterNs('xhtml','http://www.w3.org/1999/xhtml')
 
 # process requirements
-if options.has_key('--require'):
+if '--require' in options:
   for xpath in options['--require'].split('\n'):
      if xpath and not ctxt.xpathEval(xpath): sys.exit(1)
 
 # process exclusions
-if options.has_key('--exclude'):
+if '--exclude' in options:
   for xpath in options['--exclude'].split('\n'):
      if xpath and ctxt.xpathEval(xpath): sys.exit(1)
 
 # if we get this far, the feed is to be included
-print doc
+print(doc)
--- planet.archlinux.org/planet/__init__.py	(original)
+++ planet.archlinux.org/planet/__init__.py	(refactored)
@@ -4,11 +4,11 @@
 loggerParms = None
 
 import os, sys, re
-import config
+from . import config
 config.__init__()
 
-from ConfigParser import ConfigParser
-from urlparse import urljoin
+from configparser import ConfigParser
+from urllib.parse import urljoin
 
 def getLogger(level, format):
     """ get a logger with the specified log level """
@@ -39,4 +39,4 @@
 feedparser.SANITIZE_HTML=1
 feedparser.RESOLVE_RELATIVE_URIS=0
 
-import publish
+from . import publish
--- planet.archlinux.org/planet/config.py	(original)
+++ planet.archlinux.org/planet/config.py	(refactored)
@@ -26,9 +26,9 @@
   * error handling (example: no planet section)
 """
 
-import os, sys, re, urllib
-from ConfigParser import ConfigParser
-from urlparse import urljoin
+import os, sys, re, urllib.request, urllib.parse, urllib.error
+from configparser import ConfigParser
+from urllib.parse import urljoin
 
 parser = ConfigParser()
 
@@ -36,7 +36,7 @@
 
 def __init__():
     """define the struture of an ini file"""
-    import config
+    from . import config
 
     # get an option from a section
     def get(section, option, default):
@@ -209,8 +209,8 @@
                 cached_config.readfp(data)
             else:
                 from planet import shell
-                import StringIO
-                cached_config.readfp(StringIO.StringIO(shell.run(
+                import io
+                cached_config.readfp(io.StringIO(shell.run(
                     content_type(list), data.getvalue(), mode="filter")))
 
             if cached_config.sections() in [[], [list]]: 
@@ -221,10 +221,10 @@
 
 def downloadReadingList(list, orig_config, callback, use_cache=True, re_read=True):
     from planet import logger
-    import config
+    from . import config
     try:
 
-        import urllib2, StringIO
+        import urllib.request, urllib.error, urllib.parse, io
         from planet.spider import filename
 
         # list cache file name
@@ -248,7 +248,7 @@
 
         cached_config = ConfigParser()
         cached_config.add_section(list)
-        for key, value in options.items():
+        for key, value in list(options.items()):
             cached_config.set(list, key, value)
 
         # read list
@@ -259,21 +259,21 @@
             path = os.path.abspath(os.path.curdir)
             base = urljoin('file:///', path.replace(':','|').replace('\\','/'))
 
-        request = urllib2.Request(urljoin(base + '/', list))
-        if options.has_key("etag"):
+        request = urllib.request.Request(urljoin(base + '/', list))
+        if "etag" in options:
             request.add_header('If-None-Match', options['etag'])
-        if options.has_key("last-modified"):
+        if "last-modified" in options:
             request.add_header('If-Modified-Since',
                 options['last-modified'])
-        response = urllib2.urlopen(request)
-        if response.headers.has_key('etag'):
+        response = urllib.request.urlopen(request)
+        if 'etag' in response.headers:
             cached_config.set(list, 'etag', response.headers['etag'])
-        if response.headers.has_key('last-modified'):
+        if 'last-modified' in response.headers:
             cached_config.set(list, 'last-modified',
                 response.headers['last-modified'])
 
         # convert to config.ini
-        data = StringIO.StringIO(response.read())
+        data = io.StringIO(response.read())
 
         if callback: callback(data, cached_config)
 
@@ -289,7 +289,7 @@
             if use_cache:  
                 orig_config.read(cache_filename)
             else:
-                cdata = StringIO.StringIO()
+                cdata = io.StringIO()
                 cached_config.write(cdata)
                 cdata.seek(0)
                 orig_config.readfp(cdata)
@@ -299,7 +299,7 @@
                 if use_cache:  
                     if not orig_config.read(cache_filename): raise Exception()
                 else:
-                    cdata = StringIO.StringIO()
+                    cdata = io.StringIO()
                     cached_config.write(cdata)
                     cdata.seek(0)
                     orig_config.readfp(cdata)
@@ -373,12 +373,12 @@
     filters = []
     if parser.has_option('Planet', 'filters'):
         filters += parser.get('Planet', 'filters').split()
-    if filter(section):
+    if list(filter(section)):
         filters.append('regexp_sifter.py?require=' +
-            urllib.quote(filter(section)))
+            urllib.parse.quote(list(filter(section))))
     if exclude(section):
         filters.append('regexp_sifter.py?exclude=' +
-            urllib.quote(exclude(section)))
+            urllib.parse.quote(exclude(section)))
     for section in section and [section] or template_files():
         if parser.has_option(section, 'filters'):
             filters += parser.get(section, 'filters').split()
@@ -386,18 +386,16 @@
 
 def planet_options():
     """ dictionary of planet wide options"""
-    return dict(map(lambda opt: (opt,
-        parser.get('Planet', opt, raw=(opt=="log_format"))),
-        parser.options('Planet')))
+    return dict([(opt,
+        parser.get('Planet', opt, raw=(opt=="log_format"))) for opt in parser.options('Planet')])
 
 def feed_options(section):
     """ dictionary of feed specific options"""
-    import config
-    options = dict([(key,value) for key,value in planet_options().items()
+    from . import config
+    options = dict([(key,value) for key,value in list(planet_options().items())
         if key not in planet_predefined_options])
     if parser.has_section(section):
-        options.update(dict(map(lambda opt: (opt, parser.get(section,opt)),
-            parser.options(section))))
+        options.update(dict([(opt, parser.get(section,opt)) for opt in parser.options(section)]))
     return options
 
 def template_options(section):
@@ -410,4 +408,4 @@
 
 def write(file=sys.stdout):
     """ write out an updated template """
-    print parser.write(file)
+    print(parser.write(file))
--- planet.archlinux.org/planet/csv_config.py	(original)
+++ planet.archlinux.org/planet/csv_config.py	(refactored)
@@ -1,4 +1,4 @@
-from ConfigParser import ConfigParser
+from configparser import ConfigParser
 import csvRefactoringTool: Writing converted planet.archlinux.org/planet/csv_config.py to planet.archlinux.org3/planet/csv_config.py.
RefactoringTool: Refactored planet.archlinux.org/planet/expunge.py
RefactoringTool: Writing converted planet.archlinux.org/planet/expunge.py to planet.archlinux.org3/planet/expunge.py.
RefactoringTool: Refactored planet.archlinux.org/planet/foaf.py
RefactoringTool: Writing converted planet.archlinux.org/planet/foaf.py to planet.archlinux.org3/planet/foaf.py.
RefactoringTool: Refactored planet.archlinux.org/planet/idindex.py
RefactoringTool: Writing converted planet.archlinux.org/planet/idindex.py to planet.archlinux.org3/planet/idindex.py.
RefactoringTool: Refactored planet.archlinux.org/planet/opml.py

 
 # input = csv, output = ConfigParser
@@ -15,7 +15,7 @@
         section = row[reader.fieldnames[0]]
         if not config.has_section(section):
             config.add_section(section)
-        for name, value in row.items():
+        for name, value in list(row.items()):
             if value and name != reader.fieldnames[0]:
                 config.set(section, name, value) 
 
@@ -23,8 +23,8 @@
 
 if __name__ == "__main__":
     # small main program which converts CSV into config.ini format
-    import sys, urllib
+    import sys, urllib.request, urllib.parse, urllib.error
     config = ConfigParser()
     for input in sys.argv[1:]:
-        csv2config(urllib.urlopen(input), config)
+        csv2config(urllib.request.urlopen(input), config)
     config.write(sys.stdout)
--- planet.archlinux.org/planet/expunge.py	(original)
+++ planet.archlinux.org/planet/expunge.py	(refactored)
@@ -1,7 +1,7 @@
 """ Expunge old entries from a cache of entries """
 import glob, os, planet, config, feedparser
 from xml.dom import minidom
-from spider import filename
+from .spider import filename
 
 def expungeCache():
     """ Expunge old entries from a cache of entries """
@@ -12,8 +12,8 @@
     sources = config.cache_sources_directory()
     for sub in config.subscriptions():
         data=feedparser.parse(filename(sources,sub))
-        if not data.feed.has_key('id'): continue
-        if config.feed_options(sub).has_key('cache_keep_entries'):
+        if 'id' not in data.feed: continue
+        if 'cache_keep_entries' in config.feed_options(sub):
             entry_count[data.feed.id] = int(config.feed_options(sub)['cache_keep_entries'])
         else:
             entry_count[data.feed.id] = config.cache_keep_entries()
--- planet.archlinux.org/planet/foaf.py	(original)
+++ planet.archlinux.org/planet/foaf.py	(refactored)
@@ -1,4 +1,4 @@
-from ConfigParser import ConfigParser
+from configparser import ConfigParser
 
 inheritable_options = [ 'online_accounts' ]
 
@@ -86,14 +86,14 @@
                 config.set(feed, 'name', str(title))
 
         # now look for OnlineAccounts for the same person
-        if accounts.keys():
+        if list(accounts.keys()):
             for statement in model.find_statements(Statement(person,foaf.holdsAccount,None)):
                 rdfaccthome = model.get_target(statement.object,foaf.accountServiceHomepage)
                 rdfacctname = model.get_target(statement.object,foaf.accountName)
 
                 if not rdfaccthome or not rdfacctname: continue
 
-                if not rdfaccthome.is_resource() or not accounts.has_key(str(rdfaccthome.uri)): continue
+                if not rdfaccthome.is_resource() or str(rdfaccthome.uri) not in accounts: continue
 
                 if not rdfacctname.is_literal(): continue
 
@@ -145,10 +145,10 @@
 def copy_options(config, parent_section, child_section, overrides = {}):
     global inheritable_options
     for option in [x for x in config.options(parent_section) if x in inheritable_options]:
-        if not overrides.has_key(option):
+        if option not in overrides:
             config.set(child_section, option, config.get(parent_section, option))
 
-    for option, value in overrides.items():
+    for option, value in list(overrides.items()):
         config.set(child_section, option, value)
 
 
@@ -186,12 +186,12 @@
                 return
 
 if __name__ == "__main__":
-    import sys, urllib
+    import sys, urllib.request, urllib.parse, urllib.error
     config = ConfigParser()
 
     for uri in sys.argv[1:]:
         config.add_section(uri)
-        foaf2config(urllib.urlopen(uri), config, section=uri)
+        foaf2config(urllib.request.urlopen(uri), config, section=uri)
         config.remove_section(uri)
 
     config.write(sys.stdout)
--- planet.archlinux.org/planet/idindex.py	(original)
+++ planet.archlinux.org/planet/idindex.py	(refactored)
@@ -13,9 +13,9 @@
         cache = config.cache_directory()
         index=os.path.join(cache,'index')
         if not os.path.exists(index): return None
-        import dbhash
-        return dbhash.open(filename(index, 'id'),'w')
-    except Exception, e:
+        import dbm.bsd
+        return dbm.bsd.open(filename(index, 'id'),'w')
+    except Exception as e:
         if e.__class__.__name__ == 'DBError': e = e.args[-1]
         from planet import logger as log
         log.error(str(e))
@@ -35,8 +35,8 @@
     cache = config.cache_directory()
     index=os.path.join(cache,'index')
     if not os.path.exists(index): os.makedirs(index)
-    import dbhash
-    index = dbhash.open(filename(index, 'id'),'c')
+    import dbm.bsd
+    index = dbm.bsd.open(filename(index, 'id'),'c')
 
     try:
         import libxml2
@@ -73,14 +73,14 @@
             except:
                 log.error(file)
 
-    log.info(str(len(index.keys())) + " entries indexed")
+    log.info(str(len(list(index.keys()))) + " entries indexed")
     index.close()
 
     return open()
 
 if __name__ == '__main__':
     if len(sys.argv) < 2:
-        print 'Usage: %s [-c|-d]' % sys.argv[0]
+        print('Usage: %s [-c|-d]' % sys.argv[0])
         sys.exit(1)
 
     config.load(sys.argv[1])
@@ -93,7 +93,7 @@
         from planet import logger as log
         index = open()
         if index:
-            log.info(str(len(index.keys())) + " entries indexed")
+            log.info(str(len(list(index.keys()))) + " entries indexed")
             index.close()
         else:
             log.info("no entries indexed")
--- planet.archlinux.org/planet/opml.py	(original)
+++ planet.archlinux.org/planet/opml.py	(refactored)
@@ -1,9 +1,9 @@
 from xml.sax import ContentHandler, make_parser, SAXParseException
 from xml.sax.xmlreader import InputSource
 from sgmllib import SGMLParser
-from cStringIO import StringIO
-from ConfigParser import ConfigParser
-from htmlentitydefs import entitydefs
+from io import StringIO
+from configparser import ConfigParser
+from html.entities import entitydefs
 import re
 
 # input = opml, output = ConfigParser
@@ -47,20 +47,20 @@
         # A type of 'rss' is meant to be used generically to indicate that
         # this is an entry in a subscription list, but some leave this
         # attribute off, and others have placed 'atom' in here
-        if attrs.has_key('type'):
-            if attrs['type'] == 'link' and not attrs.has_key('url'):
+        if 'type' in attrs:
+            if attrs['type'] == 'link' and 'url' not in attrs:
                 # Auto-correct WordPress link manager OPML files
-                attrs = dict(attrs.items())
+                attrs = dict(list(attrs.items()))
                 attrs['type'] = 'rss'
             if attrs['type'].lower() not in['rss','atom']: return
 
         # The feed itself is supposed to be in an attribute named 'xmlUrl'
         # (note the camel casing), but this has proven to be problematic,
         # with the most common misspelling being in all lower-case
-        if not attrs.has_key('xmlUrl') or not attrs['xmlUrl'].strip():
-            for attribute in attrs.keys():
+        if 'xmlUrl' not in attrs or not attrs['xmlUrl'].strip():
+            for attribute in list(attrs.keys()):
                 if attribute.lower() == 'xmlurl' and attrs[attribute].strip():
-                    attrs = dict(attrs.items())
+                    attrs = dict(list(attrs.items()))
                     attrs['xmlUrl'] = attrs[attribute]
                     break
             else:
@@ -68,9 +68,9 @@
 
         # the text attribute is nominally required in OPML, but this
         # data is often found in a title attribute instead
-        if not attrs.has_key('text') or not attrs['text'].strip():
-            if not attrs.has_key('title') or not attrs['title'].strip(): return
-            attrs = dict(attrs.items())
+        if 'text' not in attrs or not attrs['text'].strip():
+            if 'title' not in attrs or not attrs['title'].strip(): return
+            attrs = dict(list(attrs.items()))
             attrs['text'] = attrs['title']
 
         # if we get this far, we either have a valid subscription list entry,
@@ -86,23 +86,23 @@
 
         for i in range(1,len(parsed),2):
 
-            if parsed[i] in entitydefs.keys():RefactoringTool: Writing converted planet.archlinux.org/planet/opml.py to planet.archlinux.org3/planet/opml.py.
RefactoringTool: Refactored planet.archlinux.org/planet/publish.py
RefactoringTool: Writing converted planet.archlinux.org/planet/publish.py to planet.archlinux.org3/planet/publish.py.
RefactoringTool: Refactored planet.archlinux.org/planet/reconstitute.py

+            if parsed[i] in list(entitydefs.keys()):
                 # named entities
                 codepoint=entitydefs[parsed[i]]
                 match=self.entities.match(codepoint)
                 if match:
                     parsed[i]=match.group(1)
                 else:
-                    parsed[i]=unichr(ord(codepoint))
+                    parsed[i]=chr(ord(codepoint))
 
                 # numeric entities
                 if parsed[i].startswith('#'):
                     if parsed[i].startswith('#x'):
-                        parsed[i]=unichr(int(parsed[i][2:],16))
+                        parsed[i]=chr(int(parsed[i][2:],16))
                     else:
-                        parsed[i]=unichr(int(parsed[i][1:]))
+                        parsed[i]=chr(int(parsed[i][1:]))
 
-        return u''.join(parsed).encode('utf-8')
+        return ''.join(parsed).encode('utf-8')
     # SGML => SAX
     def unknown_starttag(self, name, attrs):
         attrs = dict(attrs)
@@ -111,44 +111,44 @@
                 attrs[attribute] = attrs[attribute].decode('utf-8')
             except:
                 work = attrs[attribute].decode('iso-8859-1')
-                work = u''.join([c in cp1252 and cp1252[c] or c for c in work])
+                work = ''.join([c in cp1252 and cp1252[c] or c for c in work])
                 attrs[attribute] = work
         self.startElement(name, attrs)
 
 # http://www.intertwingly.net/stories/2004/04/14/i18n.html#CleaningWindows
 cp1252 = {
-  unichr(128): unichr(8364), # euro sign
-  unichr(130): unichr(8218), # single low-9 quotation mark
-  unichr(131): unichr( 402), # latin small letter f with hook
-  unichr(132): unichr(8222), # double low-9 quotation mark
-  unichr(133): unichr(8230), # horizontal ellipsis
-  unichr(134): unichr(8224), # dagger
-  unichr(135): unichr(8225), # double dagger
-  unichr(136): unichr( 710), # modifier letter circumflex accent
-  unichr(137): unichr(8240), # per mille sign
-  unichr(138): unichr( 352), # latin capital letter s with caron
-  unichr(139): unichr(8249), # single left-pointing angle quotation mark
-  unichr(140): unichr( 338), # latin capital ligature oe
-  unichr(142): unichr( 381), # latin capital letter z with caron
-  unichr(145): unichr(8216), # left single quotation mark
-  unichr(146): unichr(8217), # right single quotation mark
-  unichr(147): unichr(8220), # left double quotation mark
-  unichr(148): unichr(8221), # right double quotation mark
-  unichr(149): unichr(8226), # bullet
-  unichr(150): unichr(8211), # en dash
-  unichr(151): unichr(8212), # em dash
-  unichr(152): unichr( 732), # small tilde
-  unichr(153): unichr(8482), # trade mark sign
-  unichr(154): unichr( 353), # latin small letter s with caron
-  unichr(155): unichr(8250), # single right-pointing angle quotation mark
-  unichr(156): unichr( 339), # latin small ligature oe
-  unichr(158): unichr( 382), # latin small letter z with caron
-  unichr(159): unichr( 376)} # latin capital letter y with diaeresis
+  chr(128): chr(8364), # euro sign
+  chr(130): chr(8218), # single low-9 quotation mark
+  chr(131): chr( 402), # latin small letter f with hook
+  chr(132): chr(8222), # double low-9 quotation mark
+  chr(133): chr(8230), # horizontal ellipsis
+  chr(134): chr(8224), # dagger
+  chr(135): chr(8225), # double dagger
+  chr(136): chr( 710), # modifier letter circumflex accent
+  chr(137): chr(8240), # per mille sign
+  chr(138): chr( 352), # latin capital letter s with caron
+  chr(139): chr(8249), # single left-pointing angle quotation mark
+  chr(140): chr( 338), # latin capital ligature oe
+  chr(142): chr( 381), # latin capital letter z with caron
+  chr(145): chr(8216), # left single quotation mark
+  chr(146): chr(8217), # right single quotation mark
+  chr(147): chr(8220), # left double quotation mark
+  chr(148): chr(8221), # right double quotation mark
+  chr(149): chr(8226), # bullet
+  chr(150): chr(8211), # en dash
+  chr(151): chr(8212), # em dash
+  chr(152): chr( 732), # small tilde
+  chr(153): chr(8482), # trade mark sign
+  chr(154): chr( 353), # latin small letter s with caron
+  chr(155): chr(8250), # single right-pointing angle quotation mark
+  chr(156): chr( 339), # latin small ligature oe
+  chr(158): chr( 382), # latin small letter z with caron
+  chr(159): chr( 376)} # latin capital letter y with diaeresis
 
 if __name__ == "__main__":
     # small main program which converts OPML into config.ini format
-    import sys, urllib
+    import sys, urllib.request, urllib.parse, urllib.error
     config = ConfigParser()
     for opml in sys.argv[1:]:
-        opml2config(urllib.urlopen(opml), config)
+        opml2config(urllib.request.urlopen(opml), config)
     config.write(sys.stdout)
--- planet.archlinux.org/planet/publish.py	(original)
+++ planet.archlinux.org/planet/publish.py	(refactored)
@@ -1,5 +1,5 @@
 import os, sys
-import urlparse
+import urllib.parse
 import planet
 import pubsubhubbub_publisher as PuSH
 
@@ -14,7 +14,7 @@
         for root, dirs, files in os.walk(config.output_dir()):
             for file in files:
                  if file in config.pubsubhubbub_feeds():
-                     feeds.append(urlparse.urljoin(link, file))
+                     feeds.append(urllib.parse.urljoin(link, file))
 
     # publish feeds
     if feeds:
@@ -22,5 +22,5 @@
             PuSH.publish(hub, feeds)
             for feed in feeds:
                 log.info("Published %s to %s\n" % (feed, hub))
-        except PuSH.PublishError, e:
+        except PuSH.PublishError as e:
             log.error("PubSubHubbub publishing error: %s\n" % e)
--- planet.archlinux.org/planet/reconstitute.py	(original)
+++ planet.archlinux.org/planet/reconstitute.py	(refactored)
@@ -44,16 +44,16 @@
 
 def invalidate(c): 
     """ replace invalid characters """
-    return u'<abbr title="U+%s">\ufffd</abbr>' % \
+    return '<abbr title="U+%s">\ufffd</abbr>' % \
         ('000' + hex(ord(c.group(0)))[2:])[-4:]
 
 def ncr2c(value):
     """ convert numeric character references to characters """
     value=value.group(1)
     if value.startswith('x'):
-        value=unichr(int(value[1:],16))
+        value=chr(int(value[1:],16))
     else:
-        value=unichr(int(value))
+        value=chr(int(value))
     return value
 
 nonalpha=re.compile('\W+',re.UNICODE)
@@ -68,18 +68,18 @@
 def id(xentry, entry):
     """ copy or compute an id for the entry """
 
-    if entry.has_key("id") and entry.id:
+    if "id" in entry and entry.id:
         entry_id = entry.id
-        if hasattr(entry_id, 'values'): entry_id = entry_id.values()[0]
-    elif entry.has_key("link") and entry.link:
+        if hasattr(entry_id, 'values'): entry_id = list(entry_id.values())[0]
+    elif "link" in entry and entry.link:
         entry_id = entry.link
-    elif entry.has_key("title") and entry.title:
+    elif "title" in entry and entry.title:
         entry_id = (entry.title_detail.base + "/" +
             md5(entry.title).hexdigest())
-    elif entry.has_key("summary") and entry.summary:
+    elif "summary" in entry and entry.summary:
         entry_id = (entry.summary_detail.base + "/" +
             md5(entry.summary).hexdigest())
-    elif entry.has_key("content") and entry.content:
+    elif "content" in entry and entry.content:
 
         entry_id = (entry.content[0].base + "/" + 
             md5(entry.content[0].value).hexdigest())
@@ -91,22 +91,22 @@
 
 def links(xentry, entry):
     """ copy links to the entry """
-    if not entry.has_key('links'):
+    if 'links' not in entry:
        entry['links'] = []
-       if entry.has_key('link'):
+       if 'link' in entry:
          entry['links'].append({'rel':'alternate', 'href':entry.link}) 
     xdoc = xentry.ownerDocument
     for link in entry['links']:
-        if not 'href' in link.keys(): continue
+        if not 'href' in list(link.keys()): continue
         xlink = xdoc.createElement('link')
         xlink.setAttribute('href', link.get('href'))
-        if link.has_key('type'):
+        if 'type' in link:
             xlink.setAttribute('type', link.get('type'))
-        if link.has_key('rel'):
+        if 'rel' in link:
             xlink.setAttribute('rel', link.get('rel',None))
-        if link.has_key('title'):
+        if 'title' in link:
             xlink.setAttribute('title', link.get('title'))
-        if link.has_key('length'):
+        if 'length' in link:
             xlink.setAttribute('length', link.get('length'))
         xentry.appendChild(xlink)
 
@@ -120,11 +120,11 @@
 
 def category(xentry, tag):
     xtag = xentry.ownerDocument.createElement('category')
-    if not tag.has_key('term') or not tag.term: return
+    if 'term' not in tag or not tag.term: return
     xtag.setAttribute('term', tag.get('term'))
-    if tag.has_key('scheme') and tag.scheme:
+    if 'scheme' in tag and tag.scheme:
         xtag.setAttribute('scheme', tag.get('scheme'))
-    if tag.has_key('label') and tag.label:
+    if 'label' in tag and tag.label:
         xtag.setAttribute('label', tag.get('label'))
     xentry.appendChild(xtag)
 
@@ -153,10 +153,10 @@
     xdoc = xentry.ownerDocument
     xcontent = xdoc.createElement(name)
 
-    if isinstance(detail.value,unicode):
+    if isinstance(detail.value,str):
         detail.value=detail.value.encode('utf-8')
 
-    if not detail.has_key('type') or detail.type.lower().find('html')<0:
+    if 'type' not in detail or detail.type.lower().find('html')<0:
         detail['value'] = escape(detail.value)
         detail['type'] = 'text/html'
 
@@ -204,11 +204,11 @@
 
 def location(xentry, long, lat):
     """ insert geo location into the entry """
-    if not lat or not long: return
+    if not lat or not int: return
 
     xlat = createTextElement(xentry, '%s:%s' % ('geo','lat'), '%f' % lat)
     xlat.setAttribute('xmlns:%s' % 'geo', 'http://www.w3.org/2003/01/geo/wgs84_pos#')
-    xlong = createTextElement(xentry, '%s:%s' % ('geo','long'), '%f' % long)
+    xlong = createTextElement(xentry, '%s:%s' % ('geo','long'), '%f' % int)
     xlong.setAttribute('xmlns:%s' % 'geo', 'http://www.w3.org/2003/01/geo/wgs84_pos#')
 
     xentry.appendChild(xlat)
@@ -222,7 +222,7 @@
     createTextElement(xsource, 'icon', source.get('icon', None))
     createTextElement(xsource, 'logo', source.get('logo', None))
 
-    if not source.has_key('logo') and source.has_key('image'):
+    if 'logo' not in source and 'image' in source:
         createTextElement(xsource, 'logo', source.image.get('href',None))
 
     for tag in source.get('tags',[]):
@@ -232,9 +232,9 @@
     for contributor in source.get('contributors',[]):
         author(xsource, 'contributor', contributor)
 
-    if not source.has_key('links') and source.has_key('href'): #rss
+    if 'links' not in source and 'href' in source: #rss
         source['links'] = [{ 'href': source.get('href') }]
-        if source.has_key('title'): 
+        if 'title' in source: 
             source['links'][0]['title'] = source.get('title')
     links(xsource, source)
 
@@ -248,9 +248,9 @@
     if not bozo == None: source['planet_bozo'] = bozo and 'true' or 'false'
 
     # propagate planet inserted information
-    if source.has_key('planet_name') and not source.has_key('planet_css-id'):
+    if 'planet_name' in source and 'planet_css-id' not in source:
         source['planet_css-id'] = cssid(source['planet_name'])
-    for key, value in source.items():
+    for key, value in list(source.items()):
         if key.startswith('planet_'):
             createTextElement(xsource, key.replace('_',':',1), value)
 
@@ -260,16 +260,16 @@
     xentry=xdoc.documentElement
     xentry.setAttribute('xmlns:planet',planet.xmlns)
 
-    if entry.has_key('language'):
+    if 'language' in entry:
         xentry.setAttribute('xml:lang', entry.language)
-    elif feed.feed.has_key('language'):
+    elif 'language' in feed.feed:
         xentry.setAttribute('xml:lang', feed.feed.language)
 
     id(xentry, entry)
     links(xentry, entry)
 
     bozo = feed.bozo
-    if not entry.has_key('title') or not entry.title:
+    if 'title' not in entry or not entry.title:
         xentry.appendChild(xdoc.createElement('title'))
 
     content(xentry, 'title', entry.get('title_detail',None), bozo)
@@ -280,7 +280,7 @@
     date(xentry, 'updated', entry_updated(feed.feed, entry, time.gmtime()))
     date(xentry, 'published', entry.get('published_parsed',None))
 
-    if entry.has_key('dc_date.taken'):
+    if 'dc_date.taken' in entry:
         date_Taken = createTextElement(xentry, '%s:%s' % ('dc','date_Taken'), '%s' % entry.get('dc_date.taken', None))
         date_Taken.setAttribute('xmlns:%s' % 'dc', 'http://purl.org/dc/elements/1.1/')
         xentry.appendChild(date_Taken)
@@ -290,16 +290,16 @@
 
     # known, simple text extensions
     for ns,name in [('feedburner','origLink')]:
-        if entry.has_key('%s_%s' % (ns,name.lower())) and \
-            feed.namespaces.has_key(ns):
+        if '%s_%s' % (ns,name.lower()) in entry and \
+            ns in feed.namespaces:
             xoriglink = createTextElement(xentry, '%s:%s' % (ns,name),
                 entry['%s_%s' % (ns,name.lower())])
             xoriglink.setAttribute('xmlns:%s' % ns, feed.namespaces[ns])
 
     # geo location
-    if entry.has_key('where') and \
-        entry.get('where',[]).has_key('type') and \
-        entry.get('where',[]).has_key('coordinates'):
+    if 'where' in entry and \
+        'type' in entry.get('where',[]) and \
+        'coordinates' in entry.get('where',[]):
         where = entry.get('where',[])
         type = where.get('type',None)
         coordinates = where.get('coordinates',None)
@@ -307,29 +307,29 @@
             location(xentry, coordinates[0], coordinates[1])
         elif type == 'Box' or type == 'LineString' or type == 'Polygon':
             location(xentry, coordinates[0][0], coordinates[0][1])
-    if entry.has_key('geo_lat') and \
-        entry.has_key('geo_long'):
+    if 'geo_lat' in entry and \
+        'geo_long' in entry:
         location(xentry, (float)(entry.get('geo_long',None)), (float)(entry.get('geo_lat',None)))
-    if entry.has_key('georss_point'):
+    if 'georss_point' in entry:
         coordinates = re.split('[,\s]', entry.get('georss_point'))
         location(xentry, (float)(coordinates[1]), (float)(coordinates[0]))
-    elif entry.has_key('georss_line'):
+    elif 'georss_line' in entry:
         coordinates = re.split('[,\s]', entry.get('georss_line'))
         location(xentry, (float)(coordinates[1]), (float)(coordinates[0]))
-    elif entry.has_key('georss_circle'):
+    elif 'georss_circle' in entry:
         coordinates = re.split('[,\s]', entry.get('georss_circle'))
         location(xentry, (float)(coordinates[1]), (float)(coordinates[0]))
-    elif entry.has_key('georss_box'):
+    elif 'georss_box' in entry:
         coordinates = re.split('[,\s]', entry.get('georss_box'))
         location(xentry, ((float)(coordinates[1])+(float)(coordinates[3]))/2, ((float)(coordinates[0])+(float)(coordinates[2]))/2)
-    elif entry.has_key('georss_polygon'):
+    elif 'georss_polygon' in entry:
         coordinates = re.split('[,\s]', entry.get('georss_polygon'))
         location(xentry, (float)(coordinates[1]), (float)(coordinates[0]))
 
     # author / contributor
     author_detail = entry.get('author_detail',{})
-    if author_detail and not author_detail.has_key('name') and \
-        feed.feed.has_key('planet_name'):
+    if author_detail and 'name' not in author_detail and \
+        'planet_name' in feed.feed:
         author_detail['name'] = feed.feed['planet_name']
     author(xentry, 'author', author_detail)
     for contributor in entry.get('contributors',[]):
@@ -338,17 +338,17 @@
     # merge in planet:* from feed (or simply use the feed if no source)
     src = entry.get('source')
     if src:
-        for name,value in feed.feed.items():
+        for name,value in list(feed.feed.items()):
             if name.startswith('planet_'): src[name]=value
-        if feed.feed.has_key('id'):
+        if 'id' in feed.feed:
             src['planet_id'] = feed.feed.id
     else:
         src = feed.feed
 
     # source:author
     src_author = src.get('author_detail',{})
-    if (not author_detail or not author_detail.has_key('name')) and \
-       not src_author.has_key('name') and  feed.feed.has_key('planet_name'):
+    if (not author_detail or 'name' not in author_detail) and \RefactoringTool: Writing converted planet.archlinux.org/planet/reconstitute.py to planet.archlinux.org3/planet/reconstitute.py.
RefactoringTool: Refactored planet.archlinux.org/planet/scrub.py
RefactoringTool: Writing converted planet.archlinux.org/planet/scrub.py to planet.archlinux.org3/planet/scrub.py.
RefactoringTool: Refactored planet.archlinux.org/planet/spider.py

+       'name' not in src_author and  'planet_name' in feed.feed:
        if src_author: src_author = src_author.__class__(src_author.copy())
        src['author_detail'] = src_author
        src_author['name'] = feed.feed['planet_name']
@@ -365,6 +365,6 @@
             (entry, 'published_parsed'),
             (feed,  'updated_parsed'),)
     for node, field in chks:
-        if node.has_key(field) and node[field]:
+        if field in node and node[field]:
             return node[field]
     return default
--- planet.archlinux.org/planet/scrub.py	(original)
+++ planet.archlinux.org/planet/scrub.py	(refactored)
@@ -16,12 +16,12 @@
     # some data is not trustworthy
     for tag in config.ignore_in_feed(feed_uri).split():
         if tag.find('lang')>=0: tag='language'
-        if data.feed.has_key(tag): del data.feed[tag]
+        if tag in data.feed: del data.feed[tag]
         for entry in data.entries:
-            if entry.has_key(tag): del entry[tag]
-            if entry.has_key(tag + "_detail"): del entry[tag + "_detail"]
-            if entry.has_key(tag + "_parsed"): del entry[tag + "_parsed"]
-            for key in entry.keys():
+            if tag in entry: del entry[tag]
+            if tag + "_detail" in entry: del entry[tag + "_detail"]
+            if tag + "_parsed" in entry: del entry[tag + "_parsed"]
+            for key in list(entry.keys()):
                 if not key.endswith('_detail'): continue
                 for detail in entry[key].copy():
                     if detail == tag: del entry[key][detail]
@@ -31,7 +31,7 @@
         title_type = config.title_type(feed_uri)
         title_type = type_map.get(title_type, title_type)
         for entry in data.entries:
-            if entry.has_key('title_detail'):
+            if 'title_detail' in entry:
                 entry.title_detail['type'] = title_type
 
     # adjust summary types
@@ -39,7 +39,7 @@
         summary_type = config.summary_type(feed_uri)
         summary_type = type_map.get(summary_type, summary_type)
         for entry in data.entries:
-            if entry.has_key('summary_detail'):
+            if 'summary_detail' in entry:
                 entry.summary_detail['type'] = summary_type
 
     # adjust content types
@@ -47,25 +47,25 @@
         content_type = config.content_type(feed_uri)
         content_type = type_map.get(content_type, content_type)
         for entry in data.entries:
-            if entry.has_key('content'):
+            if 'content' in entry:
                 entry.content[0]['type'] = content_type
 
     # some people put html in author names
     if config.name_type(feed_uri).find('html')>=0:
-        from shell.tmpl import stripHtml
-        if data.feed.has_key('author_detail') and \
-            data.feed.author_detail.has_key('name'):
+        from .shell.tmpl import stripHtml
+        if 'author_detail' in data.feed and \
+            'name' in data.feed.author_detail:
             data.feed.author_detail['name'] = \
                 str(stripHtml(data.feed.author_detail.name))
         for entry in data.entries:
-            if entry.has_key('author_detail') and \
-                entry.author_detail.has_key('name'):
+            if 'author_detail' in entry and \
+                'name' in entry.author_detail:
                 entry.author_detail['name'] = \
                     str(stripHtml(entry.author_detail.name))
-            if entry.has_key('source'):
+            if 'source' in entry:
                 source = entry.source
-                if source.has_key('author_detail') and \
-                    source.author_detail.has_key('name'):
+                if 'author_detail' in source and \
+                    'name' in source.author_detail:
                     source.author_detail['name'] = \
                         str(stripHtml(source.author_detail.name))
 
@@ -73,53 +73,53 @@
     future_dates = config.future_dates(feed_uri).lower()
     if future_dates == 'ignore_date':
       now = time.gmtime()
-      if data.feed.has_key('updated_parsed') and data.feed['updated_parsed']:
+      if 'updated_parsed' in data.feed and data.feed['updated_parsed']:
         if data.feed['updated_parsed'] > now: del data.feed['updated_parsed']
       for entry in data.entries:
-        if entry.has_key('published_parsed') and entry['published_parsed']:
+        if 'published_parsed' in entry and entry['published_parsed']:
           if entry['published_parsed'] > now:
             del entry['published_parsed']
             del entry['published']
-        if entry.has_key('updated_parsed') and entry['updated_parsed']:
+        if 'updated_parsed' in entry and entry['updated_parsed']:
           if entry['updated_parsed'] > now:
             del entry['updated_parsed']
             del entry['updated']
     elif future_dates == 'ignore_entry':
       now = time.time()
-      if data.feed.has_key('updated_parsed') and data.feed['updated_parsed']:
+      if 'updated_parsed' in data.feed and data.feed['updated_parsed']:
         if data.feed['updated_parsed'] > now: del data.feed['updated_parsed']
       data.entries = [entry for entry in data.entries if 
-        (not entry.has_key('published_parsed') or not entry['published_parsed']
+        ('published_parsed' not in entry or not entry['published_parsed']
           or entry['published_parsed'] <= now) and
-        (not entry.has_key('updated_parsed') or not entry['updated_parsed']
+        ('updated_parsed' not in entry or not entry['updated_parsed']
           or entry['updated_parsed'] <= now)]
 
     scrub_xmlbase = config.xml_base(feed_uri)
 
     # resolve relative URIs and sanitize
     for entry in data.entries + [data.feed]:
-        for key in entry.keys():
-            if key == 'content'and not entry.has_key('content_detail'):
+        for key in list(entry.keys()):
+            if key == 'content'and 'content_detail' not in entry:
                 node = entry.content[0]
             elif key.endswith('_detail'):
                 node = entry[key]
             else:
                 continue
 
-            if not node.has_key('type'): continue
+            if 'type' not in node: continue
             if not 'html' in node['type']: continue
-            if not node.has_key('value'): continue
+            if 'value' not in node: continue
 
-            if node.has_key('base'):
+            if 'base' in node:
                 if scrub_xmlbase:
                     if scrub_xmlbase == 'feed_alternate':
-                        if entry.has_key('source') and \
-                            entry.source.has_key('link'):
+                        if 'source' in entry and \
+                            'link' in entry.source:
                             node['base'] = entry.source.link
-                        elif data.feed.has_key('link'):
+                        elif 'link' in data.feed:
                             node['base'] = data.feed.link
                     elif scrub_xmlbase == 'entry_alternate':
-                        if entry.has_key('link'):
+                        if 'link' in entry:
                             node['base'] = entry.link
                     else:
                         node['base'] = feedparser._urljoin(
--- planet.archlinux.org/planet/spider.py	(original)
+++ planet.archlinux.org/planet/spider.py	(refactored)
@@ -4,11 +4,11 @@
 """
 
 # Standard library modules
-import time, calendar, re, os, urlparse
+import time, calendar, re, os, urllib.parse
 from xml.dom import minidom
 # Planet modules
 import planet, config, feedparser, reconstitute, shell, socket, scrub
-from StringIO import StringIO 
+from io import StringIO 
 
 try:
   from hashlib import md5
@@ -37,7 +37,7 @@
                 filename=filename.encode('idna')
     except:
         pass
-    if isinstance(filename,unicode):
+    if isinstance(filename,str):
         filename=filename.encode('utf-8')
     filename = re_url_scheme.sub("", filename)
     filename = re_slash.sub(",", filename)
@@ -63,7 +63,7 @@
     if mtime: os.utime(out, (mtime, mtime))
 
 def _is_http_uri(uri):
-    parsed = urlparse.urlparse(uri)
+    parsed = urllib.parse.urlparse(uri)
     return parsed[0] in ['http', 'https']
 
 def writeCache(feed_uri, feed_info, data):
@@ -72,8 +72,8 @@
     blacklist = config.cache_blacklist_directory()
 
     # capture http status
-    if not data.has_key("status"):
-        if data.has_key("entries") and len(data.entries)>0:
+    if "status" not in data:
+        if "entries" in data and len(data.entries)>0:
             data.status = 200
         elif data.bozo and \
             data.bozo_exception.__class__.__name__.lower()=='timeout':
@@ -85,27 +85,27 @@
         time.gmtime(time.time()-86400*config.activity_threshold(feed_uri))
 
     # process based on the HTTP status code
-    if data.status == 200 and data.has_key("url"):
+    if data.status == 200 and "url" in data:
         feed_info.feed['planet_http_location'] = data.url
-        if data.has_key("entries") and len(data.entries) == 0:
+        if "entries" in data and len(data.entries) == 0:
             log.warning("No data %s", feed_uri)
             feed_info.feed['planet_message'] = 'no data'
         elif feed_uri == data.url:
             log.info("Updating feed %s", feed_uri)
         else:
             log.info("Updating feed %s @ %s", feed_uri, data.url)
-    elif data.status == 301 and data.has_key("entries") and len(data.entries)>0:
+    elif data.status == 301 and "entries" in data and len(data.entries)>0:
         log.warning("Feed has moved from <%s> to <%s>", feed_uri, data.url)
         data.feed['planet_http_location'] = data.url
-    elif data.status == 304 and data.has_key("url"):
+    elif data.status == 304 and "url" in data:
         feed_info.feed['planet_http_location'] = data.url
         if feed_uri == data.url:
             log.info("Feed %s unchanged", feed_uri)
         else:
             log.info("Feed %s unchanged @ %s", feed_uri, data.url)
 
-        if not feed_info.feed.has_key('planet_message'):
-            if feed_info.feed.has_key('planet_updated'):
+        if 'planet_message' not in feed_info.feed:
+            if 'planet_updated' in feed_info.feed:
                 updated = feed_info.feed.planet_updated
                 if feedparser._parse_date_iso8601(updated) >= activity_horizon:
                     return
@@ -133,23 +133,23 @@
     data.feed['planet_http_status'] = str(data.status)
 
     # capture etag and last-modified information
-    if data.has_key('headers'):
-        if data.has_key('etag') and data.etag:
+    if 'headers' in data:
+        if 'etag' in data and data.etag:
             data.feed['planet_http_etag'] = data.etag
-        elif data.headers.has_key('etag') and data.headers['etag']:
+        elif 'etag' in data.headers and data.headers['etag']:
             data.feed['planet_http_etag'] =  data.headers['etag']
 
-        if data.headers.has_key('last-modified'):
+        if 'last-modified' in data.headers:
             data.feed['planet_http_last_modified']=data.headers['last-modified']
-        elif data.has_key('modified') and data.modified:
+        elif 'modified' in data and data.modified:
             data.feed['planet_http_last_modified'] = time.asctime(data.modified)
 
-        if data.headers.has_key('-content-hash'):
+        if '-content-hash' in data.headers:
             data.feed['planet_content_hash'] = data.headers['-content-hash']
 
     # capture feed and data from the planet configuration file
     if data.get('version'):
-        if not data.feed.has_key('links'): data.feed['links'] = list()
+        if 'links' not in data.feed: data.feed['links'] = list()
         feedtype = 'application/atom+xml'
         if data.version.startswith('rss'): feedtype = 'application/rss+xml'
         if data.version in ['rss090','rss10']: feedtype = 'application/rdf+xml'
@@ -160,7 +160,7 @@
         else:
             data.feed.links.append(feedparser.FeedParserDict(
                 {'rel':'self', 'type':feedtype, 'href':feed_uri}))
-    for name, value in config.feed_options(feed_uri).items():
+    for name, value in list(config.feed_options(feed_uri).items()):
         data.feed['planet_'+name] = value
 
     # perform user configured scrub operations on the data
@@ -174,16 +174,16 @@
     ids = {}
     for entry in data.entries:
         # generate an id, if none is present
-        if not entry.has_key('id') or not entry.id:
+        if 'id' not in entry or not entry.id:
             entry['id'] = reconstitute.id(None, entry)
         elif hasattr(entry['id'], 'values'):
-            entry['id'] = entry['id'].values()[0]
+            entry['id'] = list(entry['id'].values())[0]
         if not entry['id']: continue
 
         # determine updated date for purposes of selection
         updated = ''
-        if entry.has_key('published'): updated=entry.published
-        if entry.has_key('updated'):   updated=entry.updated
+        if 'published' in entry: updated=entry.published
+        if 'updated' in entry:   updated=entry.updated
 
         # if not seen or newer than last seen, select it
         if updated >= ids.get(entry.id,('',))[0]:
@@ -191,7 +191,7 @@
 
     # write each entry to the cache
     cache = config.cache_directory()
-    for updated, entry in ids.values():
+    for updated, entry in list(ids.values()):
 
         # compute blacklist file name based on the id
         blacklist_file = filename(blacklist, entry.id)  
@@ -205,9 +205,9 @@
 
         # get updated-date either from the entry or the cache (default to now)
         mtime = None
-        if not entry.has_key('updated_parsed') or not entry['updated_parsed']:
+        if 'updated_parsed' not in entry or not entry['updated_parsed']:
             entry['updated_parsed'] = entry.get('published_parsed',None)
-        if entry.has_key('updated_parsed'):
+        if 'updated_parsed' in entry:
             try:
                 mtime = calendar.timegm(entry.updated_parsed)
             except:
@@ -216,7 +216,7 @@
             try:
                 mtime = os.stat(cache_file).st_mtime
             except:
-                if data.feed.has_key('updated_parsed'):
+                if 'updated_parsed' in data.feed:
                     try:
                         mtime = calendar.timegm(data.feed.updated_parsed)
                     except:
@@ -242,7 +242,7 @@
         if index != None: 
             feedid = data.feed.get('id', data.feed.get('link',None))
             if feedid:
-                if type(feedid) == unicode: feedid = feedid.encode('utf-8')
+                if type(feedid) == str: feedid = feedid.encode('utf-8')
                 index[filename('', entry.id)] = feedid
 
     if index: index.close()
@@ -250,13 +250,13 @@
     # identify inactive feeds
     if config.activity_threshold(feed_uri):
         updated = [entry.updated_parsed for entry in data.entries
-            if entry.has_key('updated_parsed')]
+            if 'updated_parsed' in entry]
         updated.sort()
 
         if updated:
             data.feed['planet_updated'] = \
                 time.strftime("%Y-%m-%dT%H:%M:%SZ", updated[-1])
-        elif data.feed.has_key('planet_updated'):
+        elif 'planet_updated' in data.feed:
            updated = [feedparser._parse_date_iso8601(data.feed.planet_updated)]
 
         if not updated or updated[-1] < activity_horizon:
@@ -266,8 +266,8 @@
 
     # report channel level errors
     if data.status == 226:
-        if data.feed.has_key('planet_message'): del data.feed['planet_message']
-        if feed_info.feed.has_key('planet_updated'):
+        if 'planet_message' in data.feed: del data.feed['planet_message']
+        if 'planet_updated' in feed_info.feed:
             data.feed['planet_updated'] = feed_info.feed['planet_updated']
     elif data.status == 403:
         data.feed['planet_message'] = "403: forbidden"
@@ -292,7 +292,7 @@
 
 def httpThread(thread_index, input_queue, output_queue, log):
     import httplib2
-    from httplib import BadStatusLine
+    from http.client import BadStatusLine
 
     h = httplib2.Http(config.http_cache_directory())
     uri, feed_info = input_queue.get(block=True)
@@ -305,7 +305,7 @@
         try:
             # map IRI => URI
             try:
-                if isinstance(uri,unicode):
+                if isinstance(uri,str):
                     idna = uri.encode('idna')RefactoringTool: Writing converted planet.archlinux.org/planet/spider.py to planet.archlinux.org3/planet/spider.py.
RefactoringTool: Refactored planet.archlinux.org/planet/splice.py
RefactoringTool: Writing converted planet.archlinux.org/planet/splice.py to planet.archlinux.org3/planet/splice.py.
RefactoringTool: Refactored planet.archlinux.org/planet/shell/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/shell/__init__.py to planet.archlinux.org3/planet/shell/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/shell/_genshi.py

                 else:
                     idna = uri.decode('utf-8').encode('idna')
@@ -316,9 +316,9 @@
 
             # cache control headers
             headers = {}
-            if feed_info.feed.has_key('planet_http_etag'):
+            if 'planet_http_etag' in feed_info.feed:
                 headers['If-None-Match'] = feed_info.feed['planet_http_etag']
-            if feed_info.feed.has_key('planet_http_last_modified'):
+            if 'planet_http_last_modified' in feed_info.feed:
                 headers['If-Modified-Since'] = \
                     feed_info.feed['planet_http_last_modified']
 
@@ -330,7 +330,7 @@
             if resp.status == 200:
                 if resp.fromcache:
                     resp.status = 304
-                elif feed_info.feed.has_key('planet_content_hash') and \
+                elif 'planet_content_hash' in feed_info.feed and \
                     feed_info.feed['planet_content_hash'] == \
                     resp['-content-hash']:
                     resp.status = 304
@@ -338,21 +338,21 @@
             # build a file-like object
             feed = StringIO(content) 
             setattr(feed, 'url', resp.get('content-location', uri))
-            if resp.has_key('content-encoding'):
+            if 'content-encoding' in resp:
                 del resp['content-encoding']
             setattr(feed, 'headers', resp)
         except BadStatusLine:
             log.error("Bad Status Line received for %s via %d",
                 uri, thread_index)
-        except httplib2.HttpLib2Error, e:
+        except httplib2.HttpLib2Error as e:
             log.error("HttpLib2Error: %s via %d", str(e), thread_index)
-        except socket.error, e:
+        except socket.error as e:
             if e.__class__.__name__.lower()=='timeout':
                 feed.headers['status'] = '408'
                 log.warn("Timeout in thread-%d", thread_index)
             else:
                 log.error("HTTP Error: %s in thread-%d", str(e), thread_index)
-        except Exception, e:
+        except Exception as e:
             import sys, traceback
             type, value, tb = sys.exc_info()
             log.error('Error processing %s', uri)
@@ -382,7 +382,7 @@
         except:
             log.warning("Timeout set to invalid value '%s', skipping", timeout)
 
-    from Queue import Queue
+    from queue import Queue
     from threading import Thread
 
     fetch_queue = Queue()
@@ -424,7 +424,7 @@
             parse_queue.put(item=(uri, feed_info, uri))
 
     # Mark the end of the fetch queue
-    for thread in threads.keys():
+    for thread in list(threads.keys()):
         fetch_queue.put(item=(None, None))
 
     # Process the results as they arrive
@@ -458,7 +458,7 @@
                 if not id: id = feed_info.feed.get('id', None)
 
                 href=uri
-                if data.has_key('href'): href=data.href
+                if 'href' in data: href=data.href
 
                 duplicate = None
                 if id and id in feeds_seen:
@@ -479,7 +479,7 @@
                 # complete processing for the feed
                 writeCache(uri, feed_info, data)
 
-            except Exception, e:
+            except Exception as e:
                 import sys, traceback
                 type, value, tb = sys.exc_info()
                 log.error('Error processing %s', uri)
@@ -489,7 +489,7 @@
 
         time.sleep(0.1)
 
-        for index in threads.keys():
+        for index in list(threads.keys()):
             if not threads[index].isAlive():
                 del threads[index]
                 if not threads:
--- planet.archlinux.org/planet/splice.py	(original)
+++ planet.archlinux.org/planet/splice.py	(refactored)
@@ -2,8 +2,8 @@
 import glob, os, time, shutil
 from xml.dom import minidom
 import planet, config, feedparser, reconstitute, shell
-from reconstitute import createTextElement, date
-from spider import filename
+from .reconstitute import createTextElement, date
+from .spider import filename
 from planet import idindex
 
 def splice():
@@ -62,12 +62,12 @@
     sources = config.cache_sources_directory()
     for sub in config.subscriptions():
         data=feedparser.parse(filename(sources,sub))
-        if data.feed.has_key('id'): sub_ids.append(data.feed.id)
+        if 'id' in data.feed: sub_ids.append(data.feed.id)
         if not data.feed: continue
 
         # warn on missing links
-        if not data.feed.has_key('planet_message'):
-            if not data.feed.has_key('links'): data.feed['links'] = []
+        if 'planet_message' not in data.feed:
+            if 'links' not in data.feed: data.feed['links'] = []
 
             for link in data.feed.links:
               if link.rel == 'self': break
@@ -94,7 +94,7 @@
     for mtime,file in dir:
         if index != None:
             base = os.path.basename(file)
-            if index.has_key(base) and index[base] not in sub_ids: continue
+            if base in index and index[base] not in sub_ids: continue
 
         try:
             entry=minidom.parse(file)
@@ -186,6 +186,6 @@
             if not os.path.exists(dest_dir): os.makedirs(dest_dir)
 
             log.info("Copying %s to %s", source, dest)
-            if os.path.exists(dest): os.chmod(dest, 0644)
+            if os.path.exists(dest): os.chmod(dest, 0o644)
             shutil.copyfile(source, dest)
             shutil.copystat(source, dest)
--- planet.archlinux.org/planet/shell/__init__.py	(original)
+++ planet.archlinux.org/planet/shell/__init__.py	(refactored)
@@ -48,7 +48,7 @@
             module = __import__("_" + module_name)
         except:
             module = __import__(module_name)
-    except Exception, inst:
+    except Exception as inst:
         return log.error("Skipping %s '%s' after failing to load '%s': %s", 
             mode, template_resolved, module_name, inst)
 
--- planet.archlinux.org/planet/shell/_genshi.py	(original)
+++ planet.archlinux.org/planet/shell/_genshi.py	(refactored)
@@ -1,4 +1,4 @@
-from StringIO import StringIO
+from io import StringIO
 from xml.sax.saxutils import escape
 
 from genshi.input import HTMLParser, XMLParser
@@ -16,7 +16,7 @@
 def norm(value):
     """ Convert to Unicode """
     if hasattr(value,'items'):
-        return dict([(norm(n),norm(v)) for n,v in value.items()])
+        return dict([(norm(n),norm(v)) for n,v in list(value.items())])
 
     try:
         return value.decode('utf-8')
@@ -26,9 +26,9 @@
 def find_config(config, feed):
     # match based on self link
     for link in feed.links:
-        if link.has_key('rel') and link.rel=='self':
-            if link.has_key('type') and link.type in feed_types:
-                if link.has_key('href') and link.href in subscriptions:
+        if 'rel' in link and link.rel=='self':
+            if 'type' in link and link.type in feed_types:
+                if 'href' in link and link.href in subscriptions:
                     return norm(dict(config.parser.items(link.href)))
     
     # match based on name
@@ -49,13 +49,13 @@
     def __iter__(self):
         self.iter = self.parser.__iter__()
         return self
-    def next(self):
-        object = self.iter.next()
+    def __next__(self):
+        object = next(self.iter)
         if object[0] == 'END': self.depth = self.depth - 1
         predepth = self.depth
         if object[0] == 'START': self.depth = self.depth + 1
         if predepth: return object
-        return self.next()
+        return next(self)
 
 def streamify(text,bozo):
     """ add a .stream to a _detail textConstruct """
@@ -91,7 +91,7 @@
         for sub in config.subscriptions():
             data=feedparser.parse(filename(sources,sub))
             data.feed.config = norm(dict(config.parser.items(sub)))
-            if data.feed.has_key('link'):
+            if 'link' in data.feed:
                 feeds.append((data.feed.config.get('name',''),data.feed))
             subscriptions.append(norm(sub))
         feeds.sort()
@@ -108,8 +108,8 @@
              # add new_feed and new_date fields
              entry.new_feed = entry.source.id
              entry.new_date = date = None
-             if entry.has_key('published_parsed'): date=entry.published_parsedRefactoringTool: Writing converted planet.archlinux.org/planet/shell/_genshi.py to planet.archlinux.org3/planet/shell/_genshi.py.
RefactoringTool: Refactored planet.archlinux.org/planet/shell/dj.py
RefactoringTool: Writing converted planet.archlinux.org/planet/shell/dj.py to planet.archlinux.org3/planet/shell/dj.py.
RefactoringTool: Refactored planet.archlinux.org/planet/shell/plugin.py
RefactoringTool: Writing converted planet.archlinux.org/planet/shell/plugin.py to planet.archlinux.org3/planet/shell/plugin.py.
RefactoringTool: Refactored planet.archlinux.org/planet/shell/py.py
RefactoringTool: Writing converted planet.archlinux.org/planet/shell/py.py to planet.archlinux.org3/planet/shell/py.py.
RefactoringTool: No changes to planet.archlinux.org/planet/shell/sed.py
RefactoringTool: Writing converted planet.archlinux.org/planet/shell/sed.py to planet.archlinux.org3/planet/shell/sed.py.
RefactoringTool: Refactored planet.archlinux.org/planet/shell/tmpl.py

-             if entry.has_key('updated_parsed'): date=entry.updated_parsed
+             if 'published_parsed' in entry: date=entry.published_parsed
+             if 'updated_parsed' in entry: date=entry.updated_parsed
              if date: entry.new_date = time.strftime(new_date_format, date)
 
              # remove new_feed and new_date fields if not "new"
@@ -124,11 +124,11 @@
                  last_feed = None
 
              # add streams for all text constructs
-             for key in entry.keys():
-                 if key.endswith("_detail") and entry[key].has_key('type') and \
-                     entry[key].has_key('value'):
+             for key in list(entry.keys()):
+                 if key.endswith("_detail") and 'type' in entry[key] and \
+                     'value' in entry[key]:
                      streamify(entry[key],entry.source.planet_bozo)
-             if entry.has_key('content'):
+             if 'content' in entry:
                  for content in entry.content:
                      streamify(content,entry.source.planet_bozo)
      
--- planet.archlinux.org/planet/shell/dj.py	(original)
+++ planet.archlinux.org/planet/shell/dj.py	(refactored)
@@ -1,8 +1,8 @@
 import os.path
-import urlparse
+import urllib.parse
 import datetime
 
-import tmpl
+from . import tmpl
 from planet import config
 
 def DjangoPlanetDate(value):
@@ -39,10 +39,10 @@
 
     if output_file:
         reluri = os.path.splitext(os.path.basename(output_file))[0]
-        context['url'] = urlparse.urljoin(config.link(),reluri)
+        context['url'] = urllib.parse.urljoin(config.link(),reluri)
         f = open(output_file, 'w')
         ss = t.render(context)
-        if isinstance(ss,unicode): ss=ss.encode('utf-8')
+        if isinstance(ss,str): ss=ss.encode('utf-8')
         f.write(ss)
         f.close()
     else:
--- planet.archlinux.org/planet/shell/plugin.py	(original)
+++ planet.archlinux.org/planet/shell/plugin.py	(refactored)
@@ -1,5 +1,5 @@
 import os, sys, imp
-from StringIO import StringIO
+from io import StringIO
 
 def run(script, doc, output_file=None, options={}):
     """ process an Python script using imp """
@@ -21,14 +21,14 @@
         sys.stderr = plugin_stderr
 
         # determine __file__ value
-        if options.has_key("__file__"):
+        if "__file__" in options:
             plugin_file = options["__file__"]
             del options["__file__"]
         else:
             plugin_file = script
 
         # set sys.argv
-        options = sum([['--'+key, value] for key,value in options.items()], [])
+        options = sum([['--'+key, value] for key,value in list(options.items())], [])
         sys.argv = [plugin_file] + options
 
         # import script
@@ -39,9 +39,9 @@
                 try:
                     description=('.plugin', 'rb', imp.PY_SOURCE)
                     imp.load_module('__main__',handle,plugin_file,description)
-                except SystemExit,e:
+                except SystemExit as e:
                     if e.code: log.error('%s exit rc=%d',(plugin_file,e.code))
-            except Exception, e:
+            except Exception as e:
                 import traceback
                 type, value, tb = sys.exc_info()
                 plugin_stderr.write(''.join(
--- planet.archlinux.org/planet/shell/py.py	(original)
+++ planet.archlinux.org/planet/shell/py.py	(refactored)
@@ -9,7 +9,7 @@
     else:
         out = PIPE
 
-    options = sum([['--'+key, value] for key,value in options.items()], [])
+    options = sum([['--'+key, value] for key,value in list(options.items())], [])
 
     proc = Popen([sys.executable, script] + options,
         stdin=PIPE, stdout=out, stderr=PIPE)
--- planet.archlinux.org/planet/shell/tmpl.py	(original)
+++ planet.archlinux.org/planet/shell/tmpl.py	(refactored)
@@ -1,5 +1,5 @@
 from xml.sax.saxutils import escape
-import sgmllib, time, os, sys, new, urlparse, re
+import sgmllib, time, os, sys, new, urllib.parse, re
 from planet import config, feedparser
 import htmltmpl
 
@@ -20,15 +20,15 @@
             self.feed(data)
         self.close()
     def __str__(self):
-        if isinstance(self.result, unicode):
+        if isinstance(self.result, str):
             return self.result.encode('utf-8')
         return self.result
     def handle_entityref(self, ref):
-        import htmlentitydefs
-        if ref in htmlentitydefs.entitydefs:
-            ref=htmlentitydefs.entitydefs[ref]
+        import html.entities
+        if ref in html.entities.entitydefs:
+            ref=html.entities.entitydefs[ref]
             if len(ref)==1:
-                self.result+=unichr(ord(ref))
+                self.result+=chr(ord(ref))
             elif ref.startswith('&#') and ref.endswith(';'):
                 self.handle_charref(ref[2:-1])
             else:
@@ -38,9 +38,9 @@
     def handle_charref(self, ref):
         try:
             if ref.startswith('x'):
-                self.result+=unichr(int(ref[1:],16))
-            else:
-                self.result+=unichr(int(ref))
+                self.result+=chr(int(ref[1:],16))
+            else:
+                self.result+=chr(int(ref))
         except:
             self.result+='&#%s;' % ref
     def handle_data(self, data):
@@ -49,7 +49,7 @@
 # Data format mappers
 
 def String(value):
-    if isinstance(value, unicode): return value.encode('utf-8')
+    if isinstance(value, str): return value.encode('utf-8')
     return value
 
 def Plain(value):
@@ -145,7 +145,7 @@
                 node = node[path]
             elif isinstance(path, dict):
                 for test in node:
-                    for key, value in path.items():
+                    for key, value in list(path.items()):
                         if test.get(key,None) != value: break
                     else:
                         node = test
@@ -158,19 +158,19 @@
             if node: output[rule[0]] = rule[1](node)
         
     # copy over all planet namespaced elements from parent source
-    for name,value in source.items():
+    for name,value in list(source.items()):
         if name.startswith('planet_'):
             output[name[7:]] = String(value)
-        if not output.get('name') and source.has_key('title_detail'):
+        if not output.get('name') and 'title_detail' in source:
             output['name'] = Plain(source.title_detail.value)
 
     # copy over all planet namespaced elements from child source element
     if 'source' in source:
-        for name,value in source.source.items():
+        for name,value in list(source.source.items()):
             if name.startswith('planet_'):
                 output['channel_' + name[7:]] = String(value)
             if not output.get('channel_name') and \
-                source.source.has_key('title_detail'):
+                'title_detail' in source.source:
                 output['channel_name'] = Plain(source.source.title_detail.value)
 
     return output
@@ -178,7 +178,7 @@
 def _end_planet_source(self):
     self._end_source()
     context = self._getContext()
-    if not context.has_key('sources'): context['sources'] = []
+    if 'sources' not in context: context['sources'] = []
     context.sources.append(context.source)
     del context['source']
 
@@ -233,14 +233,14 @@
     # remove new_dates and new_channels that aren't "new"
     date = channel = None
     for item in output['Items']:
-        if item.has_key('new_date'):
+        if 'new_date' in item:
             if item['new_date'] == date:
                 del item['new_date']
             else:
                 date = item['new_date']
 
-        if item.has_key('new_channel'):
-            if item['new_channel'] == channel and not item.has_key('new_date'):
+        if 'new_channel' in item:
+            if item['new_channel'] == channel and 'new_date' not in item:
                 del item['new_channel']
             else:
                 channel = item['new_channel']
@@ -252,14 +252,14 @@
     manager = htmltmpl.TemplateManager()
     template = manager.prepare(script)
     tp = htmltmpl.TemplateProcessor(html_escape=0)
-    for key,value in template_info(doc).items():
+    for key,value in list(template_info(doc).items()):RefactoringTool: Writing converted planet.archlinux.org/planet/shell/tmpl.py to planet.archlinux.org3/planet/shell/tmpl.py.
RefactoringTool: Refactored planet.archlinux.org/planet/shell/xslt.py
RefactoringTool: Writing converted planet.archlinux.org/planet/shell/xslt.py to planet.archlinux.org3/planet/shell/xslt.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/feedparser.py

         tp.set(key, value)
 
     if output_file:
         basename = os.path.basename(output_file)
         reluri = os.path.splitext(os.path.basename(output_file))[0]
-        tp.set('url', urlparse.urljoin(config.link(),reluri))
-        tp.set('fullurl', urlparse.urljoin(config.link(),basename))
+        tp.set('url', urllib.parse.urljoin(config.link(),reluri))
+        tp.set('fullurl', urllib.parse.urljoin(config.link(),basename))
 
         output = open(output_file, "w")
         output.write(tp.process(template))
--- planet.archlinux.org/planet/shell/xslt.py	(original)
+++ planet.archlinux.org/planet/shell/xslt.py	(refactored)
@@ -2,7 +2,7 @@
 
 def quote(string, apos):
     """ quote a string so that it can be passed as a parameter """
-    if type(string) == unicode:
+    if type(string) == str:
         string=string.encode('utf-8')
     if apos.startswith("\\"): string.replace('\\','\\\\')
 
@@ -32,7 +32,7 @@
     if dom:
         styledoc = libxml2.parseFile(script)
         style = libxslt.parseStylesheetDoc(styledoc)
-        for key in options.keys():
+        for key in list(options.keys()):
             options[key] = quote(options[key], apos="\xe2\x80\x99")
         output = style.applyStylesheet(dom, options)
         if output_file:
@@ -51,7 +51,7 @@
         file.close()
 
         cmdopts = []
-        for key,value in options.items():
+        for key,value in list(options.items()):
            if value.find("'")>=0 and value.find('"')>=0: continue
            cmdopts += ['--stringparam', key, quote(value, apos=r"\'")]
 
@@ -63,7 +63,7 @@
         from subprocess import Popen, PIPE
 
         options = sum([['--stringparam', key, value]
-            for key,value in options.items()], [])
+            for key,value in list(options.items())], [])
 
         proc = Popen(['xsltproc'] + options + [script, '-'],
             stdin=PIPE, stdout=PIPE, stderr=PIPE)
--- planet.archlinux.org/planet/vendor/feedparser.py	(original)
+++ planet.archlinux.org/planet/vendor/feedparser.py	(refactored)
@@ -76,11 +76,11 @@
 SANITIZE_HTML = 1
 
 # ---------- required modules (should come with any Python distribution) ----------
-import sgmllib, re, sys, copy, urlparse, time, rfc822, types, cgi, urllib, urllib2
+import sgmllib, re, sys, copy, urllib.parse, time, rfc822, types, cgi, urllib.request, urllib.parse, urllib.error, urllib.request, urllib.error, urllib.parse
 try:
-    from cStringIO import StringIO as _StringIO
+    from io import StringIO as _StringIO
 except:
-    from StringIO import StringIO as _StringIO
+    from io import StringIO as _StringIO
 
 # ---------- optional modules (feedparser will work without these, but with reduced functionality) ----------
 
@@ -142,13 +142,13 @@
 
 # reversable htmlentitydefs mappings for Python 2.2
 try:
-  from htmlentitydefs import name2codepoint, codepoint2name
+  from html.entities import name2codepoint, codepoint2name
 except:
-  import htmlentitydefs
+  import html.entities
   name2codepoint={}
   codepoint2name={}
-  for (name,codepoint) in htmlentitydefs.entitydefs.iteritems():
-    if codepoint.startswith('&#'): codepoint=unichr(int(codepoint[2:-1]))
+  for (name,codepoint) in html.entities.entitydefs.items():
+    if codepoint.startswith('&#'): codepoint=chr(int(codepoint[2:-1]))
     name2codepoint[name]=ord(codepoint)
     codepoint2name[ord(codepoint)]=name
 
@@ -233,16 +233,16 @@
         if key == 'category':
             return UserDict.__getitem__(self, 'tags')[0]['term']
         if key == 'enclosures':
-            norel = lambda link: FeedParserDict([(name,value) for (name,value) in link.items() if name!='rel'])
+            norel = lambda link: FeedParserDict([(name,value) for (name,value) in list(link.items()) if name!='rel'])
             return [norel(link) for link in UserDict.__getitem__(self, 'links') if link['rel']=='enclosure']
         if key == 'license':
             for link in UserDict.__getitem__(self, 'links'):
-                if link['rel']=='license' and link.has_key('href'):
+                if link['rel']=='license' and 'href' in link:
                     return link['href']
         if key == 'categories':
             return [(tag['scheme'], tag['term']) for tag in UserDict.__getitem__(self, 'tags')]
         realkey = self.keymap.get(key, key)
-        if type(realkey) == types.ListType:
+        if type(realkey) == list:
             for k in realkey:
                 if UserDict.has_key(self, k):
                     return UserDict.__getitem__(self, k)
@@ -251,21 +251,21 @@
         return UserDict.__getitem__(self, realkey)
 
     def __setitem__(self, key, value):
-        for k in self.keymap.keys():
+        for k in list(self.keymap.keys()):
             if key == k:
                 key = self.keymap[k]
-                if type(key) == types.ListType:
+                if type(key) == list:
                     key = key[0]
         return UserDict.__setitem__(self, key, value)
 
     def get(self, key, default=None):
-        if self.has_key(key):
+        if key in self:
             return self[key]
         else:
             return default
 
     def setdefault(self, key, value):
-        if not self.has_key(key):
+        if key not in self:
             self[key] = value
         return self[key]
         
@@ -284,7 +284,7 @@
             assert not key.startswith('_')
             return self.__getitem__(key)
         except:
-            raise AttributeError, "object has no attribute '%s'" % key
+            raise AttributeError("object has no attribute '%s'" % key)
 
     def __setattr__(self, key, value):
         if key.startswith('_') or key == 'data':
@@ -293,7 +293,7 @@
             return self.__setitem__(key, value)
 
     def __contains__(self, key):
-        return self.has_key(key)
+        return key in self
 
 def zopeCompatibilityHack():
     global FeedParserDict
@@ -328,46 +328,46 @@
             )
         import string
         _ebcdic_to_ascii_map = string.maketrans( \
-            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))
+            ''.join(map(chr, list(range(256)))), ''.join(map(chr, emap)))
     return s.translate(_ebcdic_to_ascii_map)
  
 _cp1252 = {
-  unichr(128): unichr(8364), # euro sign
-  unichr(130): unichr(8218), # single low-9 quotation mark
-  unichr(131): unichr( 402), # latin small letter f with hook
-  unichr(132): unichr(8222), # double low-9 quotation mark
-  unichr(133): unichr(8230), # horizontal ellipsis
-  unichr(134): unichr(8224), # dagger
-  unichr(135): unichr(8225), # double dagger
-  unichr(136): unichr( 710), # modifier letter circumflex accent
-  unichr(137): unichr(8240), # per mille sign
-  unichr(138): unichr( 352), # latin capital letter s with caron
-  unichr(139): unichr(8249), # single left-pointing angle quotation mark
-  unichr(140): unichr( 338), # latin capital ligature oe
-  unichr(142): unichr( 381), # latin capital letter z with caron
-  unichr(145): unichr(8216), # left single quotation mark
-  unichr(146): unichr(8217), # right single quotation mark
-  unichr(147): unichr(8220), # left double quotation mark
-  unichr(148): unichr(8221), # right double quotation mark
-  unichr(149): unichr(8226), # bullet
-  unichr(150): unichr(8211), # en dash
-  unichr(151): unichr(8212), # em dash
-  unichr(152): unichr( 732), # small tilde
-  unichr(153): unichr(8482), # trade mark sign
-  unichr(154): unichr( 353), # latin small letter s with caron
-  unichr(155): unichr(8250), # single right-pointing angle quotation mark
-  unichr(156): unichr( 339), # latin small ligature oe
-  unichr(158): unichr( 382), # latin small letter z with caron
-  unichr(159): unichr( 376)} # latin capital letter y with diaeresis
+  chr(128): chr(8364), # euro sign
+  chr(130): chr(8218), # single low-9 quotation mark
+  chr(131): chr( 402), # latin small letter f with hook
+  chr(132): chr(8222), # double low-9 quotation mark
+  chr(133): chr(8230), # horizontal ellipsis
+  chr(134): chr(8224), # dagger
+  chr(135): chr(8225), # double dagger
+  chr(136): chr( 710), # modifier letter circumflex accent
+  chr(137): chr(8240), # per mille sign
+  chr(138): chr( 352), # latin capital letter s with caron
+  chr(139): chr(8249), # single left-pointing angle quotation mark
+  chr(140): chr( 338), # latin capital ligature oe
+  chr(142): chr( 381), # latin capital letter z with caron
+  chr(145): chr(8216), # left single quotation mark
+  chr(146): chr(8217), # right single quotation mark
+  chr(147): chr(8220), # left double quotation mark
+  chr(148): chr(8221), # right double quotation mark
+  chr(149): chr(8226), # bullet
+  chr(150): chr(8211), # en dash
+  chr(151): chr(8212), # em dash
+  chr(152): chr( 732), # small tilde
+  chr(153): chr(8482), # trade mark sign
+  chr(154): chr( 353), # latin small letter s with caron
+  chr(155): chr(8250), # single right-pointing angle quotation mark
+  chr(156): chr( 339), # latin small ligature oe
+  chr(158): chr( 382), # latin small letter z with caron
+  chr(159): chr( 376)} # latin capital letter y with diaeresis
 
 _urifixer = re.compile('^([A-Za-z][A-Za-z0-9+-.]*://)(/*)(.*?)')
 def _urljoin(base, uri):
     uri = _urifixer.sub(r'\1\3', uri)
     try:
-        return urlparse.urljoin(base, uri)
+        return urllib.parse.urljoin(base, uri)
     except:
-        uri = urlparse.urlunparse([urllib.quote(part) for part in urlparse.urlparse(uri)])
-        return urlparse.urljoin(base, uri)
+        uri = urllib.parse.urlunparse([urllib.parse.quote(part) for part in urllib.parse.urlparse(uri)])
+        return urllib.parse.urljoin(base, uri)
 
 class _FeedParserMixin:
     namespaces = {'': '',
@@ -444,7 +444,7 @@
     def __init__(self, baseuri=None, baselang=None, encoding='utf-8'):
         if _debug: sys.stderr.write('initializing FeedParser\n')
         if not self._matchnamespaces:
-            for k, v in self.namespaces.items():
+            for k, v in list(self.namespaces.items()):
                 self._matchnamespaces[k.lower()] = v
         self.feeddata = FeedParserDict() # feed-level data
         self.encoding = encoding # character encoding
@@ -486,11 +486,11 @@
         # track xml:base and xml:lang
         attrsD = dict(attrs)
         baseuri = attrsD.get('xml:base', attrsD.get('base')) or self.baseuri
-        if type(baseuri) != type(u''):
+        if type(baseuri) != type(''):
             try:
-                baseuri = unicode(baseuri, self.encoding)
+                baseuri = str(baseuri, self.encoding)
             except:
-                baseuri = unicode(baseuri, 'iso-8859-1')
+                baseuri = str(baseuri, 'iso-8859-1')
         self.baseuri = _urljoin(self.baseuri, baseuri)
         lang = attrsD.get('xml:lang', attrsD.get('lang'))
         if lang == '':
@@ -514,12 +514,12 @@
                 self.trackNamespace(None, uri)
 
         # track inline content
-        if self.incontent and self.contentparams.has_key('type') and not self.contentparams.get('type', 'xml').endswith('xml'):
+        if self.incontent and 'type' in self.contentparams and not self.contentparams.get('type', 'xml').endswith('xml'):
             if tag in ['xhtml:div', 'div']: return # typepad does this 10/2007
             # element declared itself as escaped markup, but it isn't really
             self.contentparams['type'] = 'application/xhtml+xml'
         if self.incontent and self.contentparams.get('type') == 'application/xhtml+xml':
-            if tag.find(':') <> -1:
+            if tag.find(':') != -1:
                 prefix, tag = tag.split(':', 1)
                 namespace = self.namespacesInUse.get(prefix, '')
                 if tag=='math' and namespace=='http://www.w3.org/1998/Math/MathML':
@@ -530,7 +530,7 @@
             return self.handle_data('<%s%s>' % (tag, self.strattrs(attrs)), escape=0)
 
         # match namespaces
-        if tag.find(':') <> -1:
+        if tag.find(':') != -1:
             prefix, suffix = tag.split(':', 1)
         else:
             prefix, suffix = '', tag
@@ -563,7 +563,7 @@
     def unknown_endtag(self, tag):
         if _debug: sys.stderr.write('end %s\n' % tag)
         # match namespaces
-        if tag.find(':') <> -1:
+        if tag.find(':') != -1:
             prefix, suffix = tag.split(':', 1)
         else:
             prefix, suffix = '', tag
@@ -582,7 +582,7 @@
             self.pop(prefix + suffix)
 
         # track inline content
-        if self.incontent and self.contentparams.has_key('type') and not self.contentparams.get('type', 'xml').endswith('xml'):
+        if self.incontent and 'type' in self.contentparams and not self.contentparams.get('type', 'xml').endswith('xml'):
             # element declared itself as escaped markup, but it isn't really
             if tag in ['xhtml:div', 'div']: return # typepad does this 10/2007
             self.contentparams['type'] = 'application/xhtml+xml'
@@ -611,7 +611,7 @@
                 c = int(ref[1:], 16)
             else:
                 c = int(ref)
-            text = unichr(c).encode('utf-8')
+            text = chr(c).encode('utf-8')
         self.elementstack[-1][2].append(text)
 
     def handle_entityref(self, ref):
@@ -620,14 +620,14 @@
         if _debug: sys.stderr.write('entering handle_entityref with %s\n' % ref)
         if ref in ('lt', 'gt', 'quot', 'amp', 'apos'):
             text = '&%s;' % ref
-        elif ref in self.entities.keys():
+        elif ref in list(self.entities.keys()):
             text = self.entities[ref]
             if text.startswith('&#') and text.endswith(';'):
                 return self.handle_entityref(text)
         else:
             try: name2codepoint[ref]
             except KeyError: text = '&%s;' % ref
-            else: text = unichr(name2codepoint[ref]).encode('utf-8')
+            else: text = chr(name2codepoint[ref]).encode('utf-8')
         self.elementstack[-1][2].append(text)
 
     def handle_data(self, text, escape=1):
@@ -686,11 +686,11 @@
             self.version = 'rss10'
         if loweruri == 'http://www.w3.org/2005/atom' and not self.version:
             self.version = 'atom10'
-        if loweruri.find('backend.userland.com/rss') <> -1:
+        if loweruri.find('backend.userland.com/rss') != -1:
             # match any backend.userland.com namespace
             uri = 'http://backend.userland.com/rss'
             loweruri = uri
-        if self._matchnamespaces.has_key(loweruri):
+        if loweruri in self._matchnamespaces:
             self.namespacemap[prefix] = self._matchnamespaces[loweruri]
             self.namespacesInUse[self._matchnamespaces[loweruri]] = uri
         else:
@@ -796,23 +796,23 @@
             if element in self.can_contain_dangerous_markup:
                 output = _sanitizeHTML(output, self.encoding, self.contentparams.get('type', 'text/html'))
 
-        if self.encoding and type(output) != type(u''):
+        if self.encoding and type(output) != type(''):
             try:
-                output = unicode(output, self.encoding)
+                output = str(output, self.encoding)
             except:
                 pass
 
         # address common error where people take data that is already
         # utf-8, presume that it is iso-8859-1, and re-encode it.
-        if self.encoding=='utf-8' and type(output) == type(u''):
+        if self.encoding=='utf-8' and type(output) == type(''):
             try:
-                output = unicode(output.encode('iso-8859-1'), 'utf-8')
+                output = str(output.encode('iso-8859-1'), 'utf-8')
             except:
                 pass
 
         # map win-1252 extensions to the proper code points
-        if type(output) == type(u''):
-            output = u''.join([c in _cp1252.keys() and _cp1252[c] or c for c in output])
+        if type(output) == type(''):
+            output = ''.join([c in list(_cp1252.keys()) and _cp1252[c] or c for c in output])
 
         # categories/tags/keywords/whatever are handled in _end_category
         if element == 'category':
@@ -881,19 +881,17 @@
         if not (re.search(r'</(\w+)>',str) or re.search("&#?\w+;",str)): return
 
         # all tags must be in a restricted subset of valid HTML tags
-        if filter(lambda t: t.lower() not in _HTMLSanitizer.acceptable_elements,
-            re.findall(r'</?(\w+)',str)): return
+        if [t for t in re.findall(r'</?(\w+)',str) if t.lower() not in _HTMLSanitizer.acceptable_elements]: return
 
         # all entities must have been defined as valid HTML entities
-        from htmlentitydefs import entitydefs
-        if filter(lambda e: e not in entitydefs.keys(),
-            re.findall(r'&(\w+);',str)): return
+        from html.entities import entitydefs
+        if [e for e in re.findall(r'&(\w+);',str) if e not in list(entitydefs.keys())]: return
 
         return 1
 
     def _mapToStandardPrefix(self, name):
         colonpos = name.find(':')
-        if colonpos <> -1:
+        if colonpos != -1:
             prefix = name[:colonpos]
             suffix = name[colonpos+1:]
             prefix = self.namespacemap.get(prefix, prefix)
@@ -959,11 +957,11 @@
     _start_feedinfo = _start_channel
 
     def _cdf_common(self, attrsD):
-        if attrsD.has_key('lastmod'):
+        if 'lastmod' in attrsD:
             self._start_modified({})
             self.elementstack[-1][-1] = attrsD['lastmod']
             self._end_modified()
-        if attrsD.has_key('href'):
+        if 'href' in attrsD:
             self._start_link({})
             self.elementstack[-1][-1] = attrsD['href']
             self._end_link()
@@ -1131,7 +1129,7 @@
     def _getContext(self):
         if self.insource:
             context = self.sourcedata
-        elif self.inimage and self.feeddata.has_key('image'):
+        elif self.inimage and 'image' in self.feeddata:
             context = self.feeddata['image']
         elif self.intextinput:
             context = self.feeddata['textinput']
@@ -1365,14 +1363,14 @@
             attrsD.setdefault('type', 'text/html')
         context = self._getContext()
         attrsD = self._itsAnHrefDamnIt(attrsD)
-        if attrsD.has_key('href'):
+        if 'href' in attrsD:
             attrsD['href'] = self.resolveURI(attrsD['href'])
             if attrsD.get('rel')=='enclosure' and not context.get('id'):
                 context['id'] = attrsD.get('href')
         expectingText = self.infeed or self.inentry or self.insource
         context.setdefault('links', [])
         context['links'].append(FeedParserDict(attrsD))
-        if attrsD.has_key('href'):
+        if 'href' in attrsD:
             expectingText = 0
             if (attrsD.get('rel') == 'alternate') and (self.mapContentType(attrsD.get('type')) in self.html_types):
                 context['link'] = attrsD['href']
@@ -1391,14 +1389,14 @@
 
     def _end_guid(self):
         value = self.pop('id')
-        self._save('guidislink', self.guidislink and not self._getContext().has_key('link'))
+        self._save('guidislink', self.guidislink and 'link' not in self._getContext())
         if self.guidislink:
             # guid acts as link, but only if 'ispermalink' is not present or is 'true',
             # and only if the item doesn't already have a link element
             self._save('link', value)
 
     def _start_title(self, attrsD):
-        if self.svgOK: return self.unknown_starttag('title', attrsD.items())
+        if self.svgOK: return self.unknown_starttag('title', list(attrsD.items()))
         self.pushContent('title', attrsD, 'text/plain', self.infeed or self.inentry or self.insource)
     _start_dc_title = _start_title
     _start_media_title = _start_title
@@ -1418,7 +1416,7 @@
 
     def _start_description(self, attrsD):
         context = self._getContext()
-        if context.has_key('summary'):
+        if 'summary' in context:
             self._summaryKey = 'content'
             self._start_content(attrsD)
         else:
@@ -1448,7 +1446,7 @@
     def _start_generator(self, attrsD):
         if attrsD:
             attrsD = self._itsAnHrefDamnIt(attrsD)
-            if attrsD.has_key('href'):
+            if 'href' in attrsD:
                 attrsD['href'] = self.resolveURI(attrsD['href'])
         self._getContext()['generator_detail'] = FeedParserDict(attrsD)
         self.push('generator', 1)
@@ -1456,7 +1454,7 @@
     def _end_generator(self):
         value = self.pop('generator')
         context = self._getContext()
-        if context.has_key('generator_detail'):
+        if 'generator_detail' in context:
             context['generator_detail']['name'] = value
             
     def _start_admin_generatoragent(self, attrsD):
@@ -1476,7 +1474,7 @@
         
     def _start_summary(self, attrsD):
         context = self._getContext()
-        if context.has_key('summary'):
+        if 'summary' in context:
             self._summaryKey = 'content'
             self._start_content(attrsD)
         else:
@@ -1504,7 +1502,7 @@
     def _start_source(self, attrsD):
         if 'url' in attrsD:
           # This means that we're processing a source element from an RSS 2.0 feed
-          self.sourcedata['href'] = attrsD[u'url']
+          self.sourcedata['href'] = attrsD['url']
         self.push('source', 1)
         self.insource = 1
         self.hasTitle = 0
@@ -1575,7 +1573,7 @@
         url = self.pop('url')
         context = self._getContext()
         if url != None and len(url.strip()) != 0:
-            if not context['media_thumbnail'][-1].has_key('url'):
+            if 'url' not in context['media_thumbnail'][-1]:
                 context['media_thumbnail'][-1]['url'] = url
 
     def _start_media_player(self, attrsD):
@@ -1605,7 +1603,7 @@
         def startElementNS(self, name, qname, attrs):
             namespace, localname = name
             lowernamespace = str(namespace or '').lower()
-            if lowernamespace.find('backend.userland.com/rss') <> -1:
+            if lowernamespace.find('backend.userland.com/rss') != -1:
                 # match any backend.userland.com namespace
                 namespace = 'http://backend.userland.com/rss'
                 lowernamespace = namespace
@@ -1614,8 +1612,8 @@
             else:
                 givenprefix = None
             prefix = self._matchnamespaces.get(lowernamespace, givenprefix)
-            if givenprefix and (prefix == None or (prefix == '' and lowernamespace == '')) and not self.namespacesInUse.has_key(givenprefix):
-                    raise UndeclaredNamespace, "'%s' is not associated with a namespace" % givenprefix
+            if givenprefix and (prefix == None or (prefix == '' and lowernamespace == '')) and givenprefix not in self.namespacesInUse:
+                    raise UndeclaredNamespace("'%s' is not associated with a namespace" % givenprefix)
             localname = str(localname).lower()
 
             # qname implementation is horribly broken in Python 2.1 (it
@@ -1634,13 +1632,13 @@
             if prefix:
                 localname = prefix.lower() + ':' + localname
             elif namespace and not qname: #Expat
-                for name,value in self.namespacesInUse.items():
+                for name,value in list(self.namespacesInUse.items()):
                      if name and value == namespace:
                          localname = name + ':' + localname
                          break
-            if _debug: sys.stderr.write('startElementNS: qname = %s, namespace = %s, givenprefix = %s, prefix = %s, attrs = %s, localname = %s\n' % (qname, namespace, givenprefix, prefix, attrs.items(), localname))
-
-            for (namespace, attrlocalname), attrvalue in attrs._attrs.items():
+            if _debug: sys.stderr.write('startElementNS: qname = %s, namespace = %s, givenprefix = %s, prefix = %s, attrs = %s, localname = %s\n' % (qname, namespace, givenprefix, prefix, list(attrs.items()), localname))
+
+            for (namespace, attrlocalname), attrvalue in list(attrs._attrs.items()):
                 lowernamespace = (namespace or '').lower()
                 prefix = self._matchnamespaces.get(lowernamespace, '')
                 if prefix:
@@ -1648,7 +1646,7 @@
                 attrsD[str(attrlocalname).lower()] = attrvalue
             for qname in attrs.getQNames():
                 attrsD[str(qname).lower()] = attrs.getValueByQName(qname)
-            self.unknown_starttag(localname, attrsD.items())
+            self.unknown_starttag(localname, list(attrsD.items()))
 
         def characters(self, text):
             self.handle_data(text)
@@ -1664,7 +1662,7 @@
             if prefix:
                 localname = prefix + ':' + localname
             elif namespace and not qname: #Expat
-                for name,value in self.namespacesInUse.items():
+                for name,value in list(self.namespacesInUse.items()):
                      if name and value == namespace:
                          localname = name + ':' + localname
                          break
@@ -1718,7 +1716,7 @@
         data = re.sub(r'<([^<>\s]+?)\s*/>', self._shorttag_replace, data) 
         data = data.replace('&#39;', "'")
         data = data.replace('&#34;', '"')
-        if self.encoding and type(data) == type(u''):
+        if self.encoding and type(data) == type(''):
             data = data.encode(self.encoding)
         sgmllib.SGMLParser.feed(self, data)
         sgmllib.SGMLParser.close(self)
@@ -1726,7 +1724,7 @@
     def normalize_attrs(self, attrs):
         if not attrs: return attrs
         # utility method to be called by descendants
-        attrs = dict([(k.lower(), v) for k, v in attrs]).items()
+        attrs = list(dict([(k.lower(), v) for k, v in attrs]).items())
         attrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]
         attrs.sort()
         return attrs
@@ -1743,13 +1741,13 @@
                 value=value.replace('>','&gt;').replace('<','&lt;').replace('"','&quot;')
                 value = self.bare_ampersand.sub("&amp;", value)
                 # thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds
-                if type(value) != type(u''):
+                if type(value) != type(''):
                     try:
-                        value = unicode(value, self.encoding)
+                        value = str(value, self.encoding)
                     except:
-                        value = unicode(value, 'iso-8859-1')
-                uattrs.append((unicode(key, self.encoding), value))
-            strattrs = u''.join([u' %s="%s"' % (key, value) for key, value in uattrs])
+                        value = str(value, 'iso-8859-1')
+                uattrs.append((str(key, self.encoding), value))
+            strattrs = ''.join([' %s="%s"' % (key, value) for key, value in uattrs])
             if self.encoding:
                 try:
                     strattrs=strattrs.encode(self.encoding)
@@ -1770,11 +1768,11 @@
         # called for each character reference, e.g. for '&#160;', ref will be '160'
         # Reconstruct the original character reference.
         if ref.startswith('x'):
-            value = unichr(int(ref[1:],16))
+            value = chr(int(ref[1:],16))
         else:
-            value = unichr(int(ref))
-
-        if value in _cp1252.keys():
+            value = chr(int(ref))
+
+        if value in list(_cp1252.keys()):
             self.pieces.append('&#%s;' % hex(ord(_cp1252[value]))[1:])
         else:
             self.pieces.append('&#%(ref)s;' % locals())
@@ -1782,7 +1780,7 @@
     def handle_entityref(self, ref):
         # called for each entity reference, e.g. for '&copy;', ref will be 'copy'
         # Reconstruct the original entity reference.
-        if name2codepoint.has_key(ref):
+        if ref in name2codepoint:
             self.pieces.append('&%(ref)s;' % locals())
         else:
             self.pieces.append('&amp;%(ref)s' % locals())
@@ -1859,7 +1857,7 @@
         data = data.replace('&#x22;', '&quot;')
         data = data.replace('&#39;', '&apos;')
         data = data.replace('&#x27;', '&apos;')
-        if self.contentparams.has_key('type') and not self.contentparams.get('type', 'xml').endswith('xml'):
+        if 'type' in self.contentparams and not self.contentparams.get('type', 'xml').endswith('xml'):
             data = data.replace('&lt;', '<')
             data = data.replace('&gt;', '>')
             data = data.replace('&amp;', '&')
@@ -1884,7 +1882,7 @@
         self.document = BeautifulSoup.BeautifulSoup(data)
         self.baseuri = baseuri
         self.encoding = encoding
-        if type(data) == type(u''):
+        if type(data) == type(''):
             data = data.encode(encoding)
         self.tags = []
         self.enclosures = []
@@ -1892,7 +1890,7 @@
         self.vcard = None
     
     def vcardEscape(self, s):
-        if type(s) in (type(''), type(u'')):
+        if type(s) in (type(''), type('')):
             s = s.replace(',', '\\,').replace(';', '\\;').replace('\n', '\\n')
         return s
     
@@ -2225,13 +2223,13 @@
     
     def isProbablyDownloadable(self, elm):
         attrsD = elm.attrMap
-        if not attrsD.has_key('href'): return 0
+        if 'href' not in attrsD: return 0
         linktype = attrsD.get('type', '').strip()
         if linktype.startswith('audio/') or \
            linktype.startswith('video/') or \
            (linktype.startswith('application/') and not linktype.endswith('xml')):
             return 1
-        path = urlparse.urlparse(attrsD['href'])[2]
+        path = urllib.parse.urlparse(attrsD['href'])[2]
         if path.find('.') == -1: return 0
         fileext = path.split('.').pop().lower()
         return fileext in self.known_binary_extensions
@@ -2242,12 +2240,12 @@
             href = elm.get('href')
             if not href: continue
             urlscheme, domain, path, params, query, fragment = \
-                       urlparse.urlparse(_urljoin(self.baseuri, href))
+                       urllib.parse.urlparse(_urljoin(self.baseuri, href))
             segments = path.split('/')
             tag = segments.pop()
             if not tag:
                 tag = segments.pop()
-            tagscheme = urlparse.urlunparse((urlscheme, domain, '/'.join(segments), '', '', ''))
+            tagscheme = urllib.parse.urlunparse((urlscheme, domain, '/'.join(segments), '', '', ''))
             if not tagscheme.endswith('/'):
                 tagscheme += '/'
             self.tags.append(FeedParserDict({"term": tag, "scheme": tagscheme, "label": elm.string or ''}))
@@ -2506,7 +2504,7 @@
 
         # declare xlink namespace, if needed
         if self.mathmlOK or self.svgOK:
-            if filter(lambda (n,v): n.startswith('xlink:'),attrs):
+            if [n_v for n_v in attrs if n_v[0].startswith('xlink:')]:
                 if not ('xmlns:xlink','http://www.w3.org/1999/xlink') in attrs:
                     attrs.append(('xmlns:xlink','http://www.w3.org/1999/xlink'))
 
@@ -2594,12 +2592,12 @@
             except:
                 pass
         if _tidy:
-            utf8 = type(data) == type(u'')
+            utf8 = type(data) == type('')
             if utf8:
                 data = data.encode('utf-8')
             data = _tidy(data, output_xhtml=1, numeric_entities=1, wrap=0, char_encoding="utf8")
             if utf8:
-                data = unicode(data, 'utf-8')
+                data = str(data, 'utf-8')
             if data.count('<body'):
                 data = data.split('<body', 1)[1]
                 if data.count('>'):
@@ -2609,7 +2607,7 @@
     data = data.strip().replace('\r\n', '\n')
     return data
 
-class _FeedURLHandler(urllib2.HTTPDigestAuthHandler, urllib2.HTTPRedirectHandler, urllib2.HTTPDefaultErrorHandler):
+class _FeedURLHandler(urllib.request.HTTPDigestAuthHandler, urllib.request.HTTPRedirectHandler, urllib.request.HTTPDefaultErrorHandler):
     def http_error_default(self, req, fp, code, msg, headers):
         if ((code / 100) == 3) and (code != 304):
             return self.http_error_302(req, fp, code, msg, headers)
@@ -2618,8 +2616,8 @@
         return infourl
 
     def http_error_302(self, req, fp, code, msg, headers):
-        if headers.dict.has_key('location'):
-            infourl = urllib2.HTTPRedirectHandler.http_error_302(self, req, fp, code, msg, headers)
+        if 'location' in headers.dict:
+            infourl = urllib.request.HTTPRedirectHandler.http_error_302(self, req, fp, code, msg, headers)
         else:
             infourl = urllib.addinfourl(fp, headers, req.get_full_url())
         if not hasattr(infourl, 'status'):
@@ -2627,8 +2625,8 @@
         return infourl
 
     def http_error_301(self, req, fp, code, msg, headers):
-        if headers.dict.has_key('location'):
-            infourl = urllib2.HTTPRedirectHandler.http_error_301(self, req, fp, code, msg, headers)
+        if 'location' in headers.dict:
+            infourl = urllib.request.HTTPRedirectHandler.http_error_301(self, req, fp, code, msg, headers)
         else:
             infourl = urllib.addinfourl(fp, headers, req.get_full_url())
         if not hasattr(infourl, 'status'):
@@ -2650,7 +2648,7 @@
         # header the server sent back (for the realm) and retry
         # the request with the appropriate digest auth headers instead.
         # This evil genius hack has been brought to you by Aaron Swartz.
-        host = urlparse.urlparse(req.get_full_url())[1]
+        host = urllib.parse.urlparse(req.get_full_url())[1]
         try:
             assert sys.version.split()[0] >= '2.3.3'
             assert base64 != None
@@ -2698,23 +2696,23 @@
     if url_file_stream_or_string == '-':
         return sys.stdin
 
-    if urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):
+    if urllib.parse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):
         if not agent:
             agent = USER_AGENT
         # test for inline user:password for basic auth
         auth = None
         if base64:
-            urltype, rest = urllib.splittype(url_file_stream_or_string)
-            realhost, rest = urllib.splithost(rest)
+            urltype, rest = urllib.parse.splittype(url_file_stream_or_string)
+            realhost, rest = urllib.parse.splithost(rest)
             if realhost:
-                user_passwd, realhost = urllib.splituser(realhost)
+                user_passwd, realhost = urllib.parse.splituser(realhost)
                 if user_passwd:
                     url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)
                     auth = base64.encodestring(user_passwd).strip()
 
         # iri support
         try:
-            if isinstance(url_file_stream_or_string,unicode):
+            if isinstance(url_file_stream_or_string,str):
                 url_file_stream_or_string = url_file_stream_or_string.encode('idna')
             else:
                 url_file_stream_or_string = url_file_stream_or_string.decode('utf-8').encode('idna')
@@ -2722,7 +2720,7 @@
             pass
 
         # try to open with urllib2 (to use optional headers)
-        request = urllib2.Request(url_file_stream_or_string)
+        request = urllib.request.Request(url_file_stream_or_string)
         request.add_header('User-Agent', agent)
         if etag:
             request.add_header('If-None-Match', etag)
@@ -2751,7 +2749,7 @@
         if ACCEPT_HEADER:
             request.add_header('Accept', ACCEPT_HEADER)
         request.add_header('A-IM', 'feed') # RFC 3229 support
-        opener = apply(urllib2.build_opener, tuple([_FeedURLHandler()] + handlers))
+        opener = urllib.request.build_opener(*tuple([_FeedURLHandler()] + handlers))
         opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent
         try:
             return opener.open(request)
@@ -2848,7 +2846,7 @@
         day = int(day)
     # special case of the century - is the first year of the 21st century
     # 2000 or 2001 ? The debate goes on...
-    if 'century' in params.keys():
+    if 'century' in list(params.keys()):
         year = (int(params['century']) - 1) * 100 + 1
     # in ISO 8601 most fields are optional
     for field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:
@@ -2880,17 +2878,17 @@
 registerDateHandler(_parse_date_iso8601)
     
 # 8-bit date handling routines written by ytrewq1.
-_korean_year  = u'\ub144' # b3e2 in euc-kr
-_korean_month = u'\uc6d4' # bff9 in euc-kr
-_korean_day   = u'\uc77c' # c0cf in euc-kr
-_korean_am    = u'\uc624\uc804' # bfc0 c0fc in euc-kr
-_korean_pm    = u'\uc624\ud6c4' # bfc0 c8c4 in euc-kr
+_korean_year  = '\ub144' # b3e2 in euc-kr
+_korean_month = '\uc6d4' # bff9 in euc-kr
+_korean_day   = '\uc77c' # c0cf in euc-kr
+_korean_am    = '\uc624\uc804' # bfc0 c0fc in euc-kr
+_korean_pm    = '\uc624\ud6c4' # bfc0 c8c4 in euc-kr
 
 _korean_onblog_date_re = \
     re.compile('(\d{4})%s\s+(\d{2})%s\s+(\d{2})%s\s+(\d{2}):(\d{2}):(\d{2})' % \
                (_korean_year, _korean_month, _korean_day))
 _korean_nate_date_re = \
-    re.compile(u'(\d{4})-(\d{2})-(\d{2})\s+(%s|%s)\s+(\d{,2}):(\d{,2}):(\d{,2})' % \
+    re.compile('(\d{4})-(\d{2})-(\d{2})\s+(%s|%s)\s+(\d{,2}):(\d{,2}):(\d{,2})' % \
                (_korean_am, _korean_pm))
 def _parse_date_onblog(dateString):
     '''Parse a string according to the OnBlog 8-bit date format'''
@@ -2940,40 +2938,40 @@
 # Unicode strings for Greek date strings
 _greek_months = \
   { \
-   u'\u0399\u03b1\u03bd': u'Jan',       # c9e1ed in iso-8859-7
-   u'\u03a6\u03b5\u03b2': u'Feb',       # d6e5e2 in iso-8859-7
-   u'\u039c\u03ac\u03ce': u'Mar',       # ccdcfe in iso-8859-7
-   u'\u039c\u03b1\u03ce': u'Mar',       # cce1fe in iso-8859-7
-   u'\u0391\u03c0\u03c1': u'Apr',       # c1f0f1 in iso-8859-7
-   u'\u039c\u03ac\u03b9': u'May',       # ccdce9 in iso-8859-7
-   u'\u039c\u03b1\u03ca': u'May',       # cce1fa in iso-8859-7
-   u'\u039c\u03b1\u03b9': u'May',       # cce1e9 in iso-8859-7
-   u'\u0399\u03bf\u03cd\u03bd': u'Jun', # c9effded in iso-8859-7
-   u'\u0399\u03bf\u03bd': u'Jun',       # c9efed in iso-8859-7
-   u'\u0399\u03bf\u03cd\u03bb': u'Jul', # c9effdeb in iso-8859-7
-   u'\u0399\u03bf\u03bb': u'Jul',       # c9f9eb in iso-8859-7
-   u'\u0391\u03cd\u03b3': u'Aug',       # c1fde3 in iso-8859-7
-   u'\u0391\u03c5\u03b3': u'Aug',       # c1f5e3 in iso-8859-7
-   u'\u03a3\u03b5\u03c0': u'Sep',       # d3e5f0 in iso-8859-7
-   u'\u039f\u03ba\u03c4': u'Oct',       # cfeaf4 in iso-8859-7
-   u'\u039d\u03bf\u03ad': u'Nov',       # cdefdd in iso-8859-7
-   u'\u039d\u03bf\u03b5': u'Nov',       # cdefe5 in iso-8859-7
-   u'\u0394\u03b5\u03ba': u'Dec',       # c4e5ea in iso-8859-7
+   '\u0399\u03b1\u03bd': 'Jan',       # c9e1ed in iso-8859-7
+   '\u03a6\u03b5\u03b2': 'Feb',       # d6e5e2 in iso-8859-7
+   '\u039c\u03ac\u03ce': 'Mar',       # ccdcfe in iso-8859-7
+   '\u039c\u03b1\u03ce': 'Mar',       # cce1fe in iso-8859-7
+   '\u0391\u03c0\u03c1': 'Apr',       # c1f0f1 in iso-8859-7
+   '\u039c\u03ac\u03b9': 'May',       # ccdce9 in iso-8859-7
+   '\u039c\u03b1\u03ca': 'May',       # cce1fa in iso-8859-7
+   '\u039c\u03b1\u03b9': 'May',       # cce1e9 in iso-8859-7
+   '\u0399\u03bf\u03cd\u03bd': 'Jun', # c9effded in iso-8859-7
+   '\u0399\u03bf\u03bd': 'Jun',       # c9efed in iso-8859-7
+   '\u0399\u03bf\u03cd\u03bb': 'Jul', # c9effdeb in iso-8859-7
+   '\u0399\u03bf\u03bb': 'Jul',       # c9f9eb in iso-8859-7
+   '\u0391\u03cd\u03b3': 'Aug',       # c1fde3 in iso-8859-7
+   '\u0391\u03c5\u03b3': 'Aug',       # c1f5e3 in iso-8859-7
+   '\u03a3\u03b5\u03c0': 'Sep',       # d3e5f0 in iso-8859-7
+   '\u039f\u03ba\u03c4': 'Oct',       # cfeaf4 in iso-8859-7
+   '\u039d\u03bf\u03ad': 'Nov',       # cdefdd in iso-8859-7
+   '\u039d\u03bf\u03b5': 'Nov',       # cdefe5 in iso-8859-7
+   '\u0394\u03b5\u03ba': 'Dec',       # c4e5ea in iso-8859-7
   }
 
 _greek_wdays = \
   { \
-   u'\u039a\u03c5\u03c1': u'Sun', # caf5f1 in iso-8859-7
-   u'\u0394\u03b5\u03c5': u'Mon', # c4e5f5 in iso-8859-7
-   u'\u03a4\u03c1\u03b9': u'Tue', # d4f1e9 in iso-8859-7
-   u'\u03a4\u03b5\u03c4': u'Wed', # d4e5f4 in iso-8859-7
-   u'\u03a0\u03b5\u03bc': u'Thu', # d0e5ec in iso-8859-7
-   u'\u03a0\u03b1\u03c1': u'Fri', # d0e1f1 in iso-8859-7
-   u'\u03a3\u03b1\u03b2': u'Sat', # d3e1e2 in iso-8859-7   
+   '\u039a\u03c5\u03c1': 'Sun', # caf5f1 in iso-8859-7
+   '\u0394\u03b5\u03c5': 'Mon', # c4e5f5 in iso-8859-7
+   '\u03a4\u03c1\u03b9': 'Tue', # d4f1e9 in iso-8859-7
+   '\u03a4\u03b5\u03c4': 'Wed', # d4e5f4 in iso-8859-7
+   '\u03a0\u03b5\u03bc': 'Thu', # d0e5ec in iso-8859-7
+   '\u03a0\u03b1\u03c1': 'Fri', # d0e1f1 in iso-8859-7
+   '\u03a3\u03b1\u03b2': 'Sat', # d3e1e2 in iso-8859-7   
   }
 
 _greek_date_format_re = \
-    re.compile(u'([^,]+),\s+(\d{2})\s+([^\s]+)\s+(\d{4})\s+(\d{2}):(\d{2}):(\d{2})\s+([^\s]+)')
+    re.compile('([^,]+),\s+(\d{2})\s+([^\s]+)\s+(\d{4})\s+(\d{2}):(\d{2}):(\d{2})\s+([^\s]+)')
 
 def _parse_date_greek(dateString):
     '''Parse a string according to a Greek 8-bit date format.'''
@@ -2995,22 +2993,22 @@
 # Unicode strings for Hungarian date strings
 _hungarian_months = \
   { \
-    u'janu\u00e1r':   u'01',  # e1 in iso-8859-2
-    u'febru\u00e1ri': u'02',  # e1 in iso-8859-2
-    u'm\u00e1rcius':  u'03',  # e1 in iso-8859-2
-    u'\u00e1prilis':  u'04',  # e1 in iso-8859-2
-    u'm\u00e1ujus':   u'05',  # e1 in iso-8859-2
-    u'j\u00fanius':   u'06',  # fa in iso-8859-2
-    u'j\u00falius':   u'07',  # fa in iso-8859-2
-    u'augusztus':     u'08',
-    u'szeptember':    u'09',
-    u'okt\u00f3ber':  u'10',  # f3 in iso-8859-2
-    u'november':      u'11',
-    u'december':      u'12',
+    'janu\u00e1r':   '01',  # e1 in iso-8859-2
+    'febru\u00e1ri': '02',  # e1 in iso-8859-2
+    'm\u00e1rcius':  '03',  # e1 in iso-8859-2
+    '\u00e1prilis':  '04',  # e1 in iso-8859-2
+    'm\u00e1ujus':   '05',  # e1 in iso-8859-2
+    'j\u00fanius':   '06',  # fa in iso-8859-2
+    'j\u00falius':   '07',  # fa in iso-8859-2
+    'augusztus':     '08',
+    'szeptember':    '09',
+    'okt\u00f3ber':  '10',  # f3 in iso-8859-2
+    'november':      '11',
+    'december':      '12',
   }
 
 _hungarian_date_format_re = \
-  re.compile(u'(\d{4})-([^-]+)-(\d{,2})T(\d{,2}):(\d{2})((\+|-)(\d{,2}:\d{2}))')
+  re.compile('(\d{4})-([^-]+)-(\d{,2})T(\d{,2}):(\d{2})((\+|-)(\d{,2}:\d{2}))')
 
 def _parse_date_hungarian(dateString):
     '''Parse a string according to a Hungarian 8-bit date format.'''
@@ -3181,9 +3179,9 @@
             if len(date9tuple) != 9:
                 if _debug: sys.stderr.write('date handler function must return 9-tuple\n')
                 raise ValueError
-            map(int, date9tuple)
+            list(map(int, date9tuple))
             return date9tuple
-        except Exception, e:
+        except Exception as e:
             if _debug: sys.stderr.write('%s raised %s\n' % (handler.__name__, repr(e)))
             pass
     return None
@@ -3262,39 +3260,39 @@
         elif xml_data[:4] == '\x00\x3c\x00\x3f':
             # UTF-16BE
             sniffed_xml_encoding = 'utf-16be'
-            xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')
+            xml_data = str(xml_data, 'utf-16be').encode('utf-8')
         elif (len(xml_data) >= 4) and (xml_data[:2] == '\xfe\xff') and (xml_data[2:4] != '\x00\x00'):
             # UTF-16BE with BOM
             sniffed_xml_encoding = 'utf-16be'
-            xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')
+            xml_data = str(xml_data[2:], 'utf-16be').encode('utf-8')
         elif xml_data[:4] == '\x3c\x00\x3f\x00':
             # UTF-16LE
             sniffed_xml_encoding = 'utf-16le'
-            xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')
+            xml_data = str(xml_data, 'utf-16le').encode('utf-8')
         elif (len(xml_data) >= 4) and (xml_data[:2] == '\xff\xfe') and (xml_data[2:4] != '\x00\x00'):
             # UTF-16LE with BOM
             sniffed_xml_encoding = 'utf-16le'
-            xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')
+            xml_data = str(xml_data[2:], 'utf-16le').encode('utf-8')
         elif xml_data[:4] == '\x00\x00\x00\x3c':
             # UTF-32BE
             sniffed_xml_encoding = 'utf-32be'
-            xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')
+            xml_data = str(xml_data, 'utf-32be').encode('utf-8')
         elif xml_data[:4] == '\x3c\x00\x00\x00':
             # UTF-32LE
             sniffed_xml_encoding = 'utf-32le'
-            xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')
+            xml_data = str(xml_data, 'utf-32le').encode('utf-8')
         elif xml_data[:4] == '\x00\x00\xfe\xff':
             # UTF-32BE with BOM
             sniffed_xml_encoding = 'utf-32be'
-            xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')
+            xml_data = str(xml_data[4:], 'utf-32be').encode('utf-8')
         elif xml_data[:4] == '\xff\xfe\x00\x00':
             # UTF-32LE with BOM
             sniffed_xml_encoding = 'utf-32le'
-            xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')
+            xml_data = str(xml_data[4:], 'utf-32le').encode('utf-8')
         elif xml_data[:3] == '\xef\xbb\xbf':
             # UTF-8 with BOM
             sniffed_xml_encoding = 'utf-8'
-            xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')
+            xml_data = str(xml_data[3:], 'utf-8').encode('utf-8')
         else:
             # ASCII-compatible
             pass
@@ -3318,7 +3316,7 @@
         true_encoding = http_encoding or 'us-ascii'
     elif http_content_type.startswith('text/'):
         true_encoding = http_encoding or 'us-ascii'
-    elif http_headers and (not http_headers.has_key('content-type')):
+    elif http_headers and ('content-type' not in http_headers):
         true_encoding = xml_encoding or 'iso-8859-1'
     else:
         true_encoding = xml_encoding or 'utf-8'
@@ -3371,14 +3369,14 @@
                 sys.stderr.write('trying utf-32le instead\n')
         encoding = 'utf-32le'
         data = data[4:]
-    newdata = unicode(data, encoding)
+    newdata = str(data, encoding)
     if _debug: sys.stderr.write('successfully converted %s data to unicode\n' % encoding)
     declmatch = re.compile('^<\?xml[^>]*?>')
     newdecl = '''<?xml version='1.0' encoding='utf-8'?>'''
     if declmatch.search(newdata):
         newdata = declmatch.sub(newdecl, newdata)
     else:
-        newdata = newdecl + u'\n' + newdata
+        newdata = newdecl + '\n' + newdata
     return newdata.encode('utf-8')
 
 def _stripDoctype(data):
@@ -3406,7 +3404,7 @@
     replacement=''
     if len(doctype_results)==1 and entity_results:
        safe_pattern=re.compile('\s+(\w+)\s+"(&#\w+;|[^&"]*)"')
-       safe_entities=filter(lambda e: safe_pattern.match(e),entity_results)
+       safe_entities=[e for e in entity_results if safe_pattern.match(e)]
        if safe_entities:
            replacement='<!DOCTYPE feed [\n  <!ENTITY %s>\n]>' % '>\n  <!ENTITY '.join(safe_entities)
     data = doctype_pattern.sub(replacement, head) + data
@@ -3425,7 +3423,7 @@
     try:
         f = _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers)
         data = f.read()
-    except Exception, e:
+    except Exception as e:
         result['bozo'] = 1
         result['bozo_exception'] = e
         data = None
@@ -3436,7 +3434,7 @@
         if gzip and f.headers.get('content-encoding', '') == 'gzip':
             try:
                 data = gzip.GzipFile(fileobj=_StringIO(data)).read()
-            except Exception, e:
+            except Exception as e:
                 # Some feeds claim to be gzipped but they're not, so
                 # we get garbage.  Ideally, we should re-request the
                 # feed without the 'Accept-encoding: gzip' header,
@@ -3447,7 +3445,7 @@
         elif zlib and f.headers.get('content-encoding', '') == 'deflate':
             try:
                 data = zlib.decompress(data, -zlib.MAX_WBITS)
-            except Exception, e:
+            except Exception as e:
                 result['bozo'] = 1
                 result['bozo_exception'] = e
                 data = ''
@@ -3480,7 +3478,7 @@
     result['encoding'], http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type = \
         _getCharacterEncoding(http_headers, data)
     if http_headers and (not acceptable_content_type):
-        if http_headers.has_key('content-type'):
+        if 'content-type' in http_headers:
             bozo_message = '%s is not an XML media type' % http_headers['content-type']
         else:
             bozo_message = 'no Content-type specified'
@@ -3588,7 +3586,7 @@
             saxparser._ns_stack.append({'http://www.w3.org/XML/1998/namespace':'xml'})
         try:
             saxparser.parse(source)
-        except Exception, e:
+        except Exception as e:
             if _debug:
                 import traceback
                 traceback.print_stack()
@@ -3617,14 +3615,14 @@
     def _writer(self, stream, node, prefix):
         if not node: return
         if hasattr(node, 'keys'):
-            keys = node.keys()
+            keys = list(node.keys())
             keys.sort()RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/feedparser.py to planet.archlinux.org3/planet/vendor/feedparser.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/htmltmpl.py

             for k in keys:
                 if k in ('description', 'link'): continue
-                if node.has_key(k + '_detail'): continue
-                if node.has_key(k + '_parsed'): continue
+                if k + '_detail' in node: continue
+                if k + '_parsed' in node: continue
                 self._writer(stream, node[k], prefix + k + '.')
-        elif type(node) == types.ListType:
+        elif type(node) == list:
             index = 0
             for n in node:
                 self._writer(stream, n, prefix[:-1] + '[' + str(index) + '].')
@@ -3644,7 +3642,7 @@
         
 class PprintSerializer(Serializer):
     def write(self, stream=sys.stdout):
-        if self.results.has_key('href'):
+        if 'href' in self.results:
             stream.write(self.results['href'] + '\n\n')
         from pprint import pprint
         pprint(self.results, stream)
@@ -3673,7 +3671,7 @@
             sys.exit(0)
     else:
         if not sys.argv[1:]:
-            print __doc__
+            print(__doc__)
             sys.exit(0)
         class _Options:
             etag = modified = agent = referrer = None
--- planet.archlinux.org/planet/vendor/htmltmpl.py	(original)
+++ planet.archlinux.org/planet/vendor/htmltmpl.py	(refactored)
@@ -41,8 +41,8 @@
 import sys
 import copy
 import cgi          # for HTML escaping of variables
-import urllib       # for URL escaping of variables
-import cPickle      # for template compilation
+import urllib.request, urllib.parse, urllib.error       # for URL escaping of variables
+import pickle      # for template compilation
 import gettext
 import portalocker  # for locking
 
@@ -170,9 +170,9 @@
             if self.is_precompiled(file):
                 try:
                     precompiled = self.load_precompiled(file)
-                except PrecompiledError, template:
-                    print >> sys.stderr, "Htmltmpl: bad precompiled "\
-                                         "template '%s' removed" % template
+                except PrecompiledError as template:
+                    print("Htmltmpl: bad precompiled "\
+                                         "template '%s' removed" % template, file=sys.stderr)
                     compiled = self.compile(file)
                     self.save_precompiled(compiled)
                 else:
@@ -226,7 +226,7 @@
         """ Print debugging message to stderr if debugging is enabled. 
             @hidden
         """
-        if self._debug: print >> sys.stderr, str
+        if self._debug: print(str, file=sys.stderr)
 
     def compile(self, file):
         """ Compile the template.
@@ -264,14 +264,15 @@
             try:
                 file = open(filename, "rb")
                 portalocker.lock(file, portalocker.LOCK_SH)
-                precompiled = cPickle.load(file)
-            except IOError, (errno, errstr):
-                raise TemplateError, "IO error in load precompiled "\
+                precompiled = pickle.load(file)
+            except IOError as xxx_todo_changeme:
+                (errno, errstr) = xxx_todo_changeme.args
+                raise TemplateError("IO error in load precompiled "\
                                      "template '%s': (%d) %s"\
-                                     % (filename, errno, errstr)
-            except cPickle.UnpicklingError:
+                                     % (filename, errno, errstr))
+            except pickle.UnpicklingError:
                 remove_bad = 1
-                raise PrecompiledError, filename
+                raise PrecompiledError(filename)
             except:
                 remove_bad = 1
                 raise
@@ -302,9 +303,9 @@
         # Check if we have write permission to the template's directory.
         template_dir = os.path.dirname(os.path.abspath(filename))
         if not os.access(template_dir, os.W_OK):
-            raise TemplateError, "Cannot save precompiled templates "\
+            raise TemplateError("Cannot save precompiled templates "\
                                  "to '%s': write permission denied."\
-                                 % template_dir
+                                 % template_dir)
         try:
             remove_bad = 0
             file = None
@@ -314,19 +315,20 @@
                 BINARY = 1
                 READABLE = 0
                 if self._debug:
-                    cPickle.dump(template, file, READABLE)
+                    pickle.dump(template, file, READABLE)
                 else:
-                    cPickle.dump(template, file, BINARY)
-            except IOError, (errno, errstr):
+                    pickle.dump(template, file, BINARY)
+            except IOError as xxx_todo_changeme1:
+                (errno, errstr) = xxx_todo_changeme1.args
                 remove_bad = 1
-                raise TemplateError, "IO error while saving precompiled "\
+                raise TemplateError("IO error while saving precompiled "\
                                      "template '%s': (%d) %s"\
-                                      % (filename, errno, errstr)
-            except cPickle.PicklingError, error:
+                                      % (filename, errno, errstr))
+            except pickle.PicklingError as error:
                 remove_bad = 1
-                raise TemplateError, "Pickling error while saving "\
+                raise TemplateError("Pickling error while saving "\
                                      "precompiled template '%s': %s"\
-                                     % (filename, error)
+                                     % (filename, error))
             except:
                 remove_bad = 1
                 raise
@@ -431,14 +433,14 @@
         if self.is_ordinary_var(value):
             # template top-level ordinary variable
             if not var.islower():
-                raise TemplateError, "Invalid variable name '%s'." % var
+                raise TemplateError("Invalid variable name '%s'." % var)
         elif type(value) == ListType:
             # template top-level loop
             if var != var.capitalize():
-                raise TemplateError, "Invalid loop name '%s'." % var
+                raise TemplateError("Invalid loop name '%s'." % var)
         else:
-            raise TemplateError, "Value of toplevel variable '%s' must "\
-                                 "be either a scalar or a list." % var
+            raise TemplateError("Value of toplevel variable '%s' must "\
+                                 "be either a scalar or a list." % var)
         self._vars[var] = value
         self.DEB("VALUE SET: " + str(var))
         
@@ -493,7 +495,7 @@
         self.DEB("APP INPUT:")
         if self._debug: pprint.pprint(self._vars, sys.stderr)
         if part != None and (part == 0 or part < self._current_part):
-            raise TemplateError, "process() - invalid part number"
+            raise TemplateError("process() - invalid part number")
 
         # This flag means "jump behind the end of current statement" or
         # "skip the parameters of current statement".
@@ -536,7 +538,7 @@
                     # TMPL_VARs should be first. They are the most common.
                     var = tokens[i + PARAM_NAME]
                     if not var:
-                        raise TemplateError, "No identifier in <TMPL_VAR>."
+                        raise TemplateError("No identifier in <TMPL_VAR>.")
                     escape = tokens[i + PARAM_ESCAPE]
                     globalp = tokens[i + PARAM_GLOBAL]
                     skip_params = 1
@@ -552,7 +554,7 @@
                 elif token == "<TMPL_LOOP":
                     var = tokens[i + PARAM_NAME]
                     if not var:
-                        raise TemplateError, "No identifier in <TMPL_LOOP>."
+                        raise TemplateError("No identifier in <TMPL_LOOP>.")
                     skip_params = 1
 
                     # Find total number of passes in this loop.
@@ -579,7 +581,7 @@
                 elif token == "<TMPL_IF":
                     var = tokens[i + PARAM_NAME]
                     if not var:
-                        raise TemplateError, "No identifier in <TMPL_IF>."
+                        raise TemplateError("No identifier in <TMPL_IF>.")
                     globalp = tokens[i + PARAM_GLOBAL]
                     skip_params = 1
                     if self.find_value(var, loop_name, loop_pass,
@@ -593,7 +595,7 @@
                 elif token == "<TMPL_UNLESS":
                     var = tokens[i + PARAM_NAME]
                     if not var:
-                        raise TemplateError, "No identifier in <TMPL_UNLESS>."
+                        raise TemplateError("No identifier in <TMPL_UNLESS>.")
                     globalp = tokens[i + PARAM_GLOBAL]
                     skip_params = 1
                     if self.find_value(var, loop_name, loop_pass,
@@ -607,7 +609,7 @@
                 elif token == "</TMPL_LOOP":
                     skip_params = 1
                     if not loop_name:
-                        raise TemplateError, "Unmatched </TMPL_LOOP>."
+                        raise TemplateError("Unmatched </TMPL_LOOP>.")
                     
                     # If this loop was not disabled, then record the pass.
                     if loop_total[-1] > 0: loop_pass[-1] += 1
@@ -630,21 +632,21 @@
                 elif token == "</TMPL_IF":
                     skip_params = 1
                     if not output_control:
-                        raise TemplateError, "Unmatched </TMPL_IF>."
+                        raise TemplateError("Unmatched </TMPL_IF>.")
                     output_control.pop()
                     self.DEB("IF: END")
      
                 elif token == "</TMPL_UNLESS":
                     skip_params = 1
                     if not output_control:
-                        raise TemplateError, "Unmatched </TMPL_UNLESS>."
+                        raise TemplateError("Unmatched </TMPL_UNLESS>.")
                     output_control.pop()
                     self.DEB("UNLESS: END")
      
                 elif token == "<TMPL_ELSE":
                     skip_params = 1
                     if not output_control:
-                        raise TemplateError, "Unmatched <TMPL_ELSE>."
+                        raise TemplateError("Unmatched <TMPL_ELSE>.")
                     if output_control[-1] == DISABLE_OUTPUT:
                         # Condition was false, activate the ELSE block.
                         output_control[-1] = ENABLE_OUTPUT
@@ -654,7 +656,7 @@
                         output_control[-1] = DISABLE_OUTPUT
                         self.DEB("ELSE: DISABLE")
                     else:
-                        raise TemplateError, "BUG: ELSE: INVALID FLAG"
+                        raise TemplateError("BUG: ELSE: INVALID FLAG")
 
                 elif token == "<TMPL_BOUNDARY":
                     if part and part == self._current_part:
@@ -691,7 +693,7 @@
                     
                 else:
                     # Unknown processing directive.
-                    raise TemplateError, "Invalid statement %s>." % token
+                    raise TemplateError("Invalid statement %s>." % token)
                      
             elif DISABLE_OUTPUT not in output_control:
                 # Raw textual template data.
@@ -703,8 +705,8 @@
             # end of the big while loop
         
         # Check whether all opening statements were closed.
-        if loop_name: raise TemplateError, "Missing </TMPL_LOOP>."
-        if output_control: raise TemplateError, "Missing </TMPL_IF> or </TMPL_UNLESS>"
+        if loop_name: raise TemplateError("Missing </TMPL_LOOP>.")
+        if output_control: raise TemplateError("Missing </TMPL_IF> or </TMPL_UNLESS>")
         return out
 
     ##############################################
@@ -715,7 +717,7 @@
         """ Print debugging message to stderr if debugging is enabled.
             @hidden
         """
-        if self._debug: print >> sys.stderr, str
+        if self._debug: print(str, file=sys.stderr)
 
     def find_value(self, var, loop_name, loop_pass, loop_total,
                    global_override=None):
@@ -745,17 +747,17 @@
         for i in range(len(loop_name)):            
             # If global lookup is on then push the value on the stack.
             if ((self._global_vars and global_override != "0") or \
-                 global_override == "1") and scope.has_key(var) and \
+                 global_override == "1") and var in scope and \
                self.is_ordinary_var(scope[var]):
                 globals.append(scope[var])
             
             # Descent deeper into the hierarchy.
-            if scope.has_key(loop_name[i]) and scope[loop_name[i]]:
+            if loop_name[i] in scope and scope[loop_name[i]]:
                 scope = scope[loop_name[i]][loop_pass[i]]
             else:
                 return ""
             
-        if scope.has_key(var):
+        if var in scope:
             # Value exists in current loop.
             if type(scope[var]) == ListType:
                 # The requested value is a loop.
@@ -823,12 +825,12 @@
                 try:
                     every = int(var[9:])   # nine is length of "__EVERY__"
                 except ValueError:
-                    raise TemplateError, "Magic variable __EVERY__x: "\
-                                         "Invalid pass number."
+                    raise TemplateError("Magic variable __EVERY__x: "\
+                                         "Invalid pass number.")
                 else:
                     if not every:
-                        raise TemplateError, "Magic variable __EVERY__x: "\
-                                             "Pass number cannot be zero."
+                        raise TemplateError("Magic variable __EVERY__x: "\
+                                             "Pass number cannot be zero.")
                     elif (loop_pass + 1) % every == 0:
                         self.DEB("MAGIC: EVERY: " + str(every))
                         return 1
@@ -837,7 +839,7 @@
             else:
                 return 0
         else:
-            raise TemplateError, "Invalid magic variable '%s'." % var
+            raise TemplateError("Invalid magic variable '%s'." % var)
 
     def escape(self, str, override=""):
         """ Escape a string either by HTML escaping or by URL escaping.
@@ -848,7 +850,7 @@
             override != "URL") or override == "HTML" or override == "1":
             return cgi.escape(str, ESCAPE_QUOTES)
         elif override == "URL":
-            return urllib.quote_plus(str)
+            return urllib.parse.quote_plus(str)
         else:
             return str
 
@@ -961,7 +963,7 @@
         """ Print debugging message to stderr if debugging is enabled.
             @hidden
         """
-        if self._debug: print >> sys.stderr, str
+        if self._debug: print(str, file=sys.stderr)
     
     def read(self, filename):
         """ Read content of file and return it. Raise an error if a problem
@@ -974,9 +976,10 @@
             try:
                 f = open(filename, "r")
                 data = f.read()
-            except IOError, (errno, errstr):
-                raise TemplateError, "IO error while reading template '%s': "\
-                                     "(%d) %s" % (filename, errno, errstr)
+            except IOError as xxx_todo_changeme2:
+                (errno, errstr) = xxx_todo_changeme2.args
+                raise TemplateError("IO error while reading template '%s': "\
+                                     "(%d) %s" % (filename, errno, errstr))
             else:
                 return data
         finally:
@@ -1027,7 +1030,7 @@
             if token == "<TMPL_INCLUDE":
                 filename = tokens[i + PARAM_NAME]
                 if not filename:
-                    raise TemplateError, "No filename in <TMPL_INCLUDE>."
+                    raise TemplateError("No filename in <TMPL_INCLUDE>.")
                 self._include_level += 1
                 if self._include_level > self._max_include:
                     # Do not include the template.
@@ -1215,7 +1218,7 @@
         for pair in params:
             name, value = pair.split("=")
             if not name or not value:
-                raise TemplateError, "Syntax error in template."RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/htmltmpl.py to planet.archlinux.org3/planet/vendor/htmltmpl.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/portalocker.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/portalocker.py to planet.archlinux.org3/planet/vendor/portalocker.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/timeoutsocket.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/timeoutsocket.py to planet.archlinux.org3/planet/vendor/timeoutsocket.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/compat_logging/__init__.py

+                raise TemplateError("Syntax error in template.")
             if name == param:
                 if value[0] == '"':
                     # The value is in double quotes.
@@ -1270,15 +1273,15 @@
         if os.path.isfile(file):
             self._mtime = os.path.getmtime(file)
         else:
-            raise TemplateError, "Template: file does not exist: '%s'" % file
+            raise TemplateError("Template: file does not exist: '%s'" % file)
 
         # Save modificaton times of all included template files.
         for inc_file in include_files:
             if os.path.isfile(inc_file):
                 self._include_mtimes[inc_file] = os.path.getmtime(inc_file)
             else:
-                raise TemplateError, "Template: file does not exist: '%s'"\
-                                     % inc_file
+                raise TemplateError("Template: file does not exist: '%s'"\
+                                     % inc_file)
             
         self.DEB("NEW TEMPLATE CREATED")
 
@@ -1322,7 +1325,7 @@
             return 0        
 
         # Included templates.
-        for inc_file in self._include_mtimes.keys():
+        for inc_file in list(self._include_mtimes.keys()):
             if not (os.path.isfile(inc_file) and \
                     self._include_mtimes[inc_file] == \
                     os.path.getmtime(inc_file)):
@@ -1376,7 +1379,7 @@
         """ Print debugging message to stderr.
             @hidden
         """
-        if self._debug: print >> sys.stderr, str
+        if self._debug: print(str, file=sys.stderr)
 
 
 ##############################################
--- planet.archlinux.org/planet/vendor/portalocker.py	(original)
+++ planet.archlinux.org/planet/vendor/portalocker.py	(refactored)
@@ -86,7 +86,7 @@
 	timestamp = strftime("%m/%d/%Y %H:%M:%S\n", localtime(time()))
 	log.write( timestamp )
 
-	print "Wrote lines. Hit enter to release lock."
+	print("Wrote lines. Hit enter to release lock.")
 	dummy = sys.stdin.readline()
 
 	log.close()
--- planet.archlinux.org/planet/vendor/timeoutsocket.py	(original)
+++ planet.archlinux.org/planet/vendor/timeoutsocket.py	(refactored)
@@ -189,7 +189,7 @@
         errcode = 0
         try:
             self.connect(addr)
-        except Error, why:
+        except Error as why:
             errcode = why[0]
         return errcode
     # end connect_ex
@@ -209,7 +209,7 @@
             sock.connect(addr)
             sock.setblocking(blocking)
             return
-        except Error, why:
+        except Error as why:
             # Set the socket's blocking mode back
             sock.setblocking(blocking)
             
@@ -255,7 +255,7 @@
             timeoutnewsock = self.__class__(newsock, timeout)
             timeoutnewsock.setblocking(blocking)
             return (timeoutnewsock, addr)
-        except Error, why:
+        except Error as why:
             # Set the socket's blocking mode back
             sock.setblocking(blocking)
 
--- planet.archlinux.org/planet/vendor/compat_logging/__init__.py	(original)
+++ planet.archlinux.org/planet/vendor/compat_logging/__init__.py	(refactored)
@@ -26,10 +26,10 @@
 To use, simply 'import logging' and log away!
 """
 
-import sys, os, types, time, string, cStringIO
+import sys, os, types, time, string, io
 
 try:
-    import thread
+    import _thread
     import threading
 except ImportError:
     thread = None
@@ -200,10 +200,10 @@
         self.exc_info = exc_info
         self.lineno = lineno
         self.created = ct
-        self.msecs = (ct - long(ct)) * 1000
+        self.msecs = (ct - int(ct)) * 1000
         self.relativeCreated = (self.created - _startTime) * 1000
         if thread:
-            self.thread = thread.get_ident()
+            self.thread = _thread.get_ident()
         else:
             self.thread = None
         if hasattr(os, 'getpid'):
@@ -338,7 +338,7 @@
         traceback.print_exception()
         """
         import traceback
-        sio = cStringIO.StringIO()
+        sio = io.StringIO()
         traceback.print_exception(ei[0], ei[1], ei[2], None, sio)
         s = sio.getvalue()
         sio.close()
@@ -529,7 +529,7 @@
         Acquire a thread lock for serializing access to the underlying I/O.
         """
         if thread:
-            self.lock = thread.allocate_lock()
+            self.lock = _thread.allocate_lock()
         else:
             self.lock = None
 
@@ -573,8 +573,8 @@
         This version is intended to be implemented by subclasses and so
         raises a NotImplementedError.
         """
-        raise NotImplementedError, 'emit must be implemented '\
-                                    'by Handler subclasses'
+        raise NotImplementedError('emit must be implemented '\
+                                    'by Handler subclasses')
 
     def handle(self, record):
         """
@@ -737,8 +737,8 @@
     """
     if klass != Logger:
         if not issubclass(klass, Logger):
-            raise TypeError, "logger not derived from logging.Logger: " + \
-                            klass.__name__
+            raise TypeError("logger not derived from logging.Logger: " + \
+                            klass.__name__)
     global _loggerClass
     _loggerClass = klass
 
@@ -769,7 +769,7 @@
         rv = None
         _acquireLock()
         try:
-            if self.loggerDict.has_key(name):
+            if name in self.loggerDict:
                 rv = self.loggerDict[name]
                 if isinstance(rv, PlaceHolder):
                     ph = rv
@@ -797,7 +797,7 @@
         rv = None
         while (i > 0) and not rv:
             substr = name[:i]
-            if not self.loggerDict.has_key(substr):
+            if substr not in self.loggerDict:
                 self.loggerDict[substr] = PlaceHolder(alogger)
             else:
                 obj = self.loggerDict[substr]
@@ -817,7 +817,7 @@
         specified logger.
         """
         for c in ph.loggers:
-            if string.find(c.parent.name, alogger.name) <> 0:
+            if string.find(c.parent.name, alogger.name) != 0:
                 alogger.parent = c.parent
                 c.parent = alogger
 
@@ -876,7 +876,7 @@
         if self.manager.disable >= DEBUG:
             return
         if DEBUG >= self.getEffectiveLevel():
-            apply(self._log, (DEBUG, msg, args), kwargs)
+            self._log(*(DEBUG, msg, args), **kwargs)
 
     def info(self, msg, *args, **kwargs):
         """
@@ -890,7 +890,7 @@
         if self.manager.disable >= INFO:
             return
         if INFO >= self.getEffectiveLevel():
-            apply(self._log, (INFO, msg, args), kwargs)
+            self._log(*(INFO, msg, args), **kwargs)
 
     def warning(self, msg, *args, **kwargs):
         """
@@ -904,7 +904,7 @@
         if self.manager.disable >= WARNING:
             return
         if self.isEnabledFor(WARNING):
-            apply(self._log, (WARNING, msg, args), kwargs)
+            self._log(*(WARNING, msg, args), **kwargs)
 
     warn = warning
 
@@ -920,13 +920,13 @@
         if self.manager.disable >= ERROR:
             return
         if self.isEnabledFor(ERROR):
-            apply(self._log, (ERROR, msg, args), kwargs)
+            self._log(*(ERROR, msg, args), **kwargs)
 
     def exception(self, msg, *args):
         """
         Convenience method for logging an ERROR with exception information.
         """
-        apply(self.error, (msg,) + args, {'exc_info': 1})
+        self.error(*(msg,) + args, **{'exc_info': 1})
 
     def critical(self, msg, *args, **kwargs):
         """
@@ -940,7 +940,7 @@
         if self.manager.disable >= CRITICAL:
             return
         if CRITICAL >= self.getEffectiveLevel():
-            apply(self._log, (CRITICAL, msg, args), kwargs)
+            self._log(*(CRITICAL, msg, args), **kwargs)
 
     fatal = critical
 
@@ -956,7 +956,7 @@
         if self.manager.disable >= level:
             return
         if self.isEnabledFor(level):
-            apply(self._log, (level, msg, args), kwargs)
+            self._log(*(level, msg, args), **kwargs)
 
     def findCaller(self):
         """
@@ -1133,7 +1133,7 @@
     """
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/compat_logging/__init__.py to planet.archlinux.org3/planet/vendor/compat_logging/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/compat_logging/config.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/compat_logging/config.py to planet.archlinux.org3/planet/vendor/compat_logging/config.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/compat_logging/handlers.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/compat_logging/handlers.py to planet.archlinux.org3/planet/vendor/compat_logging/handlers.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/__init__.py to planet.archlinux.org3/planet/vendor/html5lib/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/constants.py
     if len(root.handlers) == 0:
         basicConfig()
-    apply(root.critical, (msg,)+args, kwargs)
+    root.critical(*(msg,)+args, **kwargs)
 
 fatal = critical
 
@@ -1143,14 +1143,14 @@
     """
     if len(root.handlers) == 0:
         basicConfig()
-    apply(root.error, (msg,)+args, kwargs)
+    root.error(*(msg,)+args, **kwargs)
 
 def exception(msg, *args):
     """
     Log a message with severity 'ERROR' on the root logger,
     with exception information.
     """
-    apply(error, (msg,)+args, {'exc_info': 1})
+    error(*(msg,)+args, **{'exc_info': 1})
 
 def warning(msg, *args, **kwargs):
     """
@@ -1158,7 +1158,7 @@
     """
     if len(root.handlers) == 0:
         basicConfig()
-    apply(root.warning, (msg,)+args, kwargs)
+    root.warning(*(msg,)+args, **kwargs)
 
 warn = warning
 
@@ -1168,7 +1168,7 @@
     """
     if len(root.handlers) == 0:
         basicConfig()
-    apply(root.info, (msg,)+args, kwargs)
+    root.info(*(msg,)+args, **kwargs)
 
 def debug(msg, *args, **kwargs):
     """
@@ -1176,7 +1176,7 @@
     """
     if len(root.handlers) == 0:
         basicConfig()
-    apply(root.debug, (msg,)+args, kwargs)
+    root.debug(*(msg,)+args, **kwargs)
 
 def disable(level):
     """
@@ -1191,6 +1191,6 @@
 
     Should be called at application exit.
     """
-    for h in _handlers.keys():
+    for h in list(_handlers.keys()):
         h.flush()
         h.close()
--- planet.archlinux.org/planet/vendor/compat_logging/config.py	(original)
+++ planet.archlinux.org/planet/vendor/compat_logging/config.py	(refactored)
@@ -26,9 +26,9 @@
 To use, simply 'import logging' and log away!
 """
 
-import sys, logging, logging.handlers, string, thread, threading, socket, struct, os
-
-from SocketServer import ThreadingTCPServer, StreamRequestHandler
+import sys, logging, logging.handlers, string, _thread, threading, socket, struct, os
+
+from socketserver import ThreadingTCPServer, StreamRequestHandler
 
 
 DEFAULT_LOGGING_CONFIG_PORT = 9030
@@ -57,9 +57,9 @@
     rather than a filename, in which case the file-like object will be read
     using readfp.
     """
-    import ConfigParser
-
-    cp = ConfigParser.ConfigParser(defaults)
+    import configparser
+
+    cp = configparser.ConfigParser(defaults)
     if hasattr(cp, 'readfp') and hasattr(fname, 'readline'):
         cp.readfp(fname)
     else:
@@ -106,7 +106,7 @@
                     klass = eval(klass, vars(logging))
                     args = cp.get(sectname, "args")
                     args = eval(args, vars(logging))
-                    h = apply(klass, args)
+                    h = klass(*args)
                     if "level" in opts:
                         level = cp.get(sectname, "level")
                         h.setLevel(logging._levelNames[level])
@@ -153,7 +153,7 @@
             #what's left in existing is the set of loggers
             #which were in the previous configuration but
             #which are not in the new configuration.
-            existing = root.manager.loggerDict.keys()
+            existing = list(root.manager.loggerDict.keys())
             #now set up the new ones...
             for log in llist:
                 sectname = "logger_%s" % log
@@ -202,7 +202,7 @@
     stopListening().
     """
     if not thread:
-        raise NotImplementedError, "listen() needs threading to work"
+        raise NotImplementedError("listen() needs threading to work")
 
     class ConfigStreamHandler(StreamRequestHandler):
         """
@@ -239,8 +239,8 @@
                     f.close()
                     fileConfig(file)
                     os.remove(file)
-            except socket.error, e:
-                if type(e.args) != types.TupleType:
+            except socket.error as e:
+                if type(e.args) != tuple:
                     raise
                 else:
                     errcode = e.args[0]
--- planet.archlinux.org/planet/vendor/compat_logging/handlers.py	(original)
+++ planet.archlinux.org/planet/vendor/compat_logging/handlers.py	(refactored)
@@ -26,9 +26,9 @@
 To use, simply 'import logging' and log away!
 """
 
-import sys, logging, socket, types, os, string, cPickle, struct, time
-
-from SocketServer import ThreadingTCPServer, StreamRequestHandler
+import sys, logging, socket, types, os, string, pickle, struct, time
+
+from socketserver import ThreadingTCPServer, StreamRequestHandler
 
 #
 # Some constants...
@@ -164,7 +164,7 @@
         Pickles the record in binary format with a length prefix, and
         returns it ready for transmission across the socket.
         """
-        s = cPickle.dumps(record.__dict__, 1)
+        s = pickle.dumps(record.__dict__, 1)
         #n = len(s)
         #slen = "%c%c" % ((n >> 8) & 0xFF, n & 0xFF)
         slen = struct.pack(">L", len(s))
@@ -345,7 +345,7 @@
 
         self.address = address
         self.facility = facility
-        if type(address) == types.StringType:
+        if type(address) == bytes:
             self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
             # syslog may require either DGRAM or STREAM sockets
             try:
@@ -374,9 +374,9 @@
         priority_names mapping dictionaries are used to convert them to
         integers.
         """
-        if type(facility) == types.StringType:
+        if type(facility) == bytes:
             facility = self.facility_names[facility]
-        if type(priority) == types.StringType:
+        if type(priority) == bytes:
             priority = self.priority_names[priority]
         return (facility << 3) | priority
 
@@ -424,7 +424,7 @@
         (host, port) tuple format for the mailhost argument.
         """
         logging.Handler.__init__(self)
-        if type(mailhost) == types.TupleType:
+        if type(mailhost) == tuple:
             host, port = mailhost
             self.mailhost = host
             self.mailport = port
@@ -432,7 +432,7 @@
             self.mailhost = mailhost
             self.mailport = None
         self.fromaddr = fromaddr
-        if type(toaddrs) == types.StringType:
+        if type(toaddrs) == bytes:
             toaddrs = [toaddrs]
         self.toaddrs = toaddrs
         self.subject = subject
@@ -516,8 +516,8 @@
                 logging.CRITICAL: win32evtlog.EVENTLOG_ERROR_TYPE,
          }
         except ImportError:
-            print "The Python Win32 extensions for NT (service, event "\
-                        "logging) appear not to be available."
+            print("The Python Win32 extensions for NT (service, event "\
+                        "logging) appear not to be available.")
             self._welu = None
 
     def getMessageID(self, record):
@@ -595,7 +595,7 @@
         logging.Handler.__init__(self)
         method = string.upper(method)
         if method not in ["GET", "POST"]:
-            raise ValueError, "method must be GET or POST"
+            raise ValueError("method must be GET or POST")
         self.host = host
         self.url = url
         self.method = method
@@ -615,10 +615,10 @@
         Send the record to the Web server as an URL-encoded dictionary
         """
         try:
-            import httplib, urllib
-            h = httplib.HTTP(self.host)
+            import http.client, urllib.request, urllib.parse, urllib.error
+            h = http.client.HTTP(self.host)
             url = self.url
-            data = urllib.urlencode(self.mapLogRecord(record))
+            data = urllib.parse.urlencode(self.mapLogRecord(record))
             if self.method == "GET":
                 if (string.find(url, '?') >= 0):
                     sep = '&'
--- planet.archlinux.org/planet/vendor/html5lib/__init__.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/__init__.py	(refactored)
@@ -11,7 +11,7 @@
 tree = html5lib.parse(f) 
 """
 __version__ = "%(version)s"
-from html5parser import HTMLParser, parse, parseFragment
-from treebuilders import getTreeBuilder
-from treewalkers import getTreeWalker
-from serializer import serialize
+from .html5parser import HTMLParser, parse, parseFragment
+from .treebuilders import getTreeBuilder
+from .treewalkers import getTreeWalker
+from .serializer import serialize
--- planet.archlinux.org/planet/vendor/html5lib/constants.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/constants.py	(refactored)
@@ -12,262 +12,262 @@
 
 E = {
     "null-character": 
-       _(u"Null character in input stream, replaced with U+FFFD."),
+       _("Null character in input stream, replaced with U+FFFD."),
     "invalid-character": 
-       _(u"Invalid codepoint in stream."),
+       _("Invalid codepoint in stream."),
     "incorrectly-placed-solidus":
-       _(u"Solidus (/) incorrectly placed in tag."),
+       _("Solidus (/) incorrectly placed in tag."),
     "incorrect-cr-newline-entity":
-       _(u"Incorrect CR newline entity, replaced with LF."),
+       _("Incorrect CR newline entity, replaced with LF."),
     "illegal-windows-1252-entity":
-       _(u"Entity used with illegal number (windows-1252 reference)."),
+       _("Entity used with illegal number (windows-1252 reference)."),
     "cant-convert-numeric-entity":
-       _(u"Numeric entity couldn't be converted to character "
-         u"(codepoint U+%(charAsInt)08x)."),
+       _("Numeric entity couldn't be converted to character "
+         "(codepoint U+%(charAsInt)08x)."),
     "illegal-codepoint-for-numeric-entity":
-       _(u"Numeric entity represents an illegal codepoint: "
-         u"U+%(charAsInt)08x."),
+       _("Numeric entity represents an illegal codepoint: "
+         "U+%(charAsInt)08x."),
     "numeric-entity-without-semicolon":
-       _(u"Numeric entity didn't end with ';'."),
+       _("Numeric entity didn't end with ';'."),
     "expected-numeric-entity-but-got-eof":
-       _(u"Numeric entity expected. Got end of file instead."),
+       _("Numeric entity expected. Got end of file instead."),
     "expected-numeric-entity":
-       _(u"Numeric entity expected but none found."),
+       _("Numeric entity expected but none found."),
     "named-entity-without-semicolon":
-       _(u"Named entity didn't end with ';'."),
+       _("Named entity didn't end with ';'."),
     "expected-named-entity":
-       _(u"Named entity expected. Got none."),
+       _("Named entity expected. Got none."),
     "attributes-in-end-tag":
-       _(u"End tag contains unexpected attributes."),
+       _("End tag contains unexpected attributes."),
     "expected-tag-name-but-got-right-bracket":
-       _(u"Expected tag name. Got '>' instead."),
+       _("Expected tag name. Got '>' instead."),
     "expected-tag-name-but-got-question-mark":
-       _(u"Expected tag name. Got '?' instead. (HTML doesn't "
-         u"support processing instructions.)"),
+       _("Expected tag name. Got '?' instead. (HTML doesn't "
+         "support processing instructions.)"),
     "expected-tag-name":
-       _(u"Expected tag name. Got something else instead"),
+       _("Expected tag name. Got something else instead"),
     "expected-closing-tag-but-got-right-bracket":
-       _(u"Expected closing tag. Got '>' instead. Ignoring '</>'."),
+       _("Expected closing tag. Got '>' instead. Ignoring '</>'."),
     "expected-closing-tag-but-got-eof":
-       _(u"Expected closing tag. Unexpected end of file."),
+       _("Expected closing tag. Unexpected end of file."),
     "expected-closing-tag-but-got-char":
-       _(u"Expected closing tag. Unexpected character '%(data)s' found."),
+       _("Expected closing tag. Unexpected character '%(data)s' found."),
     "eof-in-tag-name":
-       _(u"Unexpected end of file in the tag name."),
+       _("Unexpected end of file in the tag name."),
     "expected-attribute-name-but-got-eof":
-       _(u"Unexpected end of file. Expected attribute name instead."),
+       _("Unexpected end of file. Expected attribute name instead."),
     "eof-in-attribute-name":
-       _(u"Unexpected end of file in attribute name."),
+       _("Unexpected end of file in attribute name."),
     "invalid-character-in-attribute-name":
-        _(u"Invalid chracter in attribute name"),
+        _("Invalid chracter in attribute name"),
     "duplicate-attribute":
-       _(u"Dropped duplicate attribute on tag."),
+       _("Dropped duplicate attribute on tag."),
     "expected-end-of-tag-name-but-got-eof":
-       _(u"Unexpected end of file. Expected = or end of tag."),
+       _("Unexpected end of file. Expected = or end of tag."),
     "expected-attribute-value-but-got-eof":
-       _(u"Unexpected end of file. Expected attribute value."),
+       _("Unexpected end of file. Expected attribute value."),
     "expected-attribute-value-but-got-right-bracket":
-       _(u"Expected attribute value. Got '>' instead."),
+       _("Expected attribute value. Got '>' instead."),
     "eof-in-attribute-value-double-quote":
-       _(u"Unexpected end of file in attribute value (\")."),
+       _("Unexpected end of file in attribute value (\")."),
     "eof-in-attribute-value-single-quote":
-       _(u"Unexpected end of file in attribute value (')."),
+       _("Unexpected end of file in attribute value (')."),
     "eof-in-attribute-value-no-quotes":
-       _(u"Unexpected end of file in attribute value."),
+       _("Unexpected end of file in attribute value."),
     "unexpected-EOF-after-solidus-in-tag":
-        _(u"Unexpected end of file in tag. Expected >"),
+        _("Unexpected end of file in tag. Expected >"),
     "unexpected-character-after-soldius-in-tag":
-        _(u"Unexpected character after / in tag. Expected >"),
+        _("Unexpected character after / in tag. Expected >"),
     "expected-dashes-or-doctype":
-       _(u"Expected '--' or 'DOCTYPE'. Not found."),
+       _("Expected '--' or 'DOCTYPE'. Not found."),
     "incorrect-comment":
-       _(u"Incorrect comment."),
+       _("Incorrect comment."),
     "eof-in-comment":
-       _(u"Unexpected end of file in comment."),
+       _("Unexpected end of file in comment."),
     "eof-in-comment-end-dash":
-       _(u"Unexpected end of file in comment (-)"),
+       _("Unexpected end of file in comment (-)"),
     "unexpected-dash-after-double-dash-in-comment":
-       _(u"Unexpected '-' after '--' found in comment."),
+       _("Unexpected '-' after '--' found in comment."),
     "eof-in-comment-double-dash":
-       _(u"Unexpected end of file in comment (--)."),
+       _("Unexpected end of file in comment (--)."),
     "unexpected-char-in-comment":
-       _(u"Unexpected character in comment found."),
+       _("Unexpected character in comment found."),
     "need-space-after-doctype":
-       _(u"No space after literal string 'DOCTYPE'."),
+       _("No space after literal string 'DOCTYPE'."),
     "expected-doctype-name-but-got-right-bracket":
-       _(u"Unexpected > character. Expected DOCTYPE name."),
+       _("Unexpected > character. Expected DOCTYPE name."),
     "expected-doctype-name-but-got-eof":
-       _(u"Unexpected end of file. Expected DOCTYPE name."),
+       _("Unexpected end of file. Expected DOCTYPE name."),
     "eof-in-doctype-name":
-       _(u"Unexpected end of file in DOCTYPE name."),
+       _("Unexpected end of file in DOCTYPE name."),
     "eof-in-doctype":
-       _(u"Unexpected end of file in DOCTYPE."),
+       _("Unexpected end of file in DOCTYPE."),
     "expected-space-or-right-bracket-in-doctype":
-       _(u"Expected space or '>'. Got '%(data)s'"),
+       _("Expected space or '>'. Got '%(data)s'"),
     "unexpected-end-of-doctype":
-       _(u"Unexpected end of DOCTYPE."),
+       _("Unexpected end of DOCTYPE."),
     "unexpected-char-in-doctype":
-       _(u"Unexpected character in DOCTYPE."),
+       _("Unexpected character in DOCTYPE."),
     "eof-in-innerhtml":
-       _(u"XXX innerHTML EOF"),
+       _("XXX innerHTML EOF"),
     "unexpected-doctype":
-       _(u"Unexpected DOCTYPE. Ignored."),
+       _("Unexpected DOCTYPE. Ignored."),
     "non-html-root":
-       _(u"html needs to be the first start tag."),
+       _("html needs to be the first start tag."),
     "expected-doctype-but-got-eof":
-       _(u"Unexpected End of file. Expected DOCTYPE."),
+       _("Unexpected End of file. Expected DOCTYPE."),
     "unknown-doctype":
-       _(u"Erroneous DOCTYPE."),
+       _("Erroneous DOCTYPE."),
     "expected-doctype-but-got-chars":
-       _(u"Unexpected non-space characters. Expected DOCTYPE."),
+       _("Unexpected non-space characters. Expected DOCTYPE."),
     "expected-doctype-but-got-start-tag":
-       _(u"Unexpected start tag (%(name)s). Expected DOCTYPE."),
+       _("Unexpected start tag (%(name)s). Expected DOCTYPE."),
     "expected-doctype-but-got-end-tag":
-       _(u"Unexpected end tag (%(name)s). Expected DOCTYPE."),
+       _("Unexpected end tag (%(name)s). Expected DOCTYPE."),
     "end-tag-after-implied-root":
-       _(u"Unexpected end tag (%(name)s) after the (implied) root element."),
+       _("Unexpected end tag (%(name)s) after the (implied) root element."),
     "expected-named-closing-tag-but-got-eof":
-       _(u"Unexpected end of file. Expected end tag (%(name)s)."),
+       _("Unexpected end of file. Expected end tag (%(name)s)."),
     "two-heads-are-not-better-than-one":
-       _(u"Unexpected start tag head in existing head. Ignored."),
+       _("Unexpected start tag head in existing head. Ignored."),
     "unexpected-end-tag":
-       _(u"Unexpected end tag (%(name)s). Ignored."),
+       _("Unexpected end tag (%(name)s). Ignored."),
     "unexpected-start-tag-out-of-my-head":
-       _(u"Unexpected start tag (%(name)s) that can be in head. Moved."),
+       _("Unexpected start tag (%(name)s) that can be in head. Moved."),
     "unexpected-start-tag":
-       _(u"Unexpected start tag (%(name)s)."),
+       _("Unexpected start tag (%(name)s)."),
     "missing-end-tag":
-       _(u"Missing end tag (%(name)s)."),
+       _("Missing end tag (%(name)s)."),
     "missing-end-tags":
-       _(u"Missing end tags (%(name)s)."),
+       _("Missing end tags (%(name)s)."),
     "unexpected-start-tag-implies-end-tag":
-       _(u"Unexpected start tag (%(startName)s) "
-         u"implies end tag (%(endName)s)."),
+       _("Unexpected start tag (%(startName)s) "
+         "implies end tag (%(endName)s)."),
     "unexpected-start-tag-treated-as":
-       _(u"Unexpected start tag (%(originalName)s). Treated as %(newName)s."),
+       _("Unexpected start tag (%(originalName)s). Treated as %(newName)s."),
     "deprecated-tag":
-       _(u"Unexpected start tag %(name)s. Don't use it!"),
+       _("Unexpected start tag %(name)s. Don't use it!"),
     "unexpected-start-tag-ignored":
-       _(u"Unexpected start tag %(name)s. Ignored."),
+       _("Unexpected start tag %(name)s. Ignored."),
     "expected-one-end-tag-but-got-another":
-       _(u"Unexpected end tag (%(gotName)s). "
-         u"Missing end tag (%(expectedName)s)."),
+       _("Unexpected end tag (%(gotName)s). "
+         "Missing end tag (%(expectedName)s)."),
     "end-tag-too-early":
-       _(u"End tag (%(name)s) seen too early. Expected other end tag."),
+       _("End tag (%(name)s) seen too early. Expected other end tag."),
     "end-tag-too-early-named":
-       _(u"Unexpected end tag (%(gotName)s). Expected end tag (%(expectedName)s)."),
+       _("Unexpected end tag (%(gotName)s). Expected end tag (%(expectedName)s)."),
     "end-tag-too-early-ignored":
-       _(u"End tag (%(name)s) seen too early. Ignored."),
+       _("End tag (%(name)s) seen too early. Ignored."),
     "adoption-agency-1.1":
-       _(u"End tag (%(name)s) violates step 1, "
-         u"paragraph 1 of the adoption agency algorithm."),
+       _("End tag (%(name)s) violates step 1, "
+         "paragraph 1 of the adoption agency algorithm."),
     "adoption-agency-1.2":
-       _(u"End tag (%(name)s) violates step 1, "
-         u"paragraph 2 of the adoption agency algorithm."),
+       _("End tag (%(name)s) violates step 1, "
+         "paragraph 2 of the adoption agency algorithm."),
     "adoption-agency-1.3":
-       _(u"End tag (%(name)s) violates step 1, "
-         u"paragraph 3 of the adoption agency algorithm."),
+       _("End tag (%(name)s) violates step 1, "
+         "paragraph 3 of the adoption agency algorithm."),
     "unexpected-end-tag-treated-as":
-       _(u"Unexpected end tag (%(originalName)s). Treated as %(newName)s."),
+       _("Unexpected end tag (%(originalName)s). Treated as %(newName)s."),
     "no-end-tag":
-       _(u"This element (%(name)s) has no end tag."),
+       _("This element (%(name)s) has no end tag."),
     "unexpected-implied-end-tag-in-table":
-       _(u"Unexpected implied end tag (%(name)s) in the table phase."),
+       _("Unexpected implied end tag (%(name)s) in the table phase."),
     "unexpected-implied-end-tag-in-table-body":
-       _(u"Unexpected implied end tag (%(name)s) in the table body phase."),
+       _("Unexpected implied end tag (%(name)s) in the table body phase."),
     "unexpected-char-implies-table-voodoo":
-       _(u"Unexpected non-space characters in "
-         u"table context caused voodoo mode."),
+       _("Unexpected non-space characters in "
+         "table context caused voodoo mode."),
     "unexpected-hidden-input-in-table":
-       _(u"Unexpected input with type hidden in table context."),
+       _("Unexpected input with type hidden in table context."),
     "unexpected-form-in-table":
-       _(u"Unexpected form in table context."),
+       _("Unexpected form in table context."),
     "unexpected-start-tag-implies-table-voodoo":
-       _(u"Unexpected start tag (%(name)s) in "
-         u"table context caused voodoo mode."),
+       _("Unexpected start tag (%(name)s) in "
+         "table context caused voodoo mode."),
     "unexpected-end-tag-implies-table-voodoo":
-       _(u"Unexpected end tag (%(name)s) in "
-         u"table context caused voodoo mode."),
+       _("Unexpected end tag (%(name)s) in "
+         "table context caused voodoo mode."),
     "unexpected-cell-in-table-body":
-       _(u"Unexpected table cell start tag (%(name)s) "
-         u"in the table body phase."),
+       _("Unexpected table cell start tag (%(name)s) "
+         "in the table body phase."),
     "unexpected-cell-end-tag":
-       _(u"Got table cell end tag (%(name)s) "
-         u"while required end tags are missing."),
+       _("Got table cell end tag (%(name)s) "
+         "while required end tags are missing."),
     "unexpected-end-tag-in-table-body":
-       _(u"Unexpected end tag (%(name)s) in the table body phase. Ignored."),
+       _("Unexpected end tag (%(name)s) in the table body phase. Ignored."),
     "unexpected-implied-end-tag-in-table-row":
-       _(u"Unexpected implied end tag (%(name)s) in the table row phase."),
+       _("Unexpected implied end tag (%(name)s) in the table row phase."),
     "unexpected-end-tag-in-table-row":
-       _(u"Unexpected end tag (%(name)s) in the table row phase. Ignored."),
+       _("Unexpected end tag (%(name)s) in the table row phase. Ignored."),
     "unexpected-select-in-select":
-       _(u"Unexpected select start tag in the select phase "
-         u"treated as select end tag."),
+       _("Unexpected select start tag in the select phase "
+         "treated as select end tag."),
     "unexpected-input-in-select":
-       _(u"Unexpected input start tag in the select phase."),
+       _("Unexpected input start tag in the select phase."),
     "unexpected-start-tag-in-select":
-       _(u"Unexpected start tag token (%(name)s in the select phase. "
-         u"Ignored."),
+       _("Unexpected start tag token (%(name)s in the select phase. "
+         "Ignored."),
     "unexpected-end-tag-in-select":
-       _(u"Unexpected end tag (%(name)s) in the select phase. Ignored."),
+       _("Unexpected end tag (%(name)s) in the select phase. Ignored."),
     "unexpected-table-element-start-tag-in-select-in-table":
-       _(u"Unexpected table element start tag (%(name)s) in the select in table phase."),
+       _("Unexpected table element start tag (%(name)s) in the select in table phase."),
     "unexpected-table-element-end-tag-in-select-in-table":
-       _(u"Unexpected table element end tag (%(name)s) in the select in table phase."),
+       _("Unexpected table element end tag (%(name)s) in the select in table phase."),
     "unexpected-char-after-body":
-       _(u"Unexpected non-space characters in the after body phase."),
+       _("Unexpected non-space characters in the after body phase."),
     "unexpected-start-tag-after-body":
-       _(u"Unexpected start tag token (%(name)s)"
-         u" in the after body phase."),
+       _("Unexpected start tag token (%(name)s)"
+         " in the after body phase."),
     "unexpected-end-tag-after-body":
-       _(u"Unexpected end tag token (%(name)s)"
-         u" in the after body phase."),
+       _("Unexpected end tag token (%(name)s)"
+         " in the after body phase."),
     "unexpected-char-in-frameset":
-       _(u"Unepxected characters in the frameset phase. Characters ignored."),
+       _("Unepxected characters in the frameset phase. Characters ignored."),
     "unexpected-start-tag-in-frameset":
-       _(u"Unexpected start tag token (%(name)s)"
-         u" in the frameset phase. Ignored."),
+       _("Unexpected start tag token (%(name)s)"
+         " in the frameset phase. Ignored."),
     "unexpected-frameset-in-frameset-innerhtml":
-       _(u"Unexpected end tag token (frameset) "
-         u"in the frameset phase (innerHTML)."),
+       _("Unexpected end tag token (frameset) "
+         "in the frameset phase (innerHTML)."),
     "unexpected-end-tag-in-frameset":
-       _(u"Unexpected end tag token (%(name)s)"
-         u" in the frameset phase. Ignored."),
+       _("Unexpected end tag token (%(name)s)"
+         " in the frameset phase. Ignored."),
     "unexpected-char-after-frameset":
-       _(u"Unexpected non-space characters in the "
-         u"after frameset phase. Ignored."),
+       _("Unexpected non-space characters in the "
+         "after frameset phase. Ignored."),
     "unexpected-start-tag-after-frameset":
-       _(u"Unexpected start tag (%(name)s)"
-         u" in the after frameset phase. Ignored."),
+       _("Unexpected start tag (%(name)s)"
+         " in the after frameset phase. Ignored."),
     "unexpected-end-tag-after-frameset":
-       _(u"Unexpected end tag (%(name)s)"
-         u" in the after frameset phase. Ignored."),
+       _("Unexpected end tag (%(name)s)"
+         " in the after frameset phase. Ignored."),
     "unexpected-end-tag-after-body-innerhtml":
-       _(u"Unexpected end tag after body(innerHtml)"),
+       _("Unexpected end tag after body(innerHtml)"),
     "expected-eof-but-got-char":
-       _(u"Unexpected non-space characters. Expected end of file."),
+       _("Unexpected non-space characters. Expected end of file."),
     "expected-eof-but-got-start-tag":
-       _(u"Unexpected start tag (%(name)s)"
-         u". Expected end of file."),
+       _("Unexpected start tag (%(name)s)"
+         ". Expected end of file."),
     "expected-eof-but-got-end-tag":
-       _(u"Unexpected end tag (%(name)s)"
-         u". Expected end of file."),
+       _("Unexpected end tag (%(name)s)"
+         ". Expected end of file."),
     "eof-in-table":
-       _(u"Unexpected end of file. Expected table content."),
+       _("Unexpected end of file. Expected table content."),
     "eof-in-select":
-       _(u"Unexpected end of file. Expected select content."),
+       _("Unexpected end of file. Expected select content."),
     "eof-in-frameset":
-       _(u"Unexpected end of file. Expected frameset content."),
+       _("Unexpected end of file. Expected frameset content."),
     "eof-in-script-in-script":
-       _(u"Unexpected end of file. Expected script content."),
+       _("Unexpected end of file. Expected script content."),
     "non-void-element-with-trailing-solidus":
-       _(u"Trailing solidus not allowed on element %(name)s"),
+       _("Trailing solidus not allowed on element %(name)s"),
     "unexpected-html-element-in-foreign-content":
-       _(u"Element %(name)s not allowed in a non-html context"),
+       _("Element %(name)s not allowed in a non-html context"),
     "unexpected-end-tag-before-html":
-        _(u"Unexpected end tag (%(name)s) before html."),
+        _("Unexpected end tag (%(name)s) before html."),
     "XXX-undefined-error":
-        (u"Undefined error (this sucks and should be fixed)"),
+        ("Undefined error (this sucks and should be fixed)"),
 }
 
 namespaces = {
@@ -388,11 +388,11 @@
 ))
 
 spaceCharacters = frozenset((
-    u"\t",
-    u"\n",
-    u"\u000C",
-    u" ",
-    u"\r"
+    "\t",
+    "\n",
+    "\u000C",
+    " ",
+    "\r"
 ))
 
 tableInsertModeElements = frozenset((
@@ -511,410 +511,410 @@
 xmlEntities = frozenset(('lt;', 'gt;', 'amp;', 'apos;', 'quot;'))
 
 entities = {
-    "AElig;": u"\u00C6",
-    "AElig": u"\u00C6",
-    "AMP;": u"\u0026",
-    "AMP": u"\u0026",
-    "Aacute;": u"\u00C1",
-    "Aacute": u"\u00C1",
-    "Acirc;": u"\u00C2",
-    "Acirc": u"\u00C2",
-    "Agrave;": u"\u00C0",
-    "Agrave": u"\u00C0",
-    "Alpha;": u"\u0391",
-    "Aring;": u"\u00C5",
-    "Aring": u"\u00C5",
-    "Atilde;": u"\u00C3",
-    "Atilde": u"\u00C3",
-    "Auml;": u"\u00C4",
-    "Auml": u"\u00C4",
-    "Beta;": u"\u0392",
-    "COPY;": u"\u00A9",
-    "COPY": u"\u00A9",
-    "Ccedil;": u"\u00C7",
-    "Ccedil": u"\u00C7",
-    "Chi;": u"\u03A7",
-    "Dagger;": u"\u2021",
-    "Delta;": u"\u0394",
-    "ETH;": u"\u00D0",
-    "ETH": u"\u00D0",
-    "Eacute;": u"\u00C9",
-    "Eacute": u"\u00C9",
-    "Ecirc;": u"\u00CA",
-    "Ecirc": u"\u00CA",
-    "Egrave;": u"\u00C8",
-    "Egrave": u"\u00C8",
-    "Epsilon;": u"\u0395",
-    "Eta;": u"\u0397",
-    "Euml;": u"\u00CB",
-    "Euml": u"\u00CB",
-    "GT;": u"\u003E",
-    "GT": u"\u003E",
-    "Gamma;": u"\u0393",
-    "Iacute;": u"\u00CD",
-    "Iacute": u"\u00CD",
-    "Icirc;": u"\u00CE",
-    "Icirc": u"\u00CE",
-    "Igrave;": u"\u00CC",
-    "Igrave": u"\u00CC",
-    "Iota;": u"\u0399",
-    "Iuml;": u"\u00CF",
-    "Iuml": u"\u00CF",
-    "Kappa;": u"\u039A",
-    "LT;": u"\u003C",
-    "LT": u"\u003C",
-    "Lambda;": u"\u039B",
-    "Mu;": u"\u039C",
-    "Ntilde;": u"\u00D1",
-    "Ntilde": u"\u00D1",
-    "Nu;": u"\u039D",
-    "OElig;": u"\u0152",
-    "Oacute;": u"\u00D3",
-    "Oacute": u"\u00D3",
-    "Ocirc;": u"\u00D4",
-    "Ocirc": u"\u00D4",
-    "Ograve;": u"\u00D2",
-    "Ograve": u"\u00D2",
-    "Omega;": u"\u03A9",
-    "Omicron;": u"\u039F",
-    "Oslash;": u"\u00D8",
-    "Oslash": u"\u00D8",
-    "Otilde;": u"\u00D5",
-    "Otilde": u"\u00D5",
-    "Ouml;": u"\u00D6",
-    "Ouml": u"\u00D6",
-    "Phi;": u"\u03A6",
-    "Pi;": u"\u03A0",
-    "Prime;": u"\u2033",
-    "Psi;": u"\u03A8",
-    "QUOT;": u"\u0022",
-    "QUOT": u"\u0022",
-    "REG;": u"\u00AE",
-    "REG": u"\u00AE",
-    "Rho;": u"\u03A1",
-    "Scaron;": u"\u0160",
-    "Sigma;": u"\u03A3",
-    "THORN;": u"\u00DE",
-    "THORN": u"\u00DE",
-    "TRADE;": u"\u2122",
-    "Tau;": u"\u03A4",
-    "Theta;": u"\u0398",
-    "Uacute;": u"\u00DA",
-    "Uacute": u"\u00DA",
-    "Ucirc;": u"\u00DB",
-    "Ucirc": u"\u00DB",
-    "Ugrave;": u"\u00D9",
-    "Ugrave": u"\u00D9",
-    "Upsilon;": u"\u03A5",
-    "Uuml;": u"\u00DC",
-    "Uuml": u"\u00DC",
-    "Xi;": u"\u039E",
-    "Yacute;": u"\u00DD",
-    "Yacute": u"\u00DD",
-    "Yuml;": u"\u0178",
-    "Zeta;": u"\u0396",
-    "aacute;": u"\u00E1",
-    "aacute": u"\u00E1",
-    "acirc;": u"\u00E2",
-    "acirc": u"\u00E2",
-    "acute;": u"\u00B4",
-    "acute": u"\u00B4",
-    "aelig;": u"\u00E6",
-    "aelig": u"\u00E6",
-    "agrave;": u"\u00E0",
-    "agrave": u"\u00E0",
-    "alefsym;": u"\u2135",
-    "alpha;": u"\u03B1",
-    "amp;": u"\u0026",
-    "amp": u"\u0026",
-    "and;": u"\u2227",
-    "ang;": u"\u2220",
-    "apos;": u"\u0027",
-    "aring;": u"\u00E5",
-    "aring": u"\u00E5",
-    "asymp;": u"\u2248",
-    "atilde;": u"\u00E3",
-    "atilde": u"\u00E3",
-    "auml;": u"\u00E4",
-    "auml": u"\u00E4",
-    "bdquo;": u"\u201E",
-    "beta;": u"\u03B2",
-    "brvbar;": u"\u00A6",
-    "brvbar": u"\u00A6",
-    "bull;": u"\u2022",
-    "cap;": u"\u2229",
-    "ccedil;": u"\u00E7",
-    "ccedil": u"\u00E7",
-    "cedil;": u"\u00B8",
-    "cedil": u"\u00B8",
-    "cent;": u"\u00A2",
-    "cent": u"\u00A2",
-    "chi;": u"\u03C7",
-    "circ;": u"\u02C6",
-    "clubs;": u"\u2663",
-    "cong;": u"\u2245",
-    "copy;": u"\u00A9",
-    "copy": u"\u00A9",
-    "crarr;": u"\u21B5",
-    "cup;": u"\u222A",
-    "curren;": u"\u00A4",
-    "curren": u"\u00A4",
-    "dArr;": u"\u21D3",
-    "dagger;": u"\u2020",
-    "darr;": u"\u2193",
-    "deg;": u"\u00B0",
-    "deg": u"\u00B0",
-    "delta;": u"\u03B4",
-    "diams;": u"\u2666",
-    "divide;": u"\u00F7",
-    "divide": u"\u00F7",
-    "eacute;": u"\u00E9",
-    "eacute": u"\u00E9",
-    "ecirc;": u"\u00EA",
-    "ecirc": u"\u00EA",
-    "egrave;": u"\u00E8",
-    "egrave": u"\u00E8",
-    "empty;": u"\u2205",
-    "emsp;": u"\u2003",
-    "ensp;": u"\u2002",
-    "epsilon;": u"\u03B5",
-    "equiv;": u"\u2261",
-    "eta;": u"\u03B7",
-    "eth;": u"\u00F0",
-    "eth": u"\u00F0",
-    "euml;": u"\u00EB",
-    "euml": u"\u00EB",
-    "euro;": u"\u20AC",
-    "exist;": u"\u2203",
-    "fnof;": u"\u0192",
-    "forall;": u"\u2200",
-    "frac12;": u"\u00BD",
-    "frac12": u"\u00BD",
-    "frac14;": u"\u00BC",
-    "frac14": u"\u00BC",
-    "frac34;": u"\u00BE",
-    "frac34": u"\u00BE",
-    "frasl;": u"\u2044",
-    "gamma;": u"\u03B3",
-    "ge;": u"\u2265",
-    "gt;": u"\u003E",
-    "gt": u"\u003E",
-    "hArr;": u"\u21D4",
-    "harr;": u"\u2194",
-    "hearts;": u"\u2665",
-    "hellip;": u"\u2026",
-    "iacute;": u"\u00ED",
-    "iacute": u"\u00ED",
-    "icirc;": u"\u00EE",
-    "icirc": u"\u00EE",
-    "iexcl;": u"\u00A1",
-    "iexcl": u"\u00A1",
-    "igrave;": u"\u00EC",
-    "igrave": u"\u00EC",
-    "image;": u"\u2111",
-    "infin;": u"\u221E",
-    "int;": u"\u222B",
-    "iota;": u"\u03B9",
-    "iquest;": u"\u00BF",
-    "iquest": u"\u00BF",
-    "isin;": u"\u2208",
-    "iuml;": u"\u00EF",
-    "iuml": u"\u00EF",
-    "kappa;": u"\u03BA",
-    "lArr;": u"\u21D0",
-    "lambda;": u"\u03BB",
-    "lang;": u"\u27E8",
-    "laquo;": u"\u00AB",
-    "laquo": u"\u00AB",
-    "larr;": u"\u2190",
-    "lceil;": u"\u2308",
-    "ldquo;": u"\u201C",
-    "le;": u"\u2264",
-    "lfloor;": u"\u230A",
-    "lowast;": u"\u2217",
-    "loz;": u"\u25CA",
-    "lrm;": u"\u200E",
-    "lsaquo;": u"\u2039",
-    "lsquo;": u"\u2018",
-    "lt;": u"\u003C",
-    "lt": u"\u003C",
-    "macr;": u"\u00AF",
-    "macr": u"\u00AF",
-    "mdash;": u"\u2014",
-    "micro;": u"\u00B5",
-    "micro": u"\u00B5",
-    "middot;": u"\u00B7",
-    "middot": u"\u00B7",
-    "minus;": u"\u2212",
-    "mu;": u"\u03BC",
-    "nabla;": u"\u2207",
-    "nbsp;": u"\u00A0",
-    "nbsp": u"\u00A0",
-    "ndash;": u"\u2013",
-    "ne;": u"\u2260",
-    "ni;": u"\u220B",
-    "not;": u"\u00AC",
-    "not": u"\u00AC",
-    "notin;": u"\u2209",
-    "nsub;": u"\u2284",
-    "ntilde;": u"\u00F1",
-    "ntilde": u"\u00F1",
-    "nu;": u"\u03BD",
-    "oacute;": u"\u00F3",
-    "oacute": u"\u00F3",
-    "ocirc;": u"\u00F4",
-    "ocirc": u"\u00F4",
-    "oelig;": u"\u0153",
-    "ograve;": u"\u00F2",
-    "ograve": u"\u00F2",
-    "oline;": u"\u203E",
-    "omega;": u"\u03C9",
-    "omicron;": u"\u03BF",
-    "oplus;": u"\u2295",
-    "or;": u"\u2228",
-    "ordf;": u"\u00AA",
-    "ordf": u"\u00AA",
-    "ordm;": u"\u00BA",
-    "ordm": u"\u00BA",
-    "oslash;": u"\u00F8",
-    "oslash": u"\u00F8",
-    "otilde;": u"\u00F5",
-    "otilde": u"\u00F5",
-    "otimes;": u"\u2297",
-    "ouml;": u"\u00F6",
-    "ouml": u"\u00F6",
-    "para;": u"\u00B6",
-    "para": u"\u00B6",
-    "part;": u"\u2202",
-    "permil;": u"\u2030",
-    "perp;": u"\u22A5",
-    "phi;": u"\u03C6",
-    "pi;": u"\u03C0",
-    "piv;": u"\u03D6",
-    "plusmn;": u"\u00B1",
-    "plusmn": u"\u00B1",
-    "pound;": u"\u00A3",
-    "pound": u"\u00A3",
-    "prime;": u"\u2032",
-    "prod;": u"\u220F",
-    "prop;": u"\u221D",
-    "psi;": u"\u03C8",
-    "quot;": u"\u0022",
-    "quot": u"\u0022",
-    "rArr;": u"\u21D2",
-    "radic;": u"\u221A",
-    "rang;": u"\u27E9",
-    "raquo;": u"\u00BB",
-    "raquo": u"\u00BB",
-    "rarr;": u"\u2192",
-    "rceil;": u"\u2309",
-    "rdquo;": u"\u201D",
-    "real;": u"\u211C",
-    "reg;": u"\u00AE",
-    "reg": u"\u00AE",
-    "rfloor;": u"\u230B",
-    "rho;": u"\u03C1",
-    "rlm;": u"\u200F",
-    "rsaquo;": u"\u203A",
-    "rsquo;": u"\u2019",
-    "sbquo;": u"\u201A",
-    "scaron;": u"\u0161",
-    "sdot;": u"\u22C5",
-    "sect;": u"\u00A7",
-    "sect": u"\u00A7",
-    "shy;": u"\u00AD",
-    "shy": u"\u00AD",
-    "sigma;": u"\u03C3",
-    "sigmaf;": u"\u03C2",
-    "sim;": u"\u223C",
-    "spades;": u"\u2660",
-    "sub;": u"\u2282",
-    "sube;": u"\u2286",
-    "sum;": u"\u2211",
-    "sup1;": u"\u00B9",
-    "sup1": u"\u00B9",
-    "sup2;": u"\u00B2",
-    "sup2": u"\u00B2",
-    "sup3;": u"\u00B3",
-    "sup3": u"\u00B3",
-    "sup;": u"\u2283",
-    "supe;": u"\u2287",
-    "szlig;": u"\u00DF",
-    "szlig": u"\u00DF",
-    "tau;": u"\u03C4",
-    "there4;": u"\u2234",
-    "theta;": u"\u03B8",
-    "thetasym;": u"\u03D1",
-    "thinsp;": u"\u2009",
-    "thorn;": u"\u00FE",
-    "thorn": u"\u00FE",
-    "tilde;": u"\u02DC",
-    "times;": u"\u00D7",
-    "times": u"\u00D7",
-    "trade;": u"\u2122",
-    "uArr;": u"\u21D1",
-    "uacute;": u"\u00FA",
-    "uacute": u"\u00FA",
-    "uarr;": u"\u2191",
-    "ucirc;": u"\u00FB",
-    "ucirc": u"\u00FB",
-    "ugrave;": u"\u00F9",
-    "ugrave": u"\u00F9",
-    "uml;": u"\u00A8",
-    "uml": u"\u00A8",
-    "upsih;": u"\u03D2",
-    "upsilon;": u"\u03C5",
-    "uuml;": u"\u00FC",
-    "uuml": u"\u00FC",
-    "weierp;": u"\u2118",
-    "xi;": u"\u03BE",
-    "yacute;": u"\u00FD",
-    "yacute": u"\u00FD",
-    "yen;": u"\u00A5",
-    "yen": u"\u00A5",
-    "yuml;": u"\u00FF",
-    "yuml": u"\u00FF",
-    "zeta;": u"\u03B6",
-    "zwj;": u"\u200D",
-    "zwnj;": u"\u200C"
+    "AElig;": "\u00C6",
+    "AElig": "\u00C6",
+    "AMP;": "\u0026",
+    "AMP": "\u0026",
+    "Aacute;": "\u00C1",
+    "Aacute": "\u00C1",
+    "Acirc;": "\u00C2",
+    "Acirc": "\u00C2",
+    "Agrave;": "\u00C0",
+    "Agrave": "\u00C0",
+    "Alpha;": "\u0391",
+    "Aring;": "\u00C5",
+    "Aring": "\u00C5",
+    "Atilde;": "\u00C3",
+    "Atilde": "\u00C3",
+    "Auml;": "\u00C4",
+    "Auml": "\u00C4",
+    "Beta;": "\u0392",
+    "COPY;": "\u00A9",
+    "COPY": "\u00A9",
+    "Ccedil;": "\u00C7",
+    "Ccedil": "\u00C7",
+    "Chi;": "\u03A7",
+    "Dagger;": "\u2021",
+    "Delta;": "\u0394",
+    "ETH;": "\u00D0",
+    "ETH": "\u00D0",
+    "Eacute;": "\u00C9",
+    "Eacute": "\u00C9",
+    "Ecirc;": "\u00CA",
+    "Ecirc": "\u00CA",
+    "Egrave;": "\u00C8",
+    "Egrave": "\u00C8",
+    "Epsilon;": "\u0395",
+    "Eta;": "\u0397",
+    "Euml;": "\u00CB",
+    "Euml": "\u00CB",
+    "GT;": "\u003E",
+    "GT": "\u003E",
+    "Gamma;": "\u0393",
+    "Iacute;": "\u00CD",
+    "Iacute": "\u00CD",
+    "Icirc;": "\u00CE",
+    "Icirc": "\u00CE",
+    "Igrave;": "\u00CC",
+    "Igrave": "\u00CC",
+    "Iota;": "\u0399",
+    "Iuml;": "\u00CF",
+    "Iuml": "\u00CF",
+    "Kappa;": "\u039A",
+    "LT;": "\u003C",
+    "LT": "\u003C",
+    "Lambda;": "\u039B",
+    "Mu;": "\u039C",
+    "Ntilde;": "\u00D1",
+    "Ntilde": "\u00D1",
+    "Nu;": "\u039D",
+    "OElig;": "\u0152",
+    "Oacute;": "\u00D3",
+    "Oacute": "\u00D3",
+    "Ocirc;": "\u00D4",
+    "Ocirc": "\u00D4",
+    "Ograve;": "\u00D2",
+    "Ograve": "\u00D2",
+    "Omega;": "\u03A9",
+    "Omicron;": "\u039F",
+    "Oslash;": "\u00D8",
+    "Oslash": "\u00D8",
+    "Otilde;": "\u00D5",
+    "Otilde": "\u00D5",
+    "Ouml;": "\u00D6",
+    "Ouml": "\u00D6",
+    "Phi;": "\u03A6",
+    "Pi;": "\u03A0",
+    "Prime;": "\u2033",
+    "Psi;": "\u03A8",
+    "QUOT;": "\u0022",
+    "QUOT": "\u0022",
+    "REG;": "\u00AE",
+    "REG": "\u00AE",
+    "Rho;": "\u03A1",
+    "Scaron;": "\u0160",
+    "Sigma;": "\u03A3",
+    "THORN;": "\u00DE",
+    "THORN": "\u00DE",
+    "TRADE;": "\u2122",
+    "Tau;": "\u03A4",
+    "Theta;": "\u0398",
+    "Uacute;": "\u00DA",
+    "Uacute": "\u00DA",
+    "Ucirc;": "\u00DB",
+    "Ucirc": "\u00DB",
+    "Ugrave;": "\u00D9",
+    "Ugrave": "\u00D9",
+    "Upsilon;": "\u03A5",
+    "Uuml;": "\u00DC",
+    "Uuml": "\u00DC",
+    "Xi;": "\u039E",
+    "Yacute;": "\u00DD",
+    "Yacute": "\u00DD",
+    "Yuml;": "\u0178",
+    "Zeta;": "\u0396",
+    "aacute;": "\u00E1",
+    "aacute": "\u00E1",
+    "acirc;": "\u00E2",
+    "acirc": "\u00E2",
+    "acute;": "\u00B4",
+    "acute": "\u00B4",
+    "aelig;": "\u00E6",
+    "aelig": "\u00E6",
+    "agrave;": "\u00E0",
+    "agrave": "\u00E0",
+    "alefsym;": "\u2135",
+    "alpha;": "\u03B1",
+    "amp;": "\u0026",
+    "amp": "\u0026",RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/constants.py to planet.archlinux.org3/planet/vendor/html5lib/constants.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/html5parser.py

+    "and;": "\u2227",
+    "ang;": "\u2220",
+    "apos;": "\u0027",
+    "aring;": "\u00E5",
+    "aring": "\u00E5",
+    "asymp;": "\u2248",
+    "atilde;": "\u00E3",
+    "atilde": "\u00E3",
+    "auml;": "\u00E4",
+    "auml": "\u00E4",
+    "bdquo;": "\u201E",
+    "beta;": "\u03B2",
+    "brvbar;": "\u00A6",
+    "brvbar": "\u00A6",
+    "bull;": "\u2022",
+    "cap;": "\u2229",
+    "ccedil;": "\u00E7",
+    "ccedil": "\u00E7",
+    "cedil;": "\u00B8",
+    "cedil": "\u00B8",
+    "cent;": "\u00A2",
+    "cent": "\u00A2",
+    "chi;": "\u03C7",
+    "circ;": "\u02C6",
+    "clubs;": "\u2663",
+    "cong;": "\u2245",
+    "copy;": "\u00A9",
+    "copy": "\u00A9",
+    "crarr;": "\u21B5",
+    "cup;": "\u222A",
+    "curren;": "\u00A4",
+    "curren": "\u00A4",
+    "dArr;": "\u21D3",
+    "dagger;": "\u2020",
+    "darr;": "\u2193",
+    "deg;": "\u00B0",
+    "deg": "\u00B0",
+    "delta;": "\u03B4",
+    "diams;": "\u2666",
+    "divide;": "\u00F7",
+    "divide": "\u00F7",
+    "eacute;": "\u00E9",
+    "eacute": "\u00E9",
+    "ecirc;": "\u00EA",
+    "ecirc": "\u00EA",
+    "egrave;": "\u00E8",
+    "egrave": "\u00E8",
+    "empty;": "\u2205",
+    "emsp;": "\u2003",
+    "ensp;": "\u2002",
+    "epsilon;": "\u03B5",
+    "equiv;": "\u2261",
+    "eta;": "\u03B7",
+    "eth;": "\u00F0",
+    "eth": "\u00F0",
+    "euml;": "\u00EB",
+    "euml": "\u00EB",
+    "euro;": "\u20AC",
+    "exist;": "\u2203",
+    "fnof;": "\u0192",
+    "forall;": "\u2200",
+    "frac12;": "\u00BD",
+    "frac12": "\u00BD",
+    "frac14;": "\u00BC",
+    "frac14": "\u00BC",
+    "frac34;": "\u00BE",
+    "frac34": "\u00BE",
+    "frasl;": "\u2044",
+    "gamma;": "\u03B3",
+    "ge;": "\u2265",
+    "gt;": "\u003E",
+    "gt": "\u003E",
+    "hArr;": "\u21D4",
+    "harr;": "\u2194",
+    "hearts;": "\u2665",
+    "hellip;": "\u2026",
+    "iacute;": "\u00ED",
+    "iacute": "\u00ED",
+    "icirc;": "\u00EE",
+    "icirc": "\u00EE",
+    "iexcl;": "\u00A1",
+    "iexcl": "\u00A1",
+    "igrave;": "\u00EC",
+    "igrave": "\u00EC",
+    "image;": "\u2111",
+    "infin;": "\u221E",
+    "int;": "\u222B",
+    "iota;": "\u03B9",
+    "iquest;": "\u00BF",
+    "iquest": "\u00BF",
+    "isin;": "\u2208",
+    "iuml;": "\u00EF",
+    "iuml": "\u00EF",
+    "kappa;": "\u03BA",
+    "lArr;": "\u21D0",
+    "lambda;": "\u03BB",
+    "lang;": "\u27E8",
+    "laquo;": "\u00AB",
+    "laquo": "\u00AB",
+    "larr;": "\u2190",
+    "lceil;": "\u2308",
+    "ldquo;": "\u201C",
+    "le;": "\u2264",
+    "lfloor;": "\u230A",
+    "lowast;": "\u2217",
+    "loz;": "\u25CA",
+    "lrm;": "\u200E",
+    "lsaquo;": "\u2039",
+    "lsquo;": "\u2018",
+    "lt;": "\u003C",
+    "lt": "\u003C",
+    "macr;": "\u00AF",
+    "macr": "\u00AF",
+    "mdash;": "\u2014",
+    "micro;": "\u00B5",
+    "micro": "\u00B5",
+    "middot;": "\u00B7",
+    "middot": "\u00B7",
+    "minus;": "\u2212",
+    "mu;": "\u03BC",
+    "nabla;": "\u2207",
+    "nbsp;": "\u00A0",
+    "nbsp": "\u00A0",
+    "ndash;": "\u2013",
+    "ne;": "\u2260",
+    "ni;": "\u220B",
+    "not;": "\u00AC",
+    "not": "\u00AC",
+    "notin;": "\u2209",
+    "nsub;": "\u2284",
+    "ntilde;": "\u00F1",
+    "ntilde": "\u00F1",
+    "nu;": "\u03BD",
+    "oacute;": "\u00F3",
+    "oacute": "\u00F3",
+    "ocirc;": "\u00F4",
+    "ocirc": "\u00F4",
+    "oelig;": "\u0153",
+    "ograve;": "\u00F2",
+    "ograve": "\u00F2",
+    "oline;": "\u203E",
+    "omega;": "\u03C9",
+    "omicron;": "\u03BF",
+    "oplus;": "\u2295",
+    "or;": "\u2228",
+    "ordf;": "\u00AA",
+    "ordf": "\u00AA",
+    "ordm;": "\u00BA",
+    "ordm": "\u00BA",
+    "oslash;": "\u00F8",
+    "oslash": "\u00F8",
+    "otilde;": "\u00F5",
+    "otilde": "\u00F5",
+    "otimes;": "\u2297",
+    "ouml;": "\u00F6",
+    "ouml": "\u00F6",
+    "para;": "\u00B6",
+    "para": "\u00B6",
+    "part;": "\u2202",
+    "permil;": "\u2030",
+    "perp;": "\u22A5",
+    "phi;": "\u03C6",
+    "pi;": "\u03C0",
+    "piv;": "\u03D6",
+    "plusmn;": "\u00B1",
+    "plusmn": "\u00B1",
+    "pound;": "\u00A3",
+    "pound": "\u00A3",
+    "prime;": "\u2032",
+    "prod;": "\u220F",
+    "prop;": "\u221D",
+    "psi;": "\u03C8",
+    "quot;": "\u0022",
+    "quot": "\u0022",
+    "rArr;": "\u21D2",
+    "radic;": "\u221A",
+    "rang;": "\u27E9",
+    "raquo;": "\u00BB",
+    "raquo": "\u00BB",
+    "rarr;": "\u2192",
+    "rceil;": "\u2309",
+    "rdquo;": "\u201D",
+    "real;": "\u211C",
+    "reg;": "\u00AE",
+    "reg": "\u00AE",
+    "rfloor;": "\u230B",
+    "rho;": "\u03C1",
+    "rlm;": "\u200F",
+    "rsaquo;": "\u203A",
+    "rsquo;": "\u2019",
+    "sbquo;": "\u201A",
+    "scaron;": "\u0161",
+    "sdot;": "\u22C5",
+    "sect;": "\u00A7",
+    "sect": "\u00A7",
+    "shy;": "\u00AD",
+    "shy": "\u00AD",
+    "sigma;": "\u03C3",
+    "sigmaf;": "\u03C2",
+    "sim;": "\u223C",
+    "spades;": "\u2660",
+    "sub;": "\u2282",
+    "sube;": "\u2286",
+    "sum;": "\u2211",
+    "sup1;": "\u00B9",
+    "sup1": "\u00B9",
+    "sup2;": "\u00B2",
+    "sup2": "\u00B2",
+    "sup3;": "\u00B3",
+    "sup3": "\u00B3",
+    "sup;": "\u2283",
+    "supe;": "\u2287",
+    "szlig;": "\u00DF",
+    "szlig": "\u00DF",
+    "tau;": "\u03C4",
+    "there4;": "\u2234",
+    "theta;": "\u03B8",
+    "thetasym;": "\u03D1",
+    "thinsp;": "\u2009",
+    "thorn;": "\u00FE",
+    "thorn": "\u00FE",
+    "tilde;": "\u02DC",
+    "times;": "\u00D7",
+    "times": "\u00D7",
+    "trade;": "\u2122",
+    "uArr;": "\u21D1",
+    "uacute;": "\u00FA",
+    "uacute": "\u00FA",
+    "uarr;": "\u2191",
+    "ucirc;": "\u00FB",
+    "ucirc": "\u00FB",
+    "ugrave;": "\u00F9",
+    "ugrave": "\u00F9",
+    "uml;": "\u00A8",
+    "uml": "\u00A8",
+    "upsih;": "\u03D2",
+    "upsilon;": "\u03C5",
+    "uuml;": "\u00FC",
+    "uuml": "\u00FC",
+    "weierp;": "\u2118",
+    "xi;": "\u03BE",
+    "yacute;": "\u00FD",
+    "yacute": "\u00FD",
+    "yen;": "\u00A5",
+    "yen": "\u00A5",
+    "yuml;": "\u00FF",
+    "yuml": "\u00FF",
+    "zeta;": "\u03B6",
+    "zwj;": "\u200D",
+    "zwnj;": "\u200C"
 }
 
 replacementCharacters = {
-    0x0:u"\uFFFD",
-    0x0d:u"\u000A",
-    0x80:u"\u20AC",
-    0x81:u"\u0081",
-    0x81:u"\u0081",
-    0x82:u"\u201A",
-    0x83:u"\u0192",
-    0x84:u"\u201E",
-    0x85:u"\u2026",
-    0x86:u"\u2020",
-    0x87:u"\u2021",
-    0x88:u"\u02C6",
-    0x89:u"\u2030",
-    0x8A:u"\u0160",
-    0x8B:u"\u2039",
-    0x8C:u"\u0152",
-    0x8D:u"\u008D",
-    0x8E:u"\u017D",
-    0x8F:u"\u008F",
-    0x90:u"\u0090",
-    0x91:u"\u2018",
-    0x92:u"\u2019",
-    0x93:u"\u201C",
-    0x94:u"\u201D",
-    0x95:u"\u2022",
-    0x96:u"\u2013",
-    0x97:u"\u2014",
-    0x98:u"\u02DC",
-    0x99:u"\u2122",
-    0x9A:u"\u0161",
-    0x9B:u"\u203A",
-    0x9C:u"\u0153",
-    0x9D:u"\u009D",
-    0x9E:u"\u017E",
-    0x9F:u"\u0178",
+    0x0:"\uFFFD",
+    0x0d:"\u000A",
+    0x80:"\u20AC",
+    0x81:"\u0081",
+    0x81:"\u0081",
+    0x82:"\u201A",
+    0x83:"\u0192",
+    0x84:"\u201E",
+    0x85:"\u2026",
+    0x86:"\u2020",
+    0x87:"\u2021",
+    0x88:"\u02C6",
+    0x89:"\u2030",
+    0x8A:"\u0160",
+    0x8B:"\u2039",
+    0x8C:"\u0152",
+    0x8D:"\u008D",
+    0x8E:"\u017D",
+    0x8F:"\u008F",
+    0x90:"\u0090",
+    0x91:"\u2018",
+    0x92:"\u2019",
+    0x93:"\u201C",
+    0x94:"\u201D",
+    0x95:"\u2022",
+    0x96:"\u2013",
+    0x97:"\u2014",
+    0x98:"\u02DC",
+    0x99:"\u2122",
+    0x9A:"\u0161",
+    0x9B:"\u203A",
+    0x9C:"\u0153",
+    0x9D:"\u009D",
+    0x9E:"\u017E",
+    0x9F:"\u0178",
 }
 
 encodings = {
@@ -1161,7 +1161,7 @@
                            tokenTypes["EmptyTag"]))
 
 
-prefixes = dict([(v,k) for k,v in namespaces.iteritems()])
+prefixes = dict([(v,k) for k,v in namespaces.items()])
 prefixes["http://www.w3.org/1998/Math/MathML"] = "math"
 
 class DataLossWarning(UserWarning):
--- planet.archlinux.org/planet/vendor/html5lib/html5parser.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/html5parser.py	(refactored)
@@ -29,19 +29,19 @@
 
 import sys
 
-import inputstream
-import tokenizer
-
-import treebuilders
-from treebuilders._base import Marker
-from treebuilders import simpletree
-
-import utilsRefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/html5parser.py to planet.archlinux.org3/planet/vendor/html5lib/html5parser.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/ihatexml.py

-from constants import spaceCharacters, asciiUpper2Lower
-from constants import scopingElements, formattingElements, specialElements
-from constants import headingElements, tableInsertModeElements
-from constants import cdataElements, rcdataElements, voidElements
-from constants import tokenTypes, ReparseException, namespaces
+from . import inputstream
+from . import tokenizer
+
+from . import treebuilders
+from .treebuilders._base import Marker
+from .treebuilders import simpletree
+
+from . import utils
+from .constants import spaceCharacters, asciiUpper2Lower
+from .constants import scopingElements, formattingElements, specialElements
+from .constants import headingElements, tableInsertModeElements
+from .constants import cdataElements, rcdataElements, voidElements
+from .constants import tokenTypes, ReparseException, namespaces
 
 def parse(doc, treebuilder="simpletree", encoding=None, 
           namespaceHTMLElements=True):
@@ -122,7 +122,7 @@
             try:
                 self.mainLoop()
                 break
-            except ReparseException, e:
+            except ReparseException as e:
                 self.reset()
 
     def reset(self):
@@ -256,7 +256,7 @@
 
     def adjustMathMLAttributes(self, token):
         replacements = {"definitionurl":"definitionURL"}
-        for k,v in replacements.iteritems():
+        for k,v in replacements.items():
             if k in token["data"]:
                 token["data"][v] = token["data"][k]
                 del token["data"][k]
@@ -326,7 +326,7 @@
             "ychannelselector" : "yChannelSelector",
             "zoomandpan" : "zoomAndPan"
             }
-        for originalName in token["data"].keys():
+        for originalName in list(token["data"].keys()):
             if originalName in replacements:
                 svgName = replacements[originalName]
                 token["data"][svgName] = token["data"][originalName]
@@ -348,7 +348,7 @@
             "xmlns:xlink":("xmlns", "xlink", namespaces["xmlns"])
             }
 
-        for originalName in token["data"].iterkeys():
+        for originalName in token["data"].keys():
             if originalName in replacements:
                 foreignName = replacements[originalName]
                 token["data"][foreignName] = token["data"][originalName]
@@ -463,7 +463,7 @@
            self.parser.parseError("non-html-root")
         # XXX Need a check here to see if the first start tag token emitted is
         # this token... If it's not, invoke self.parser.parseError().
-        for attr, value in token["data"].iteritems():
+        for attr, value in token["data"].items():
             if attr not in self.tree.openElements[0].attributes:
                 self.tree.openElements[0].attributes[attr] = value
         self.parser.firstStartTag = False
@@ -973,7 +973,7 @@
             or self.tree.openElements[1].name != "body"):
             assert self.parser.innerHTML
         else:
-            for attr, value in token["data"].iteritems():
+            for attr, value in token["data"].items():
                 if attr not in self.tree.openElements[1].attributes:
                     self.tree.openElements[1].attributes[attr] = value
 
@@ -1005,7 +1005,7 @@
 
     def startTagForm(self, token):
         if self.tree.formPointer:
-            self.parser.parseError(u"unexpected-start-tag", {"name": "form"})
+            self.parser.parseError("unexpected-start-tag", {"name": "form"})
         else:
             if self.tree.elementInScope("p"):
                 self.endTagP("p")
--- planet.archlinux.org/planet/vendor/html5lib/ihatexml.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/ihatexml.py	(refactored)
@@ -72,10 +72,10 @@
     rv = []
     for item in charList:
         if item[0] == item[1]:
-           rv.append(escapeRegexp(unichr(item[0])))
+           rv.append(escapeRegexp(chr(item[0])))
         else:
-            rv.append(escapeRegexp(unichr(item[0])) + "-" +
-                      escapeRegexp(unichr(item[1])))
+            rv.append(escapeRegexp(chr(item[0])) + "-" +
+                      escapeRegexp(chr(item[1])))
     return "[%s]"%"".join(rv)
 
 def hexToInt(hex_str):
@@ -87,14 +87,14 @@
     for char in specialCharacters:
         string = string.replace(char, "\\" + char)
         if char in string:
-            print string
+            print(string)
 
     return string
 
 #output from the above
-nonXmlNameBMPRegexp = re.compile(u'[\x00-,/:-@\\[-\\^`\\{-\xb6\xb8-\xbf\xd7\xf7\u0132-\u0133\u013f-\u0140\u0149\u017f\u01c4-\u01cc\u01f1-\u01f3\u01f6-\u01f9\u0218-\u024f\u02a9-\u02ba\u02c2-\u02cf\u02d2-\u02ff\u0346-\u035f\u0362-\u0385\u038b\u038d\u03a2\u03cf\u03d7-\u03d9\u03db\u03dd\u03df\u03e1\u03f4-\u0400\u040d\u0450\u045d\u0482\u0487-\u048f\u04c5-\u04c6\u04c9-\u04ca\u04cd-\u04cf\u04ec-\u04ed\u04f6-\u04f7\u04fa-\u0530\u0557-\u0558\u055a-\u0560\u0587-\u0590\u05a2\u05ba\u05be\u05c0\u05c3\u05c5-\u05cf\u05eb-\u05ef\u05f3-\u0620\u063b-\u063f\u0653-\u065f\u066a-\u066f\u06b8-\u06b9\u06bf\u06cf\u06d4\u06e9\u06ee-\u06ef\u06fa-\u0900\u0904\u093a-\u093b\u094e-\u0950\u0955-\u0957\u0964-\u0965\u0970-\u0980\u0984\u098d-\u098e\u0991-\u0992\u09a9\u09b1\u09b3-\u09b5\u09ba-\u09bb\u09bd\u09c5-\u09c6\u09c9-\u09ca\u09ce-\u09d6\u09d8-\u09db\u09de\u09e4-\u09e5\u09f2-\u0a01\u0a03-\u0a04\u0a0b-\u0a0e\u0a11-\u0a12\u0a29\u0a31\u0a34\u0a37\u0a3a-\u0a3b\u0a3d\u0a43-\u0a46\u0a49-\u0a4a\u0a4e-\u0a58\u0a5d\u0a5f-\u0a65\u0a75-\u0a80\u0a84\u0a8c\u0a8e\u0a92\u0aa9\u0ab1\u0ab4\u0aba-\u0abb\u0ac6\u0aca\u0ace-\u0adf\u0ae1-\u0ae5\u0af0-\u0b00\u0b04\u0b0d-\u0b0e\u0b11-\u0b12\u0b29\u0b31\u0b34-\u0b35\u0b3a-\u0b3b\u0b44-\u0b46\u0b49-\u0b4a\u0b4e-\u0b55\u0b58-\u0b5b\u0b5e\u0b62-\u0b65\u0b70-\u0b81\u0b84\u0b8b-\u0b8d\u0b91\u0b96-\u0b98\u0b9b\u0b9d\u0ba0-\u0ba2\u0ba5-\u0ba7\u0bab-\u0bad\u0bb6\u0bba-\u0bbd\u0bc3-\u0bc5\u0bc9\u0bce-\u0bd6\u0bd8-\u0be6\u0bf0-\u0c00\u0c04\u0c0d\u0c11\u0c29\u0c34\u0c3a-\u0c3d\u0c45\u0c49\u0c4e-\u0c54\u0c57-\u0c5f\u0c62-\u0c65\u0c70-\u0c81\u0c84\u0c8d\u0c91\u0ca9\u0cb4\u0cba-\u0cbd\u0cc5\u0cc9\u0cce-\u0cd4\u0cd7-\u0cdd\u0cdf\u0ce2-\u0ce5\u0cf0-\u0d01\u0d04\u0d0d\u0d11\u0d29\u0d3a-\u0d3d\u0d44-\u0d45\u0d49\u0d4e-\u0d56\u0d58-\u0d5f\u0d62-\u0d65\u0d70-\u0e00\u0e2f\u0e3b-\u0e3f\u0e4f\u0e5a-\u0e80\u0e83\u0e85-\u0e86\u0e89\u0e8b-\u0e8c\u0e8e-\u0e93\u0e98\u0ea0\u0ea4\u0ea6\u0ea8-\u0ea9\u0eac\u0eaf\u0eba\u0ebe-\u0ebf\u0ec5\u0ec7\u0ece-\u0ecf\u0eda-\u0f17\u0f1a-\u0f1f\u0f2a-\u0f34\u0f36\u0f38\u0f3a-\u0f3d\u0f48\u0f6a-\u0f70\u0f85\u0f8c-\u0f8f\u0f96\u0f98\u0fae-\u0fb0\u0fb8\u0fba-\u109f\u10c6-\u10cf\u10f7-\u10ff\u1101\u1104\u1108\u110a\u110d\u1113-\u113b\u113d\u113f\u1141-\u114b\u114d\u114f\u1151-\u1153\u1156-\u1158\u115a-\u115e\u1162\u1164\u1166\u1168\u116a-\u116c\u116f-\u1171\u1174\u1176-\u119d\u119f-\u11a7\u11a9-\u11aa\u11ac-\u11ad\u11b0-\u11b6\u11b9\u11bb\u11c3-\u11ea\u11ec-\u11ef\u11f1-\u11f8\u11fa-\u1dff\u1e9c-\u1e9f\u1efa-\u1eff\u1f16-\u1f17\u1f1e-\u1f1f\u1f46-\u1f47\u1f4e-\u1f4f\u1f58\u1f5a\u1f5c\u1f5e\u1f7e-\u1f7f\u1fb5\u1fbd\u1fbf-\u1fc1\u1fc5\u1fcd-\u1fcf\u1fd4-\u1fd5\u1fdc-\u1fdf\u1fed-\u1ff1\u1ff5\u1ffd-\u20cf\u20dd-\u20e0\u20e2-\u2125\u2127-\u2129\u212c-\u212d\u212f-\u217f\u2183-\u3004\u3006\u3008-\u3020\u3030\u3036-\u3040\u3095-\u3098\u309b-\u309c\u309f-\u30a0\u30fb\u30ff-\u3104\u312d-\u4dff\u9fa6-\uabff\ud7a4-\uffff]')
+nonXmlNameBMPRegexp = re.compile('[\x00-,/:-@\\[-\\^`\\{-\xb6\xb8-\xbf\xd7\xf7\u0132-\u0133\u013f-\u0140\u0149\u017f\u01c4-\u01cc\u01f1-\u01f3\u01f6-\u01f9\u0218-\u024f\u02a9-\u02ba\u02c2-\u02cf\u02d2-\u02ff\u0346-\u035f\u0362-\u0385\u038b\u038d\u03a2\u03cf\u03d7-\u03d9\u03db\u03dd\u03df\u03e1\u03f4-\u0400\u040d\u0450\u045d\u0482\u0487-\u048f\u04c5-\u04c6\u04c9-\u04ca\u04cd-\u04cf\u04ec-\u04ed\u04f6-\u04f7\u04fa-\u0530\u0557-\u0558\u055a-\u0560\u0587-\u0590\u05a2\u05ba\u05be\u05c0\u05c3\u05c5-\u05cf\u05eb-\u05ef\u05f3-\u0620\u063b-\u063f\u0653-\u065f\u066a-\u066f\u06b8-\u06b9\u06bf\u06cf\u06d4\u06e9\u06ee-\u06ef\u06fa-\u0900\u0904\u093a-\u093b\u094e-\u0950\u0955-\u0957\u0964-\u0965\u0970-\u0980\u0984\u098d-\u098e\u0991-\u0992\u09a9\u09b1\u09b3-\u09b5\u09ba-\u09bb\u09bd\u09c5-\u09c6\u09c9-\u09ca\u09ce-\u09d6\u09d8-\u09db\u09de\u09e4-\u09e5\u09f2-\u0a01\u0a03-\u0a04\u0a0b-\u0a0e\u0a11-\u0a12\u0a29\u0a31\u0a34\u0a37\u0a3a-\u0a3b\u0a3d\u0a43-\u0a46\u0a49-\u0a4a\u0a4e-\u0a58\u0a5d\u0a5f-\u0a65\u0a75-\u0a80\u0a84\u0a8c\u0a8e\u0a92\u0aa9\u0ab1\u0ab4\u0aba-\u0abb\u0ac6\u0aca\u0ace-\u0adf\u0ae1-\u0ae5\u0af0-\u0b00\u0b04\u0b0d-\u0b0e\u0b11-\u0b12\u0b29\u0b31\u0b34-\u0b35\u0b3a-\u0b3b\u0b44-\u0b46\u0b49-\u0b4a\u0b4e-\u0b55\u0b58-\u0b5b\u0b5e\u0b62-\u0b65\u0b70-\u0b81\u0b84\u0b8b-\u0b8d\u0b91\u0b96-\u0b98\u0b9b\u0b9d\u0ba0-\u0ba2\u0ba5-\u0ba7\u0bab-\u0bad\u0bb6\u0bba-\u0bbd\u0bc3-\u0bc5\u0bc9\u0bce-\u0bd6\u0bd8-\u0be6\u0bf0-\u0c00\u0c04\u0c0d\u0c11\u0c29\u0c34\u0c3a-\u0c3d\u0c45\u0c49\u0c4e-\u0c54\u0c57-\u0c5f\u0c62-\u0c65\u0c70-\u0c81\u0c84\u0c8d\u0c91\u0ca9\u0cb4\u0cba-\u0cbd\u0cc5\u0cc9\u0cce-\u0cd4\u0cd7-\u0cdd\u0cdf\u0ce2-\u0ce5\u0cf0-\u0d01\u0d04\u0d0d\u0d11\u0d29\u0d3a-\u0d3d\u0d44-\u0d45\u0d49\u0d4e-\u0d56\u0d58-\u0d5f\u0d62-\u0d65\u0d70-\u0e00\u0e2f\u0e3b-\u0e3f\u0e4f\u0e5a-\u0e80\u0e83\u0e85-\u0e86\u0e89\u0e8b-\u0e8c\u0e8e-\u0e93\u0e98\u0ea0\u0ea4\u0ea6\u0ea8-\u0ea9\u0eac\u0eaf\u0eba\u0ebe-\u0ebf\u0ec5\u0ec7\u0ece-\u0ecf\u0eda-\u0f17\u0f1a-\u0f1f\u0f2a-\u0f34\u0f36\u0f38\u0f3a-\u0f3d\u0f48\u0f6a-\u0f70\u0f85\u0f8c-\u0f8f\u0f96\u0f98\u0fae-\u0fb0\u0fb8\u0fba-\u109f\u10c6-\u10cf\u10f7-\u10ff\u1101\u1104\u1108\u110a\u110d\u1113-\u113b\u113d\u113f\u1141-\u114b\u114d\u114f\u1151-\u1153\u1156-\u1158\u115a-\u115e\u1162\u1164\u1166\u1168\u116a-\u116c\u116f-\u1171\u1174\u1176-\u119d\u119f-\u11a7\u11a9-\u11aa\u11ac-\u11ad\u11b0-\u11b6\u11b9\u11bb\u11c3-\u11ea\u11ec-\u11ef\u11f1-\u11f8\u11fa-\u1dff\u1e9c-\u1e9f\u1efa-\u1eff\u1f16-\u1f17\u1f1e-\u1f1f\u1f46-\u1f47\u1f4e-\u1f4f\u1f58\u1f5a\u1f5c\u1f5e\u1f7e-\u1f7f\u1fb5\u1fbd\u1fbf-\u1fc1\u1fc5\u1fcd-\u1fcf\u1fd4-\u1fd5\u1fdc-\u1fdf\u1fed-\u1ff1\u1ff5\u1ffd-\u20cf\u20dd-\u20e0\u20e2-\u2125\u2127-\u2129\u212c-\u212d\u212f-\u217f\u2183-\u3004\u3006\u3008-\u3020\u3030\u3036-\u3040\u3095-\u3098\u309b-\u309c\u309f-\u30a0\u30fb\u30ff-\u3104\u312d-\u4dff\u9fa6-\uabff\ud7a4-\uffff]')RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/ihatexml.py to planet.archlinux.org3/planet/vendor/html5lib/ihatexml.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/inputstream.py

 
-nonXmlNameFirstBMPRegexp = re.compile(u'[\x00-@\\[-\\^`\\{-\xbf\xd7\xf7\u0132-\u0133\u013f-\u0140\u0149\u017f\u01c4-\u01cc\u01f1-\u01f3\u01f6-\u01f9\u0218-\u024f\u02a9-\u02ba\u02c2-\u0385\u0387\u038b\u038d\u03a2\u03cf\u03d7-\u03d9\u03db\u03dd\u03df\u03e1\u03f4-\u0400\u040d\u0450\u045d\u0482-\u048f\u04c5-\u04c6\u04c9-\u04ca\u04cd-\u04cf\u04ec-\u04ed\u04f6-\u04f7\u04fa-\u0530\u0557-\u0558\u055a-\u0560\u0587-\u05cf\u05eb-\u05ef\u05f3-\u0620\u063b-\u0640\u064b-\u0670\u06b8-\u06b9\u06bf\u06cf\u06d4\u06d6-\u06e4\u06e7-\u0904\u093a-\u093c\u093e-\u0957\u0962-\u0984\u098d-\u098e\u0991-\u0992\u09a9\u09b1\u09b3-\u09b5\u09ba-\u09db\u09de\u09e2-\u09ef\u09f2-\u0a04\u0a0b-\u0a0e\u0a11-\u0a12\u0a29\u0a31\u0a34\u0a37\u0a3a-\u0a58\u0a5d\u0a5f-\u0a71\u0a75-\u0a84\u0a8c\u0a8e\u0a92\u0aa9\u0ab1\u0ab4\u0aba-\u0abc\u0abe-\u0adf\u0ae1-\u0b04\u0b0d-\u0b0e\u0b11-\u0b12\u0b29\u0b31\u0b34-\u0b35\u0b3a-\u0b3c\u0b3e-\u0b5b\u0b5e\u0b62-\u0b84\u0b8b-\u0b8d\u0b91\u0b96-\u0b98\u0b9b\u0b9d\u0ba0-\u0ba2\u0ba5-\u0ba7\u0bab-\u0bad\u0bb6\u0bba-\u0c04\u0c0d\u0c11\u0c29\u0c34\u0c3a-\u0c5f\u0c62-\u0c84\u0c8d\u0c91\u0ca9\u0cb4\u0cba-\u0cdd\u0cdf\u0ce2-\u0d04\u0d0d\u0d11\u0d29\u0d3a-\u0d5f\u0d62-\u0e00\u0e2f\u0e31\u0e34-\u0e3f\u0e46-\u0e80\u0e83\u0e85-\u0e86\u0e89\u0e8b-\u0e8c\u0e8e-\u0e93\u0e98\u0ea0\u0ea4\u0ea6\u0ea8-\u0ea9\u0eac\u0eaf\u0eb1\u0eb4-\u0ebc\u0ebe-\u0ebf\u0ec5-\u0f3f\u0f48\u0f6a-\u109f\u10c6-\u10cf\u10f7-\u10ff\u1101\u1104\u1108\u110a\u110d\u1113-\u113b\u113d\u113f\u1141-\u114b\u114d\u114f\u1151-\u1153\u1156-\u1158\u115a-\u115e\u1162\u1164\u1166\u1168\u116a-\u116c\u116f-\u1171\u1174\u1176-\u119d\u119f-\u11a7\u11a9-\u11aa\u11ac-\u11ad\u11b0-\u11b6\u11b9\u11bb\u11c3-\u11ea\u11ec-\u11ef\u11f1-\u11f8\u11fa-\u1dff\u1e9c-\u1e9f\u1efa-\u1eff\u1f16-\u1f17\u1f1e-\u1f1f\u1f46-\u1f47\u1f4e-\u1f4f\u1f58\u1f5a\u1f5c\u1f5e\u1f7e-\u1f7f\u1fb5\u1fbd\u1fbf-\u1fc1\u1fc5\u1fcd-\u1fcf\u1fd4-\u1fd5\u1fdc-\u1fdf\u1fed-\u1ff1\u1ff5\u1ffd-\u2125\u2127-\u2129\u212c-\u212d\u212f-\u217f\u2183-\u3006\u3008-\u3020\u302a-\u3040\u3095-\u30a0\u30fb-\u3104\u312d-\u4dff\u9fa6-\uabff\ud7a4-\uffff]')
+nonXmlNameFirstBMPRegexp = re.compile('[\x00-@\\[-\\^`\\{-\xbf\xd7\xf7\u0132-\u0133\u013f-\u0140\u0149\u017f\u01c4-\u01cc\u01f1-\u01f3\u01f6-\u01f9\u0218-\u024f\u02a9-\u02ba\u02c2-\u0385\u0387\u038b\u038d\u03a2\u03cf\u03d7-\u03d9\u03db\u03dd\u03df\u03e1\u03f4-\u0400\u040d\u0450\u045d\u0482-\u048f\u04c5-\u04c6\u04c9-\u04ca\u04cd-\u04cf\u04ec-\u04ed\u04f6-\u04f7\u04fa-\u0530\u0557-\u0558\u055a-\u0560\u0587-\u05cf\u05eb-\u05ef\u05f3-\u0620\u063b-\u0640\u064b-\u0670\u06b8-\u06b9\u06bf\u06cf\u06d4\u06d6-\u06e4\u06e7-\u0904\u093a-\u093c\u093e-\u0957\u0962-\u0984\u098d-\u098e\u0991-\u0992\u09a9\u09b1\u09b3-\u09b5\u09ba-\u09db\u09de\u09e2-\u09ef\u09f2-\u0a04\u0a0b-\u0a0e\u0a11-\u0a12\u0a29\u0a31\u0a34\u0a37\u0a3a-\u0a58\u0a5d\u0a5f-\u0a71\u0a75-\u0a84\u0a8c\u0a8e\u0a92\u0aa9\u0ab1\u0ab4\u0aba-\u0abc\u0abe-\u0adf\u0ae1-\u0b04\u0b0d-\u0b0e\u0b11-\u0b12\u0b29\u0b31\u0b34-\u0b35\u0b3a-\u0b3c\u0b3e-\u0b5b\u0b5e\u0b62-\u0b84\u0b8b-\u0b8d\u0b91\u0b96-\u0b98\u0b9b\u0b9d\u0ba0-\u0ba2\u0ba5-\u0ba7\u0bab-\u0bad\u0bb6\u0bba-\u0c04\u0c0d\u0c11\u0c29\u0c34\u0c3a-\u0c5f\u0c62-\u0c84\u0c8d\u0c91\u0ca9\u0cb4\u0cba-\u0cdd\u0cdf\u0ce2-\u0d04\u0d0d\u0d11\u0d29\u0d3a-\u0d5f\u0d62-\u0e00\u0e2f\u0e31\u0e34-\u0e3f\u0e46-\u0e80\u0e83\u0e85-\u0e86\u0e89\u0e8b-\u0e8c\u0e8e-\u0e93\u0e98\u0ea0\u0ea4\u0ea6\u0ea8-\u0ea9\u0eac\u0eaf\u0eb1\u0eb4-\u0ebc\u0ebe-\u0ebf\u0ec5-\u0f3f\u0f48\u0f6a-\u109f\u10c6-\u10cf\u10f7-\u10ff\u1101\u1104\u1108\u110a\u110d\u1113-\u113b\u113d\u113f\u1141-\u114b\u114d\u114f\u1151-\u1153\u1156-\u1158\u115a-\u115e\u1162\u1164\u1166\u1168\u116a-\u116c\u116f-\u1171\u1174\u1176-\u119d\u119f-\u11a7\u11a9-\u11aa\u11ac-\u11ad\u11b0-\u11b6\u11b9\u11bb\u11c3-\u11ea\u11ec-\u11ef\u11f1-\u11f8\u11fa-\u1dff\u1e9c-\u1e9f\u1efa-\u1eff\u1f16-\u1f17\u1f1e-\u1f1f\u1f46-\u1f47\u1f4e-\u1f4f\u1f58\u1f5a\u1f5c\u1f5e\u1f7e-\u1f7f\u1fb5\u1fbd\u1fbf-\u1fc1\u1fc5\u1fcd-\u1fcf\u1fd4-\u1fd5\u1fdc-\u1fdf\u1fed-\u1ff1\u1ff5\u1ffd-\u2125\u2127-\u2129\u212c-\u212d\u212f-\u217f\u2183-\u3006\u3008-\u3020\u302a-\u3040\u3095-\u30a0\u30fb-\u3104\u312d-\u4dff\u9fa6-\uabff\ud7a4-\uffff]')
 
 class InfosetFilter(object):
     replacementRegexp = re.compile(r"U[\dA-F]{5,5}")
@@ -174,4 +174,4 @@
         return replacement
 
     def unescapeChar(self, charcode):
-        return unichr(int(charcode[1:], 16))
+        return chr(int(charcode[1:], 16))
--- planet.archlinux.org/planet/vendor/html5lib/inputstream.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/inputstream.py	(refactored)
@@ -3,9 +3,9 @@
 import types
 import sys
 
-from constants import EOF, spaceCharacters, asciiLetters, asciiUppercase
-from constants import encodings, ReparseException
-import utils
+from .constants import EOF, spaceCharacters, asciiLetters, asciiUppercase
+from .constants import encodings, ReparseException
+from . import utils
 
 #Non-unicode versions of constants for use in the pre-parser
 spaceCharactersBytes = frozenset([str(item) for item in spaceCharacters])
@@ -13,7 +13,7 @@
 asciiUppercaseBytes = frozenset([str(item) for item in asciiUppercase])
 spacesAngleBrackets = spaceCharactersBytes | frozenset([">", "<"])
 
-invalid_unicode_re = re.compile(u"[\u0001-\u0008\u000B\u000E-\u001F\u007F-\u009F\uD800-\uDFFF\uFDD0-\uFDEF\uFFFE\uFFFF\U0001FFFE\U0001FFFF\U0002FFFE\U0002FFFF\U0003FFFE\U0003FFFF\U0004FFFE\U0004FFFF\U0005FFFE\U0005FFFF\U0006FFFE\U0006FFFF\U0007FFFE\U0007FFFF\U0008FFFE\U0008FFFF\U0009FFFE\U0009FFFF\U000AFFFE\U000AFFFF\U000BFFFE\U000BFFFF\U000CFFFE\U000CFFFF\U000DFFFE\U000DFFFF\U000EFFFE\U000EFFFF\U000FFFFE\U000FFFFF\U0010FFFE\U0010FFFF]")
+invalid_unicode_re = re.compile("[\u0001-\u0008\u000B\u000E-\u001F\u007F-\u009F\uD800-\uDFFF\uFDD0-\uFDEF\uFFFE\uFFFF\U0001FFFE\U0001FFFF\U0002FFFE\U0002FFFF\U0003FFFE\U0003FFFF\U0004FFFE\U0004FFFF\U0005FFFE\U0005FFFF\U0006FFFE\U0006FFFF\U0007FFFE\U0007FFFF\U0008FFFE\U0008FFFF\U0009FFFE\U0009FFFF\U000AFFFE\U000AFFFF\U000BFFFE\U000BFFFF\U000CFFFE\U000CFFFF\U000DFFFE\U000DFFFF\U000EFFFE\U000EFFFF\U000FFFFE\U000FFFFF\U0010FFFE\U0010FFFF]")
 
 non_bmp_invalid_codepoints = set([0x1FFFE, 0x1FFFF, 0x2FFFE, 0x2FFFF, 0x3FFFE,
                                   0x3FFFF, 0x4FFFE, 0x4FFFF, 0x5FFFE, 0x5FFFF,
@@ -23,7 +23,7 @@
                                   0xDFFFF, 0xEFFFE, 0xEFFFF, 0xFFFFE, 0xFFFFF,
                                   0x10FFFE, 0x10FFFF])
 
-ascii_punctuation_re = re.compile(ur"[\u0009-\u000D\u0020-\u002F\u003A-\u0040\u005B-\u0060\u007B-\u007E]")
+ascii_punctuation_re = re.compile(r"[\u0009-\u000D\u0020-\u002F\u003A-\u0040\u005B-\u0060\u007B-\u007E]")
 
 # Cache for charsUntil()
 charsUntilRegEx = {}
@@ -132,7 +132,7 @@
         """
 
         #Craziness
-        if len(u"\U0010FFFF") == 1:
+        if len("\U0010FFFF") == 1:
             self.reportCharacterErrors = self.characterErrorsUCS4
         else:
             self.reportCharacterErrors = self.characterErrorsUCS2
@@ -165,7 +165,7 @@
         self.dataStream = codecs.getreader(self.charEncoding[0])(self.rawStream,
                                                                  'replace')
 
-        self.chunk = u""
+        self.chunk = ""
         self.chunkSize = 0
         self.chunkOffset = 0
         self.errors = []
@@ -189,11 +189,11 @@
             stream = source
         else:
             # Otherwise treat source as a string and convert to a file object
-            if isinstance(source, unicode):
+            if isinstance(source, str):
                 source = source.encode('utf-8')
                 self.charEncoding = ("utf-8", "certain")
-            import cStringIO
-            stream = cStringIO.StringIO(str(source))
+            import io
+            stream = io.StringIO(str(source))
 
         if (not(hasattr(stream, "tell") and hasattr(stream, "seek")) or
             stream is sys.stdin):
@@ -254,7 +254,7 @@
             self.rawStream.seek(0)
             self.reset()
             self.charEncoding = (newEncoding, "certain")
-            raise ReparseException, "Encoding changed from %s to %s"%(self.charEncoding[0], newEncoding)
+            raise ReparseException("Encoding changed from %s to %s"%(self.charEncoding[0], newEncoding))RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/inputstream.py to planet.archlinux.org3/planet/vendor/html5lib/inputstream.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/sanitizer.py

             
     def detectBOM(self):
         """Attempts to detect at BOM at the start of the stream. If
@@ -301,9 +301,9 @@
 
     def _position(self, offset):
         chunk = self.chunk
-        nLines = chunk.count(u'\n', 0, offset)
+        nLines = chunk.count('\n', 0, offset)
         positionLine = self.prevNumLines + nLines
-        lastLinePos = chunk.rfind(u'\n', 0, offset)
+        lastLinePos = chunk.rfind('\n', 0, offset)
         if lastLinePos == -1:
             positionColumn = self.prevNumCols + offset
         else:
@@ -336,7 +336,7 @@
 
         self.prevNumLines, self.prevNumCols = self._position(self.chunkSize)
 
-        self.chunk = u""
+        self.chunk = ""
         self.chunkSize = 0
         self.chunkOffset = 0
 
@@ -347,16 +347,16 @@
         
         self.reportCharacterErrors(data)
 
-        data = data.replace(u"\u0000", u"\ufffd")
+        data = data.replace("\u0000", "\ufffd")
         #Check for CR LF broken across chunks
-        if (self._lastChunkEndsWithCR and data[0] == u"\n"):
+        if (self._lastChunkEndsWithCR and data[0] == "\n"):
             data = data[1:]
             # Stop if the chunk is now empty
             if not data:
                 return False
-        self._lastChunkEndsWithCR = data[-1] == u"\r"
-        data = data.replace(u"\r\n", u"\n")
-        data = data.replace(u"\r", u"\n")
+        self._lastChunkEndsWithCR = data[-1] == "\r"
+        data = data.replace("\r\n", "\n")
+        data = data.replace("\r", "\n")
 
         self.chunk = data
         self.chunkSize = len(data)
@@ -364,15 +364,15 @@
         return True
 
     def characterErrorsUCS4(self, data):
-        for i in xrange(data.count(u"\u0000")):
+        for i in range(data.count("\u0000")):
             self.errors.append("null-character")
-        for i in xrange(len(invalid_unicode_re.findall(data))):
+        for i in range(len(invalid_unicode_re.findall(data))):
             self.errors.append("invalid-codepoint")
 
     def characterErrorsUCS2(self, data):
         #Someone picked the wrong compile option
         #You lose
-        for i in xrange(data.count(u"\u0000")):
+        for i in range(data.count("\u0000")):
             self.errors.append("null-character")
         skip = False
         import sys
@@ -411,10 +411,10 @@
             if __debug__:
                 for c in characters: 
                     assert(ord(c) < 128)
-            regex = u"".join([u"\\x%02x" % ord(c) for c in characters])
+            regex = "".join(["\\x%02x" % ord(c) for c in characters])
             if not opposite:
-                regex = u"^%s" % regex
-            chars = charsUntilRegEx[(characters, opposite)] = re.compile(u"[%s]+" % regex)
+                regex = "^%s" % regex
+            chars = charsUntilRegEx[(characters, opposite)] = re.compile("[%s]+" % regex)
 
         rv = []
 
@@ -441,7 +441,7 @@
                 # Reached EOF
                 break
 
-        r = u"".join(rv)
+        r = "".join(rv)
         return r
 
     def charsUntilEOF(self):
@@ -455,7 +455,7 @@
                 # Reached EOF
                 break
 
-        r = u"".join(rv)
+        r = "".join(rv)
         return r
 
     def unget(self, char):
@@ -488,7 +488,7 @@
     def __iter__(self):
         return self
     
-    def next(self):
+    def __next__(self):
         p = self._position = self._position + 1
         if p >= len(self):
             raise StopIteration
@@ -636,7 +636,7 @@
         return self.handlePossibleTag(False)
 
     def handlePossibleEndTag(self):
-        self.data.next()
+        next(self.data)
         return self.handlePossibleTag(True)
 
     def handlePossibleTag(self, endTag):
@@ -684,7 +684,7 @@
             elif c in spaceCharactersBytes:
                 #Step 6!
                 c = data.skip()
-                c = data.next()
+                c = next(data)
                 break
             elif c in ("/", ">"):
                 return "".join(attrName), ""
@@ -695,13 +695,13 @@
             else:
                 attrName.append(c)
             #Step 5
-            c = data.next()
+            c = next(data)
         #Step 7
         if c != "=":
             data.previous()
             return "".join(attrName), ""
         #Step 8
-        data.next()
+        next(data)
         #Step 9
         c = data.skip()
         #Step 10
@@ -710,10 +710,10 @@
             quoteChar = c
             while True:
                 #10.2
-                c = data.next()
+                c = next(data)
                 #10.3
                 if c == quoteChar:
-                    data.next()
+                    next(data)
                     return "".join(attrName), "".join(attrValue)
                 #10.4
                 elif c in asciiUppercaseBytes:
@@ -731,7 +731,7 @@
             attrValue.append(c)
         # Step 11
         while True:
-            c = data.next()
+            c = next(data)
             if c in spacesAngleBrackets:
                 return "".join(attrName), "".join(attrValue)
             elif c in asciiUppercaseBytes:
@@ -782,7 +782,7 @@
 def codecName(encoding):
     """Return the python codec name corresponding to an encoding or None if the
     string doesn't correspond to a valid encoding."""
-    if (encoding is not None and type(encoding) in types.StringTypes):
+    if (encoding is not None and type(encoding) in (str,)):
         canonicalName = ascii_punctuation_re.sub("", encoding).lower()
         return encodings.get(canonicalName, None)
     else:
--- planet.archlinux.org/planet/vendor/html5lib/sanitizer.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/sanitizer.py	(refactored)
@@ -1,8 +1,8 @@
 import re
 from xml.sax.saxutils import escape, unescape
 
-from tokenizer import HTMLTokenizer
-from constants import tokenTypes
+from .tokenizer import HTMLTokenizer
+from .constants import tokenTypes
 
 class HTMLSanitizerMixin(object):
     """ sanitization of XHTML+MathML+SVG and of inline style attributes."""
@@ -160,23 +160,23 @@
 
         # accommodate filters which use token_type differently
         token_type = token["type"]
-        if token_type in tokenTypes.keys():
+        if token_type in list(tokenTypes.keys()):
           token_type = tokenTypes[token_type]
 
         if token_type in (tokenTypes["StartTag"], tokenTypes["EndTag"], 
                              tokenTypes["EmptyTag"]):
             if token["name"] in self.allowed_elements:
-                if token.has_key("data"):
+                if "data" in token:
                     attrs = dict([(name,val) for name,val in
                                   token["data"][::-1] 
                                   if name in self.allowed_attributes])
                     for attr in self.attr_val_is_uri:
-                        if not attrs.has_key(attr):
+                        if attr not in attrs:
                             continue
                         val_unescaped = re.sub("[`\000-\040\177-\240\s]+", '',
                                                unescape(attrs[attr])).lower()
                         #remove replacement characters from unescaped characters
-                        val_unescaped = val_unescaped.replace(u"\ufffd", "")
+                        val_unescaped = val_unescaped.replace("\ufffd", "")
                         if (re.match("^[a-z0-9][-+.a-z0-9]*:",val_unescaped) and
                             (val_unescaped.split(':')[0] not in 
                              self.allowed_protocols)):
@@ -190,9 +190,9 @@
                         'xlink:href' in attrs and re.search('^\s*[^#\s].*',
                                                             attrs['xlink:href'])):
                         del attrs['xlink:href']
-                    if attrs.has_key('style'):
+                    if 'style' in attrs:
                         attrs['style'] = self.sanitize_css(attrs['style'])
-                    token["data"] = [[name,val] for name,val in attrs.items()]
+                    token["data"] = [[name,val] for name,val in list(attrs.items())]
                 return token
             else:
                 if token_type == tokenTypes["EndTag"]:RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/sanitizer.py to planet.archlinux.org3/planet/vendor/html5lib/sanitizer.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/tokenizer.py

@@ -205,7 +205,7 @@
                 if token.get("selfClosing"):
                     token["data"]=token["data"][:-1] + "/>"
 
-                if token["type"] in tokenTypes.keys():
+                if token["type"] in list(tokenTypes.keys()):
                     token["type"] = "Characters"
                 else:
                     token["type"] = tokenTypes["Characters"]
--- planet.archlinux.org/planet/vendor/html5lib/tokenizer.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/tokenizer.py	(refactored)
@@ -7,16 +7,16 @@
 try:
     from collections import deque
 except ImportError:
-    from utils import deque
-    
-from constants import spaceCharacters
-from constants import entitiesWindows1252, entities
-from constants import asciiLowercase, asciiLetters, asciiUpper2Lower
-from constants import digits, hexDigits, EOF
-from constants import tokenTypes, tagTokenTypes
-from constants import replacementCharacters
-
-from inputstream import HTMLInputStream
+    from .utils import deque
+    
+from .constants import spaceCharacters
+from .constants import entitiesWindows1252, entities
+from .constants import asciiLowercase, asciiLetters, asciiUpper2Lower
+from .constants import digits, hexDigits, EOF
+from .constants import tokenTypes, tagTokenTypes
+from .constants import replacementCharacters
+
+from .inputstream import HTMLInputStream
 
 # Group entities by their first character, for faster lookups
 entitiesByFirstChar = {}
@@ -104,7 +104,7 @@
               "datavars": {"charAsInt": charAsInt}})
         elif ((0xD800 <= charAsInt <= 0xDFFF) or 
               (charAsInt > 0x10FFFF)):
-            char = u"\uFFFD"
+            char = "\uFFFD"
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "illegal-codepoint-for-numeric-entity",
               "datavars": {"charAsInt": charAsInt}})
@@ -130,13 +130,13 @@
             try:
                 # Try/except needed as UCS-2 Python builds' unichar only works
                 # within the BMP.
-                char = unichr(charAsInt)
+                char = chr(charAsInt)
             except ValueError:
                 char = eval("u'\\U%08x'" % charAsInt)
 
         # Discard the ; if present. Otherwise, put it back on the queue and
         # invoke parseError on parser.
-        if c != u";":
+        if c != ";":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "numeric-entity-without-semicolon"})
             self.stream.unget(c)
@@ -145,18 +145,18 @@
 
     def consumeEntity(self, allowedChar=None, fromAttribute=False):
         # Initialise to the default output for when no entity is matched
-        output = u"&"
+        output = "&"
 
         charStack = [self.stream.char()]
-        if (charStack[0] in spaceCharacters or charStack[0] in (EOF, u"<", u"&") 
+        if (charStack[0] in spaceCharacters or charStack[0] in (EOF, "<", "&") 
             or (allowedChar is not None and allowedChar == charStack[0])):
             self.stream.unget(charStack[0])
 
-        elif charStack[0] == u"#":
+        elif charStack[0] == "#":
             # Read the next character to see if it's hex or decimal
             hex = False
             charStack.append(self.stream.char())
-            if charStack[-1] in (u"x", u"X"):
+            if charStack[-1] in ("x", "X"):
                 hex = True
                 charStack.append(self.stream.char())
 
@@ -171,7 +171,7 @@
                 self.tokenQueue.append({"type": tokenTypes["ParseError"],
                     "data": "expected-numeric-entity"})
                 self.stream.unget(charStack.pop())
-                output = u"&" + u"".join(charStack)
+                output = "&" + "".join(charStack)
 
         else:
             # At this point in the process might have named entity. Entities
@@ -194,7 +194,7 @@
 
             # Try to find the longest entity the string will match to take care
             # of &noti for instance.
-            for entityLength in xrange(len(charStack)-1, 1, -1):
+            for entityLength in range(len(charStack)-1, 1, -1):
                 possibleEntityName = "".join(charStack[:entityLength])
                 if possibleEntityName in entities:
                     entityName = possibleEntityName
@@ -208,16 +208,16 @@
                   (charStack[entityLength] in asciiLetters
                   or charStack[entityLength] in digits):
                     self.stream.unget(charStack.pop())
-                    output = u"&" + u"".join(charStack)
+                    output = "&" + "".join(charStack)
                 else:
                     output = entities[entityName]
                     self.stream.unget(charStack.pop())
-                    output += u"".join(charStack[entityLength:])
+                    output += "".join(charStack[entityLength:])
             else:
                 self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
                   "expected-named-entity"})
                 self.stream.unget(charStack.pop())
-                output = u"&" + u"".join(charStack)
+                output = "&" + "".join(charStack)
 
         if fromAttribute:
             self.currentToken["data"][-1][1] += output
@@ -271,7 +271,7 @@
             # have already been appended to lastFourChars and will have broken
             # any <!-- or --> sequences
         else:
-            chars = self.stream.charsUntil((u"&", u"<"))
+            chars = self.stream.charsUntil(("&", "<"))
             self.tokenQueue.append({"type": tokenTypes["Characters"], "data": 
               data + chars})
         return True
@@ -300,7 +300,7 @@
             # have already been appended to lastFourChars and will have broken
             # any <!-- or --> sequences
         else:
-            chars = self.stream.charsUntil((u"&", u"<"))
+            chars = self.stream.charsUntil(("&", "<"))
             self.tokenQueue.append({"type": tokenTypes["Characters"], "data": 
               data + chars})
         return True
@@ -318,7 +318,7 @@
             # Tokenization ends.
             return False
         else:
-            chars = self.stream.charsUntil((u"<"))
+            chars = self.stream.charsUntil(("<"))
             self.tokenQueue.append({"type": tokenTypes["Characters"], "data": 
               data + chars})
         return True
@@ -331,7 +331,7 @@
             # Tokenization ends.
             return False
         else:
-            chars = self.stream.charsUntil((u"<"))
+            chars = self.stream.charsUntil(("<"))
             self.tokenQueue.append({"type": tokenTypes["Characters"], "data": 
               data + chars})
         return True
@@ -348,9 +348,9 @@
 
     def tagOpenState(self):
         data = self.stream.char()
-        if data == u"!":
+        if data == "!":
             self.state = self.markupDeclarationOpenState
-        elif data == u"/":
+        elif data == "/":
             self.state = self.closeTagOpenState
         elif data in asciiLetters:
             self.currentToken = {"type": tokenTypes["StartTag"], 
@@ -358,14 +358,14 @@
                                  "selfClosing": False,
                                  "selfClosingAcknowledged": False}
             self.state = self.tagNameState
-        elif data == u">":
+        elif data == ">":
             # XXX In theory it could be something besides a tag name. But
             # do we really care?
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "expected-tag-name-but-got-right-bracket"})
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<>"})
-            self.state = self.dataState
-        elif data == u"?":
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<>"})
+            self.state = self.dataState
+        elif data == "?":
             # XXX In theory it could be something besides a tag name. But
             # do we really care?
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -376,7 +376,7 @@
             # XXX
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "expected-tag-name"})
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.stream.unget(data)
             self.state = self.dataState
         return True
@@ -387,14 +387,14 @@
             self.currentToken = {"type": tokenTypes["EndTag"], "name": data,
                                  "data": [], "selfClosing":False}
             self.state = self.tagNameState
-        elif data == u">":
+        elif data == ">":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "expected-closing-tag-but-got-right-bracket"})
             self.state = self.dataState
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "expected-closing-tag-but-got-eof"})
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"</"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "</"})
             self.state = self.dataState
         else:
             # XXX data can be _'_...
@@ -409,13 +409,13 @@
         data = self.stream.char()
         if data in spaceCharacters:
             self.state = self.beforeAttributeNameState
-        elif data == u">":
+        elif data == ">":
             self.emitCurrentToken()
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "eof-in-tag-name"})
             self.state = self.dataState
-        elif data == u"/":
+        elif data == "/":
             self.state = self.selfClosingStartTagState
         else:
             self.currentToken["name"] += data
@@ -429,7 +429,7 @@
             self.temporaryBuffer = ""
             self.state = self.rcdataEndTagOpenState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.stream.unget(data)
             self.state = self.rcdataState
         return True
@@ -440,7 +440,7 @@
             self.temporaryBuffer += data
             self.state = self.rcdataEndTagNameState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"</"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "</"})
             self.stream.unget(data)
             self.state = self.rcdataState
         return True
@@ -468,7 +468,7 @@
             self.temporaryBuffer += data
         else:
             self.tokenQueue.append({"type": tokenTypes["Characters"],
-                                    "data": u"</" + self.temporaryBuffer})
+                                    "data": "</" + self.temporaryBuffer})
             self.stream.unget(data)
             self.state = self.rcdataState
         return True
@@ -479,7 +479,7 @@
             self.temporaryBuffer = ""
             self.state = self.rawtextEndTagOpenState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.stream.unget(data)
             self.state = self.rawtextState
         return True
@@ -490,7 +490,7 @@
             self.temporaryBuffer += data
             self.state = self.rawtextEndTagNameState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"</"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "</"})
             self.stream.unget(data)
             self.state = self.rawtextState
         return True
@@ -518,7 +518,7 @@
             self.temporaryBuffer += data
         else:
             self.tokenQueue.append({"type": tokenTypes["Characters"],
-                                    "data": u"</" + self.temporaryBuffer})
+                                    "data": "</" + self.temporaryBuffer})
             self.stream.unget(data)
             self.state = self.rawtextState
         return True
@@ -529,10 +529,10 @@
             self.temporaryBuffer = ""
             self.state = self.scriptDataEndTagOpenState
         elif data == "!":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<!"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<!"})
             self.state = self.scriptDataEscapeStartState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.stream.unget(data)
             self.state = self.scriptDataState
         return True
@@ -543,7 +543,7 @@
             self.temporaryBuffer += data
             self.state = self.scriptDataEndTagNameState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"</"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "</"})
             self.stream.unget(data)
             self.state = self.scriptDataState
         return True
@@ -571,7 +571,7 @@
             self.temporaryBuffer += data
         else:
             self.tokenQueue.append({"type": tokenTypes["Characters"],
-                                    "data": u"</" + self.temporaryBuffer})
+                                    "data": "</" + self.temporaryBuffer})
             self.stream.unget(data)
             self.state = self.scriptDataState
         return True
@@ -579,7 +579,7 @@
     def scriptDataEscapeStartState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
             self.state = self.scriptDataEscapeStartDashState
         else:
             self.stream.unget(data)
@@ -589,7 +589,7 @@
     def scriptDataEscapeStartDashState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
             self.state = self.scriptDataEscapedDashDashState
         else:
             self.stream.unget(data)
@@ -599,14 +599,14 @@
     def scriptDataEscapedState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
             self.state = self.scriptDataEscapedDashState
         elif data == "<":
             self.state = self.scriptDataEscapedLessThanSignState
         elif data == EOF:
             self.state = self.dataState
         else:
-            chars = self.stream.charsUntil((u"<-"))
+            chars = self.stream.charsUntil(("<-"))
             self.tokenQueue.append({"type": tokenTypes["Characters"], "data": 
               data + chars})
         return True
@@ -614,7 +614,7 @@
     def scriptDataEscapedDashState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
             self.state = self.scriptDataEscapedDashDashState
         elif data == "<":
             self.state = self.scriptDataEscapedLessThanSignState
@@ -628,11 +628,11 @@
     def scriptDataEscapedDashDashState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
         elif data == "<":
             self.state = self.scriptDataEscapedLessThanSignState
         elif data == ">":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u">"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": ">"})
             self.state = self.scriptDataState
         elif data == EOF:
             self.state = self.dataState
@@ -647,11 +647,11 @@
             self.temporaryBuffer = ""
             self.state = self.scriptDataEscapedEndTagOpenState
         elif data in asciiLetters:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<" + data})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<" + data})
             self.temporaryBuffer = data
             self.state = self.scriptDataDoubleEscapeStartState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.stream.unget(data)
             self.state = self.scriptDataEscapedState
         return True
@@ -662,7 +662,7 @@
             self.temporaryBuffer = data
             self.state = self.scriptDataEscapedEndTagNameState
         else:
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"</"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "</"})
             self.stream.unget(data)
             self.state = self.scriptDataEscapedState
         return True
@@ -690,7 +690,7 @@
             self.temporaryBuffer += data
         else:
             self.tokenQueue.append({"type": tokenTypes["Characters"],
-                                    "data": u"</" + self.temporaryBuffer})
+                                    "data": "</" + self.temporaryBuffer})
             self.stream.unget(data)
             self.state = self.scriptDataEscapedState
         return True
@@ -714,10 +714,10 @@
     def scriptDataDoubleEscapedState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
             self.state = self.scriptDataDoubleEscapedDashState
         elif data == "<":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.state = self.scriptDataDoubleEscapedLessThanSignState
         elif data == EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -730,10 +730,10 @@
     def scriptDataDoubleEscapedDashState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
             self.state = self.scriptDataDoubleEscapedDashDashState
         elif data == "<":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.state = self.scriptDataDoubleEscapedLessThanSignState
         elif data == EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -747,12 +747,12 @@
     def scriptDataDoubleEscapedDashState(self):
         data = self.stream.char()
         if data == "-":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"-"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "-"})
         elif data == "<":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"<"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "<"})
             self.state = self.scriptDataDoubleEscapedLessThanSignState
         elif data == ">":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u">"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": ">"})
             self.state = self.scriptDataState
         elif data == EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -766,7 +766,7 @@
     def scriptDataDoubleEscapedLessThanSignState(self):
         data = self.stream.char()
         if data == "/":
-            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": u"/"})
+            self.tokenQueue.append({"type": tokenTypes["Characters"], "data": "/"})
             self.temporaryBuffer = ""
             self.state = self.scriptDataDoubleEscapeEndState
         else:
@@ -797,11 +797,11 @@
         elif data in asciiLetters:
             self.currentToken["data"].append([data, ""])
             self.state = self.attributeNameState
-        elif data == u">":
-            self.emitCurrentToken()
-        elif data == u"/":
+        elif data == ">":
+            self.emitCurrentToken()
+        elif data == "/":
             self.state = self.selfClosingStartTagState
-        elif data in (u"'", u'"', u"=", u"<"):
+        elif data in ("'", '"', "=", "<"):
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "invalid-character-in-attribute-name"})
             self.currentToken["data"].append([data, ""])
@@ -819,22 +819,22 @@
         data = self.stream.char()
         leavingThisState = True
         emitToken = False
-        if data == u"=":
+        if data == "=":
             self.state = self.beforeAttributeValueState
         elif data in asciiLetters:
             self.currentToken["data"][-1][0] += data +\
               self.stream.charsUntil(asciiLetters, True)
             leavingThisState = False
-        elif data == u">":
+        elif data == ">":
             # XXX If we emit here the attributes are converted to a dict
             # without being checked and when the code below runs we error
             # because data is a dict not a list
             emitToken = True
         elif data in spaceCharacters:
             self.state = self.afterAttributeNameState
-        elif data == u"/":
+        elif data == "/":
             self.state = self.selfClosingStartTagState
-        elif data in (u"'", u'"', u"<"):
+        elif data in ("'", '"', "<"):
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "invalid-character-in-attribute-name"})
             self.currentToken["data"][-1][0] += data
@@ -869,16 +869,16 @@
         data = self.stream.char()
         if data in spaceCharacters:
             self.stream.charsUntil(spaceCharacters, True)
-        elif data == u"=":
+        elif data == "=":
             self.state = self.beforeAttributeValueState
-        elif data == u">":
+        elif data == ">":
             self.emitCurrentToken()
         elif data in asciiLetters:
             self.currentToken["data"].append([data, ""])
             self.state = self.attributeNameState
-        elif data == u"/":
+        elif data == "/":
             self.state = self.selfClosingStartTagState
-        elif data in (u"'", u'"', u"<"):
+        elif data in ("'", '"', "<"):
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "invalid-character-after-attribute-name"})
             self.currentToken["data"].append([data, ""])
@@ -896,18 +896,18 @@
         data = self.stream.char()
         if data in spaceCharacters:
             self.stream.charsUntil(spaceCharacters, True)
-        elif data == u"\"":
+        elif data == "\"":
             self.state = self.attributeValueDoubleQuotedState
-        elif data == u"&":
+        elif data == "&":
             self.state = self.attributeValueUnQuotedState
             self.stream.unget(data);
-        elif data == u"'":
+        elif data == "'":
             self.state = self.attributeValueSingleQuotedState
-        elif data == u">":
+        elif data == ">":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "expected-attribute-value-but-got-right-bracket"})
             self.emitCurrentToken()
-        elif data in (u"=", u"<", u"`"):
+        elif data in ("=", "<", "`"):
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "equals-in-unquoted-attribute-value"})
             self.currentToken["data"][-1][1] += data
@@ -925,41 +925,41 @@
         data = self.stream.char()
         if data == "\"":
             self.state = self.afterAttributeValueState
-        elif data == u"&":
-            self.processEntityInAttribute(u'"')
+        elif data == "&":
+            self.processEntityInAttribute('"')
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "eof-in-attribute-value-double-quote"})
             self.emitCurrentToken()
         else:
             self.currentToken["data"][-1][1] += data +\
-              self.stream.charsUntil(("\"", u"&"))
+              self.stream.charsUntil(("\"", "&"))
         return True
 
     def attributeValueSingleQuotedState(self):
         data = self.stream.char()
         if data == "'":
             self.state = self.afterAttributeValueState
-        elif data == u"&":
-            self.processEntityInAttribute(u"'")
+        elif data == "&":
+            self.processEntityInAttribute("'")
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "eof-in-attribute-value-single-quote"})
             self.emitCurrentToken()
         else:
             self.currentToken["data"][-1][1] += data +\
-              self.stream.charsUntil(("'", u"&"))
+              self.stream.charsUntil(("'", "&"))
         return True
 
     def attributeValueUnQuotedState(self):
         data = self.stream.char()
         if data in spaceCharacters:
             self.state = self.beforeAttributeNameState
-        elif data == u"&":
+        elif data == "&":
             self.processEntityInAttribute(">")
-        elif data == u">":
-            self.emitCurrentToken()
-        elif data in (u'"', u"'", u"=", u"<", u"`"):
+        elif data == ">":
+            self.emitCurrentToken()
+        elif data in ('"', "'", "=", "<", "`"):
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "unexpected-character-in-unquoted-attribute-value"})
             self.currentToken["data"][-1][1] += data
@@ -969,16 +969,16 @@
             self.emitCurrentToken()
         else:
             self.currentToken["data"][-1][1] += data + self.stream.charsUntil(
-              frozenset((u"&", u">", u'"', u"'", u"=", u"<", u"`")) | spaceCharacters)
+              frozenset(("&", ">", '"', "'", "=", "<", "`")) | spaceCharacters)
         return True
 
     def afterAttributeValueState(self):
         data = self.stream.char()
         if data in spaceCharacters:
             self.state = self.beforeAttributeNameState
-        elif data == u">":
-            self.emitCurrentToken()
-        elif data == u"/":
+        elif data == ">":
+            self.emitCurrentToken()
+        elif data == "/":
             self.state = self.selfClosingStartTagState
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -1016,7 +1016,7 @@
         # until the first > or EOF (charsUntil checks for EOF automatically)
         # and emit it.
         self.tokenQueue.append(
-          {"type": tokenTypes["Comment"], "data": self.stream.charsUntil(u">")})
+          {"type": tokenTypes["Comment"], "data": self.stream.charsUntil(">")})
 
         # Eat the character directly after the bogus comment which is either a
         # ">" or an EOF.
@@ -1027,7 +1027,7 @@
     def bogusCommentContinuationState(self):
         # Like bogusCommentState, but the caller must create the comment token
         # and this state just adds more characters to it
-        self.currentToken["data"] += self.stream.charsUntil(u">")
+        self.currentToken["data"] += self.stream.charsUntil(">")
         self.tokenQueue.append(self.currentToken)
 
         # Eat the character directly after the bogus comment which is either a
@@ -1038,23 +1038,23 @@
 
     def markupDeclarationOpenState(self):
         charStack = [self.stream.char()]
-        if charStack[-1] == u"-":
+        if charStack[-1] == "-":
             charStack.append(self.stream.char())
-            if charStack[-1] == u"-":
-                self.currentToken = {"type": tokenTypes["Comment"], "data": u""}
+            if charStack[-1] == "-":
+                self.currentToken = {"type": tokenTypes["Comment"], "data": ""}
                 self.state = self.commentStartState
                 return True
-        elif charStack[-1] in (u'd', u'D'):
+        elif charStack[-1] in ('d', 'D'):
             matched = True
-            for expected in ((u'o', u'O'), (u'c', u'C'), (u't', u'T'),
-                             (u'y', u'Y'), (u'p', u'P'), (u'e', u'E')):
+            for expected in (('o', 'O'), ('c', 'C'), ('t', 'T'),
+                             ('y', 'Y'), ('p', 'P'), ('e', 'E')):
                 charStack.append(self.stream.char())
                 if charStack[-1] not in expected:
                     matched = False
                     break
             if matched:
                 self.currentToken = {"type": tokenTypes["Doctype"],
-                                     "name": u"",
+                                     "name": "",
                                      "publicId": None, "systemId": None, 
                                      "correct": True}
                 self.state = self.doctypeState
@@ -1067,7 +1067,7 @@
         # the last character might be '>' or EOF and needs to be ungetted
         self.stream.unget(charStack.pop())
         self.currentToken = {"type": tokenTypes["Comment"], 
-                             "data": u"".join(charStack)}
+                             "data": "".join(charStack)}
         self.state = self.bogusCommentContinuationState
         return True
 
@@ -1086,7 +1086,7 @@
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         else:
-            self.currentToken["data"] += data + self.stream.charsUntil(u"-")
+            self.currentToken["data"] += data + self.stream.charsUntil("-")
             self.state = self.commentState
         return True
     
@@ -1105,14 +1105,14 @@
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         else:
-            self.currentToken["data"] += "-" + data + self.stream.charsUntil(u"-")
+            self.currentToken["data"] += "-" + data + self.stream.charsUntil("-")
             self.state = self.commentState
         return True
 
     
     def commentState(self):
         data = self.stream.char()
-        if data == u"-":
+        if data == "-":
             self.state = self.commentEndDashState
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -1120,12 +1120,12 @@
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         else:
-            self.currentToken["data"] += data + self.stream.charsUntil(u"-")
+            self.currentToken["data"] += data + self.stream.charsUntil("-")
         return True
 
     def commentEndDashState(self):
         data = self.stream.char()
-        if data == u"-":
+        if data == "-":
             self.state = self.commentEndState
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -1133,8 +1133,8 @@
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         else:
-            self.currentToken["data"] += u"-" + data +\
-              self.stream.charsUntil(u"-")
+            self.currentToken["data"] += "-" + data +\
+              self.stream.charsUntil("-")
             # Consume the next character which is either a "-" or an EOF as
             # well so if there's a "-" directly after the "-" we go nicely to
             # the "comment end state" without emitting a ParseError() there.
@@ -1143,10 +1143,10 @@
 
     def commentEndState(self):
         data = self.stream.char()RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/tokenizer.py to planet.archlinux.org3/planet/vendor/html5lib/tokenizer.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/utils.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/utils.py to planet.archlinux.org3/planet/vendor/html5lib/utils.py.
RefactoringTool: No changes to planet.archlinux.org/planet/vendor/html5lib/filters/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/__init__.py to planet.archlinux.org3/planet/vendor/html5lib/filters/__init__.py.
RefactoringTool: No changes to planet.archlinux.org/planet/vendor/html5lib/filters/_base.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/_base.py to planet.archlinux.org3/planet/vendor/html5lib/filters/_base.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/filters/formfiller.py

-        if data == u">":
-            self.tokenQueue.append(self.currentToken)
-            self.state = self.dataState
-        elif data == u"-":
+        if data == ">":
+            self.tokenQueue.append(self.currentToken)
+            self.state = self.dataState
+        elif data == "-":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
              "unexpected-dash-after-double-dash-in-comment"})
             self.currentToken["data"] += data
@@ -1168,16 +1168,16 @@
             # XXX
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "unexpected-char-in-comment"})
-            self.currentToken["data"] += u"--" + data
+            self.currentToken["data"] += "--" + data
             self.state = self.commentState
         return True
 
     def commentEndBangState(self):
         data = self.stream.char()
-        if data == u">":
-            self.tokenQueue.append(self.currentToken)
-            self.state = self.dataState
-        elif data == u"-":
+        if data == ">":
+            self.tokenQueue.append(self.currentToken)
+            self.state = self.dataState
+        elif data == "-":
             self.currentToken["data"] += "--!"
             self.state = self.commentEndDashState
         elif data is EOF:
@@ -1186,16 +1186,16 @@
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         else:
-            self.currentToken["data"] += u"--!" + data
+            self.currentToken["data"] += "--!" + data
             self.state = self.commentState
         return True
 
     def commentEndSpaceState(self):
         data = self.stream.char()
-        if data == u">":
-            self.tokenQueue.append(self.currentToken)
-            self.state = self.dataState
-        elif data == u"-":
+        if data == ">":
+            self.tokenQueue.append(self.currentToken)
+            self.state = self.dataState
+        elif data == "-":
             self.state = self.commentEndDashState
         elif data in spaceCharacters:
             self.currentToken["data"] += data
@@ -1230,7 +1230,7 @@
         data = self.stream.char()
         if data in spaceCharacters:
             pass
-        elif data == u">":
+        elif data == ">":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "expected-doctype-name-but-got-right-bracket"})
             self.currentToken["correct"] = False
@@ -1252,7 +1252,7 @@
         if data in spaceCharacters:
             self.currentToken["name"] = self.currentToken["name"].translate(asciiUpper2Lower)
             self.state = self.afterDoctypeNameState
-        elif data == u">":
+        elif data == ">":
             self.currentToken["name"] = self.currentToken["name"].translate(asciiUpper2Lower)
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
@@ -1271,7 +1271,7 @@
         data = self.stream.char()
         if data in spaceCharacters:
             pass
-        elif data == u">":
+        elif data == ">":
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         elif data is EOF:
@@ -1282,10 +1282,10 @@
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         else:
-            if data in (u"p", u"P"):
+            if data in ("p", "P"):
                 matched = True
-                for expected in ((u"u", u"U"), (u"b", u"B"), (u"l", u"L"),
-                                 (u"i", u"I"), (u"c", u"C")):
+                for expected in (("u", "U"), ("b", "B"), ("l", "L"),
+                                 ("i", "I"), ("c", "C")):
                     data = self.stream.char()
                     if data not in expected:
                         matched = False
@@ -1293,10 +1293,10 @@
                 if matched:
                     self.state = self.afterDoctypePublicKeywordState
                     return True
-            elif data in (u"s", u"S"):
+            elif data in ("s", "S"):
                 matched = True
-                for expected in ((u"y", u"Y"), (u"s", u"S"), (u"t", u"T"),
-                                 (u"e", u"E"), (u"m", u"M")):
+                for expected in (("y", "Y"), ("s", "S"), ("t", "T"),
+                                 ("e", "E"), ("m", "M")):
                     data = self.stream.char()
                     if data not in expected:
                         matched = False
@@ -1343,10 +1343,10 @@
         if data in spaceCharacters:
             pass
         elif data == "\"":
-            self.currentToken["publicId"] = u""
+            self.currentToken["publicId"] = ""
             self.state = self.doctypePublicIdentifierDoubleQuotedState
         elif data == "'":
-            self.currentToken["publicId"] = u""
+            self.currentToken["publicId"] = ""
             self.state = self.doctypePublicIdentifierSingleQuotedState
         elif data == ">":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -1417,12 +1417,12 @@
         elif data == '"':
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "unexpected-char-in-doctype"})
-            self.currentToken["systemId"] = u""
+            self.currentToken["systemId"] = ""
             self.state = self.doctypeSystemIdentifierDoubleQuotedState
         elif data == "'":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
               "unexpected-char-in-doctype"})
-            self.currentToken["systemId"] = u""
+            self.currentToken["systemId"] = ""
             self.state = self.doctypeSystemIdentifierSingleQuotedState
         elif data is EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -1445,10 +1445,10 @@
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         elif data == '"':
-            self.currentToken["systemId"] = u""
+            self.currentToken["systemId"] = ""
             self.state = self.doctypeSystemIdentifierDoubleQuotedState
         elif data == "'":
-            self.currentToken["systemId"] = u""
+            self.currentToken["systemId"] = ""
             self.state = self.doctypeSystemIdentifierSingleQuotedState
         elif data == EOF:
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -1488,10 +1488,10 @@
         if data in spaceCharacters:
             pass
         elif data == "\"":
-            self.currentToken["systemId"] = u""
+            self.currentToken["systemId"] = ""
             self.state = self.doctypeSystemIdentifierDoubleQuotedState
         elif data == "'":
-            self.currentToken["systemId"] = u""
+            self.currentToken["systemId"] = ""
             self.state = self.doctypeSystemIdentifierSingleQuotedState
         elif data == ">":
             self.tokenQueue.append({"type": tokenTypes["ParseError"], "data":
@@ -1573,7 +1573,7 @@
 
     def bogusDoctypeState(self):
         data = self.stream.char()
-        if data == u">":
+        if data == ">":
             self.tokenQueue.append(self.currentToken)
             self.state = self.dataState
         elif data is EOF:
--- planet.archlinux.org/planet/vendor/html5lib/utils.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/utils.py	(refactored)
@@ -90,7 +90,7 @@
     def rotate(self, n=1):
         if self:
             n %= len(self)
-            for i in xrange(n):
+            for i in range(n):
                 self.appendleft(self.pop())
 
     def __getitem__(self, i):
@@ -116,7 +116,7 @@
         data = self.data
         if i < 0:
             i += size
-        for j in xrange(self.left+i, self.right-1):
+        for j in range(self.left+i, self.right-1):
             data[j] = data[j+1]
         self.pop()
     
--- planet.archlinux.org/planet/vendor/html5lib/filters/formfiller.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/filters/formfiller.py	(refactored)
@@ -4,10 +4,10 @@
 # See http://www.whatwg.org/specs/web-forms/current-work/#seedingRefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/formfiller.py to planet.archlinux.org3/planet/vendor/html5lib/filters/formfiller.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/filters/inject_meta_charset.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/inject_meta_charset.py to planet.archlinux.org3/planet/vendor/html5lib/filters/inject_meta_charset.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/filters/lint.py

 #
 
-import _base
+from . import _base
 
 from html5lib.constants import spaceCharacters
-spaceCharacters = u"".join(spaceCharacters)
+spaceCharacters = "".join(spaceCharacters)
 
 class SimpleFilter(_base.Filter):
     def __init__(self, source, fieldStorage):
@@ -29,13 +29,13 @@
                     input_checked_index = -1
                     for i,(n,v) in enumerate(token["data"]):
                         n = n.lower()
-                        if n == u"name":
+                        if n == "name":
                             field_name = v.strip(spaceCharacters)
-                        elif n == u"type":
+                        elif n == "type":
                             field_type = v.strip(spaceCharacters)
-                        elif n == u"checked":
+                        elif n == "checked":
                             input_checked_index = i
-                        elif n == u"value":
+                        elif n == "value":
                             input_value_index = i
 
                     value_list = self.fieldStorage.getlist(field_name)
@@ -45,20 +45,20 @@
                     else:
                         value = ""
 
-                    if field_type in (u"checkbox", u"radio"):
+                    if field_type in ("checkbox", "radio"):
                         if value_list:
                             if token["data"][input_value_index][1] == value:
                                 if input_checked_index < 0:
-                                    token["data"].append((u"checked", u""))
+                                    token["data"].append(("checked", ""))
                                 field_indices[field_name] = field_index + 1
                             elif input_checked_index >= 0:
                                 del token["data"][input_checked_index]
 
-                    elif field_type not in (u"button", u"submit", u"reset"):
+                    elif field_type not in ("button", "submit", "reset"):
                         if input_value_index >= 0:
-                            token["data"][input_value_index] = (u"value", value)
+                            token["data"][input_value_index] = ("value", value)
                         else:
-                            token["data"].append((u"value", value))
+                            token["data"].append(("value", value))
                         field_indices[field_name] = field_index + 1
 
                     field_type = None
@@ -96,7 +96,7 @@
                                 value = ""
                             if (is_select_multiple or not is_selected_option_found) and option_value == value:
                                 if option_selected_index < 0:
-                                    token["data"].append((u"selected", u""))
+                                    token["data"].append(("selected", ""))
                                 field_indices[field_name] = field_index + 1
                                 is_selected_option_found = True
                             elif option_selected_index >= 0:
--- planet.archlinux.org/planet/vendor/html5lib/filters/inject_meta_charset.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/filters/inject_meta_charset.py	(refactored)
@@ -1,4 +1,4 @@
-import _base
+from . import _base
 
 class Filter(_base.Filter):
     def __init__(self, source, encoding):
@@ -23,7 +23,7 @@
                    content_index = -1
                    for i,(name,value) in enumerate(token["data"]):
                        if name.lower() == 'charset':
-                          token["data"][i] = (u'charset', self.encoding)
+                          token["data"][i] = ('charset', self.encoding)
                           meta_found = True
                           break
                        elif name == 'http-equiv' and value.lower() == 'content-type':
@@ -32,7 +32,7 @@
                            content_index = i
                    else:
                        if has_http_equiv_content_type and content_index >= 0:
-                           token["data"][content_index] = (u'content', u'text/html; charset=%s' % self.encoding)
+                           token["data"][content_index] = ('content', 'text/html; charset=%s' % self.encoding)
                            meta_found = True
 
                 elif token["name"].lower() == "head" and not meta_found:
--- planet.archlinux.org/planet/vendor/html5lib/filters/lint.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/filters/lint.py	(refactored)
@@ -1,11 +1,11 @@
 from gettext import gettext
 _ = gettext
 
-import _base
+from . import _base
 from html5lib.constants import cdataElements, rcdataElements, voidElements
 
 from html5lib.constants import spaceCharacters
-spaceCharacters = u"".join(spaceCharacters)
+spaceCharacters = "".join(spaceCharacters)
 
 class LintError(Exception): pass
 
@@ -19,22 +19,22 @@
                 name = token["name"]
                 if contentModelFlag != "PCDATA":
                     raise LintError(_("StartTag not in PCDATA content model flag: %s") % name)
-                if not isinstance(name, unicode):
-                    raise LintError(_(u"Tag name is not a string: %r") % name)
+                if not isinstance(name, str):
+                    raise LintError(_("Tag name is not a string: %r") % name)
                 if not name:
-                    raise LintError(_(u"Empty tag name"))
+                    raise LintError(_("Empty tag name"))
                 if type == "StartTag" and name in voidElements:
-                    raise LintError(_(u"Void element reported as StartTag token: %s") % name)
+                    raise LintError(_("Void element reported as StartTag token: %s") % name)
                 elif type == "EmptyTag" and name not in voidElements:
-                    raise LintError(_(u"Non-void element reported as EmptyTag token: %s") % token["name"])
+                    raise LintError(_("Non-void element reported as EmptyTag token: %s") % token["name"])
                 if type == "StartTag":
                     open_elements.append(name)
                 for name, value in token["data"]:
-                    if not isinstance(name, unicode):
+                    if not isinstance(name, str):
                         raise LintError(_("Attribute name is not a string: %r") % name)
                     if not name:
-                        raise LintError(_(u"Empty attribute name"))
-                    if not isinstance(value, unicode):
+                        raise LintError(_("Empty attribute name"))
+                    if not isinstance(value, str):
                         raise LintError(_("Attribute value is not a string: %r") % value)
                 if name in cdataElements:
                     contentModelFlag = "CDATA"
@@ -45,15 +45,15 @@
 
             elif type == "EndTag":
                 name = token["name"]
-                if not isinstance(name, unicode):
-                    raise LintError(_(u"Tag name is not a string: %r") % name)
+                if not isinstance(name, str):
+                    raise LintError(_("Tag name is not a string: %r") % name)
                 if not name:
-                    raise LintError(_(u"Empty tag name"))
+                    raise LintError(_("Empty tag name"))
                 if name in voidElements:
-                    raise LintError(_(u"Void element reported as EndTag token: %s") % name)
+                    raise LintError(_("Void element reported as EndTag token: %s") % name)
                 start_name = open_elements.pop()
                 if start_name != name:
-                    raise LintError(_(u"EndTag (%s) does not match StartTag (%s)") % (name, start_name))
+                    raise LintError(_("EndTag (%s) does not match StartTag (%s)") % (name, start_name))
                 contentModelFlag = "PCDATA"
 
             elif type == "Comment":
@@ -62,27 +62,27 @@
 
             elif type in ("Characters", "SpaceCharacters"):
                 data = token["data"]
-                if not isinstance(data, unicode):
+                if not isinstance(data, str):
                     raise LintError(_("Attribute name is not a string: %r") % data)RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/lint.py to planet.archlinux.org3/planet/vendor/html5lib/filters/lint.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/filters/optionaltags.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/optionaltags.py to planet.archlinux.org3/planet/vendor/html5lib/filters/optionaltags.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/filters/sanitizer.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/sanitizer.py to planet.archlinux.org3/planet/vendor/html5lib/filters/sanitizer.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/filters/whitespace.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/filters/whitespace.py to planet.archlinux.org3/planet/vendor/html5lib/filters/whitespace.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/serializer/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/serializer/__init__.py to planet.archlinux.org3/planet/vendor/html5lib/serializer/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/serializer/htmlserializer.py

                 if not data:
-                    raise LintError(_(u"%s token with empty data") % type)
+                    raise LintError(_("%s token with empty data") % type)
                 if type == "SpaceCharacters":
                     data = data.strip(spaceCharacters)
                     if data:
-                        raise LintError(_(u"Non-space character(s) found in SpaceCharacters token: ") % data)
+                        raise LintError(_("Non-space character(s) found in SpaceCharacters token: ") % data)
 
             elif type == "Doctype":
                 name = token["name"]
                 if contentModelFlag != "PCDATA":
                     raise LintError(_("Doctype not in PCDATA content model flag: %s") % name)
-                if not isinstance(name, unicode):
-                    raise LintError(_(u"Tag name is not a string: %r") % name)
+                if not isinstance(name, str):
+                    raise LintError(_("Tag name is not a string: %r") % name)
                 # XXX: what to do with token["data"] ?
 
             elif type in ("ParseError", "SerializeError"):
                 pass
 
             else:
-                raise LintError(_(u"Unknown token type: %s") % type)
+                raise LintError(_("Unknown token type: %s") % type)
 
             yield token
--- planet.archlinux.org/planet/vendor/html5lib/filters/optionaltags.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/filters/optionaltags.py	(refactored)
@@ -1,4 +1,4 @@
-import _base
+from . import _base
 
 class Filter(_base.Filter):
     def slider(self):
--- planet.archlinux.org/planet/vendor/html5lib/filters/sanitizer.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/filters/sanitizer.py	(refactored)
@@ -1,4 +1,4 @@
-import _base
+from . import _base
 from html5lib.sanitizer import HTMLSanitizerMixin
 
 class Filter(_base.Filter, HTMLSanitizerMixin):
--- planet.archlinux.org/planet/vendor/html5lib/filters/whitespace.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/filters/whitespace.py	(refactored)
@@ -6,11 +6,11 @@
 
 import re
 
-import _base
+from . import _base
 from html5lib.constants import rcdataElements, spaceCharacters
-spaceCharacters = u"".join(spaceCharacters)
+spaceCharacters = "".join(spaceCharacters)
 
-SPACES_REGEX = re.compile(u"[%s]+" % spaceCharacters)
+SPACES_REGEX = re.compile("[%s]+" % spaceCharacters)
 
 class Filter(_base.Filter):
 
@@ -29,7 +29,7 @@
 
             elif not preserve and type == "SpaceCharacters" and token["data"]:
                 # Test on token["data"] above to not introduce spaces where there were not
-                token["data"] = u" "
+                token["data"] = " "
 
             elif not preserve and type == "Characters":
                 token["data"] = collapse_spaces(token["data"])
--- planet.archlinux.org/planet/vendor/html5lib/serializer/__init__.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/serializer/__init__.py	(refactored)
@@ -1,8 +1,8 @@
 
 from html5lib import treewalkers
 
-from htmlserializer import HTMLSerializer
-from xhtmlserializer import XHTMLSerializer
+from .htmlserializer import HTMLSerializer
+from .xhtmlserializer import XHTMLSerializer
 
 def serialize(input, tree="simpletree", format="html", encoding=None,
               **serializer_opts):
@@ -13,5 +13,5 @@
     elif format == "xhtml":
         s = XHTMLSerializer(**serializer_opts)
     else:
-        raise ValueError, "type must be either html or xhtml"
+        raise ValueError("type must be either html or xhtml")
     return s.render(walker(input), encoding)
--- planet.archlinux.org/planet/vendor/html5lib/serializer/htmlserializer.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/serializer/htmlserializer.py	(refactored)
@@ -5,6 +5,7 @@
     from sets import ImmutableSet as frozenset
 
 import gettext
+from functools import reduce
 _ = gettext.gettext
 
 from html5lib.constants import voidElements, booleanAttributes, spaceCharacters
@@ -12,7 +13,7 @@
 from html5lib import utils
 from xml.sax.saxutils import escape
 
-spaceCharacters = u"".join(spaceCharacters)
+spaceCharacters = "".join(spaceCharacters)
 
 try:
     from codecs import register_error, xmlcharrefreplace_errors
@@ -24,7 +25,7 @@
     from html5lib.constants import entities
 
     encode_entity_map = {}
-    for k, v in entities.items():
+    for k, v in list(entities.items()):
         if v != "&" and encode_entity_map.get(v) != k.lower():
             # prefer &lt; over &LT; and similarly for &amp;, &gt;, etc.
             encode_entity_map[ord(v)] = k
@@ -54,7 +55,7 @@
                         res.append(";")
                 else:
                     res.append("&#x%s;"%(hex(cp)[2:]))
-            return (u"".join(res), exc.end)
+            return ("".join(res), exc.end)
         else:
             return xmlcharrefreplace_errors(exc)
 
@@ -95,7 +96,7 @@
           "escape_rcdata", "resolve_entities", "sanitize")
 
     def __init__(self, **kwargs):
-        if kwargs.has_key('quote_char'):
+        if 'quote_char' in kwargs:
             self.use_best_quote_char = False
         for attr in self.options:
             setattr(self, attr, kwargs.get(attr, getattr(self, attr)))
@@ -122,22 +123,22 @@
         for token in treewalker:
             type = token["type"]
             if type == "Doctype":
-                doctype = u"<!DOCTYPE %s" % token["name"]
+                doctype = "<!DOCTYPE %s" % token["name"]
                 
                 if token["publicId"]:
-                    doctype += u' PUBLIC "%s"' % token["publicId"]
+                    doctype += ' PUBLIC "%s"' % token["publicId"]
                 elif token["systemId"]:
-                    doctype += u" SYSTEM"
+                    doctype += " SYSTEM"
                 if token["systemId"]:                
-                    if token["systemId"].find(u'"') >= 0:
-                        if token["systemId"].find(u"'") >= 0:
+                    if token["systemId"].find('"') >= 0:
+                        if token["systemId"].find("'") >= 0:
                             self.serializeError(_("System identifer contains both single and double quote characters"))
-                        quote_char = u"'"
+                        quote_char = "'"
                     else:
-                        quote_char = u'"'
-                    doctype += u" %s%s%s" % (quote_char, token["systemId"], quote_char)
+                        quote_char = '"'
+                    doctype += " %s%s%s" % (quote_char, token["systemId"], quote_char)
                 
-                doctype += u">"
+                doctype += ">"
                 
                 if encoding:
                     yield doctype.encode(encoding)
@@ -165,7 +166,7 @@
                     self.serializeError(_("Unexpected child element of a CDATA element"))
                 attrs = token["data"]
                 if hasattr(attrs, "items"):
-                    attrs = attrs.items()
+                    attrs = list(attrs.items())
                 attrs.sort()
                 attributes = []
                 for k,v in attrs:
@@ -211,7 +212,7 @@
                 if encoding:
                     yield "<%s%s>" % (name.encode(encoding, "strict"), "".join(attributes))
                 else:
-                    yield u"<%s%s>" % (name, u"".join(attributes))
+                    yield "<%s%s>" % (name, "".join(attributes))
 
             elif type == "EndTag":
                 name = token["name"]
@@ -219,7 +220,7 @@
                     in_cdata = False
                 elif in_cdata:
                     self.serializeError(_("Unexpected child element of a CDATA element"))
-                end_tag = u"</%s>" % name
+                end_tag = "</%s>" % name
                 if encoding:
                     end_tag = end_tag.encode(encoding, "strict")
                 yield end_tag
@@ -228,7 +229,7 @@
                 data = token["data"]
                 if data.find("--") >= 0:
                     self.serializeError(_("Comment contains --"))
-                comment = u"<!--%s-->" % token["data"]
+                comment = "<!--%s-->" % token["data"]RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/serializer/htmlserializer.py to planet.archlinux.org3/planet/vendor/html5lib/serializer/htmlserializer.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/serializer/xhtmlserializer.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/serializer/xhtmlserializer.py to planet.archlinux.org3/planet/vendor/html5lib/serializer/xhtmlserializer.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treebuilders/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treebuilders/__init__.py to planet.archlinux.org3/planet/vendor/html5lib/treebuilders/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treebuilders/_base.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treebuilders/_base.py to planet.archlinux.org3/planet/vendor/html5lib/treebuilders/_base.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treebuilders/dom.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treebuilders/dom.py to planet.archlinux.org3/planet/vendor/html5lib/treebuilders/dom.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree.py

                 if encoding:
                     comment = comment.encode(encoding, unicode_encode_errors)
                 yield comment
@@ -241,7 +242,7 @@
                 if self.resolve_entities and key not in xmlEntities:
                     data = entities[key]
                 else:
-                    data = u"&%s;" % name
+                    data = "&%s;" % name
                 if encoding:
                     data = data.encode(encoding, unicode_encode_errors)
                 yield data
@@ -253,7 +254,7 @@
         if encoding:
             return "".join(list(self.serialize(treewalker, encoding)))
         else:
-            return u"".join(list(self.serialize(treewalker)))
+            return "".join(list(self.serialize(treewalker)))
 
     def serializeError(self, data="XXX ERROR MESSAGE NEEDED"):
         # XXX The idea is to make data mandatory.
--- planet.archlinux.org/planet/vendor/html5lib/serializer/xhtmlserializer.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/serializer/xhtmlserializer.py	(refactored)
@@ -1,4 +1,4 @@
-from htmlserializer import HTMLSerializer
+from .htmlserializer import HTMLSerializer
 
 class XHTMLSerializer(HTMLSerializer):
     quote_attr_values = True
--- planet.archlinux.org/planet/vendor/html5lib/treebuilders/__init__.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treebuilders/__init__.py	(refactored)
@@ -56,7 +56,7 @@
     treeType = treeType.lower()
     if treeType not in treeBuilderCache:
         if treeType == "dom":
-            import dom
+            from . import dom
             # XXX: Keep backwards compatibility by using minidom if no implementation is given
             if implementation == None:
                 from xml.dom import minidom
@@ -64,13 +64,13 @@
             # XXX: NEVER cache here, caching is done in the dom submodule
             return dom.getDomModule(implementation, **kwargs).TreeBuilder
         elif treeType == "simpletree":
-            import simpletree
+            from . import simpletree
             treeBuilderCache[treeType] = simpletree.TreeBuilder
         elif treeType == "beautifulsoup":
-            import soup
+            from . import soup
             treeBuilderCache[treeType] = soup.TreeBuilder
         elif treeType == "lxml":
-            import etree_lxml
+            from . import etree_lxml
             treeBuilderCache[treeType] = etree_lxml.TreeBuilder
         elif treeType == "etree":
             # Come up with a sane default
@@ -86,7 +86,7 @@
                         except ImportError:
                             import elementtree.ElementTree as ET
                 implementation = ET
-            import etree
+            from . import etree
             # NEVER cache here, caching is done in the etree submodule
             return etree.getETreeModule(implementation, **kwargs).TreeBuilder
         else:
--- planet.archlinux.org/planet/vendor/html5lib/treebuilders/_base.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treebuilders/_base.py	(refactored)
@@ -33,7 +33,7 @@
     def __unicode__(self):
         attributesStr =  " ".join(["%s=\"%s\""%(name, value) 
                                    for name, value in 
-                                   self.attributes.iteritems()])
+                                   self.attributes.items()])
         if attributesStr:
             return "<%s %s>"%(self.name,attributesStr)
         else:
--- planet.archlinux.org/planet/vendor/html5lib/treebuilders/dom.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treebuilders/dom.py	(refactored)
@@ -4,7 +4,7 @@
 import re
 import weakref
 
-import _base
+from . import _base
 from html5lib import constants, ihatexml
 from html5lib.constants import namespaces
 
@@ -27,14 +27,14 @@
         def __init__(self, element):
             self.element = element
         def __iter__(self):
-            return self.element.attributes.items().__iter__()
+            return list(self.element.attributes.items()).__iter__()
         def __setitem__(self, name, value):
             self.element.setAttribute(name, value)
         def items(self):
             return [(item[0], item[1]) for item in
-                     self.element.attributes.items()]
+                     list(self.element.attributes.items())]
         def keys(self):
-            return self.element.attributes.keys()
+            return list(self.element.attributes.keys())
         def __getitem__(self, name):
             return self.element.getAttribute(name)
 
@@ -84,7 +84,7 @@
     
         def setAttributes(self, attributes):
             if attributes:
-                for name, value in attributes.items():
+                for name, value in list(attributes.items()):
                     if isinstance(name, tuple):
                         if name[0] is not None:
                             qualifiedName = (name[0] + ":" + name[1])
@@ -155,7 +155,7 @@
     
         def insertText(self, data, parent=None):
             data=data
-            if parent <> self:
+            if parent != self:
                 _base.TreeBuilder.insertText(self, data, parent)
             else:
                 # HACK: allow text nodes as children of the document node
@@ -231,7 +231,7 @@
     
           # gather namespace declarations
           prefixes = []
-          for attrname in node.attributes.keys():
+          for attrname in list(node.attributes.keys()):
             attr = node.getAttributeNode(attrname)
             if (attr.namespaceURI == XMLNS_NAMESPACE or
                (attr.namespaceURI == None and attr.nodeName.startswith('xmlns'))):
@@ -243,11 +243,11 @@
               del attributes[(attr.namespaceURI, attr.nodeName)]
     
           # apply namespace declarations
-          for attrname in node.attributes.keys():
+          for attrname in list(node.attributes.keys()):
             attr = node.getAttributeNode(attrname)
             if attr.namespaceURI == None and ':' in attr.nodeName:
               prefix = attr.nodeName.split(':')[0]
-              if nsmap.has_key(prefix):
+              if prefix in nsmap:
                 del attributes[(attr.namespaceURI, attr.nodeName)]
                 attributes[(nsmap[prefix],attr.nodeName)]=attr.nodeValue
     
@@ -282,5 +282,5 @@
 
 # Keep backwards compatibility with things that directly load 
 # classes/functions from this module
-for key, value in getDomModule(minidom).__dict__.items():
+for key, value in list(getDomModule(minidom).__dict__.items()):
 	globals()[key] = value
--- planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree.py	(refactored)
@@ -1,7 +1,7 @@
 import new
 import re
 
-import _base
+from . import _base
 from html5lib import ihatexml
 from html5lib import constants
 from html5lib.constants import namespaces
@@ -68,9 +68,9 @@
         def _setAttributes(self, attributes):
             #Delete existing attributes first
             #XXX - there may be a better way to do this...
-            for key in self._element.attrib.keys():
+            for key in list(self._element.attrib.keys()):
                 del self._element.attrib[key]
-            for key, value in attributes.iteritems():
+            for key, value in attributes.items():
                 if isinstance(key, tuple):
                     name = "{%s}%s"%(key[2], key[1])
                 else:
@@ -132,7 +132,7 @@
     
         def cloneNode(self):
             element = Element(self.name, self.namespace)
-            for name, value in self.attributes.iteritems():
+            for name, value in self.attributes.items():
                 element.attributes[name] = value
             return element
     
@@ -172,20 +172,20 @@
             self.systemId = systemId
 
         def _getPublicId(self):
-            return self._element.get(u"publicId", "")
+            return self._element.get("publicId", "")
 
         def _setPublicId(self, value):
             if value is not None:
-                self._element.set(u"publicId", value)
+                self._element.set("publicId", value)
 
         publicId = property(_getPublicId, _setPublicId)RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree.py to planet.archlinux.org3/planet/vendor/html5lib/treebuilders/etree.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree_lxml.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree_lxml.py to planet.archlinux.org3/planet/vendor/html5lib/treebuilders/etree_lxml.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treebuilders/simpletree.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treebuilders/simpletree.py to planet.archlinux.org3/planet/vendor/html5lib/treebuilders/simpletree.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treebuilders/soup.py

     
         def _getSystemId(self):
-            return self._element.get(u"systemId", "")
+            return self._element.get("systemId", "")
 
         def _setSystemId(self, value):
             if value is not None:
-                self._element.set(u"systemId", value)
+                self._element.set("systemId", value)
 
         systemId = property(_getSystemId, _setSystemId)
     
@@ -231,7 +231,7 @@
                 rv.append("|%s<%s>"%(' '*indent, name))
 
                 if hasattr(element, "attrib"):
-                    for name, value in element.attrib.iteritems():
+                    for name, value in element.attrib.items():
                         nsmatch = tag_regexp.match(name)
                         if nsmatch is not None:
                             ns, name = nsmatch.groups()
@@ -287,7 +287,7 @@
                 else:
                     attr = " ".join(["%s=\"%s\""%(
                                 filter.fromXmlName(name), value) 
-                                     for name, value in element.attrib.iteritems()])
+                                     for name, value in element.attrib.items()])
                     rv.append("<%s %s>"%(element.tag, attr))
                 if element.text:
                     rv.append(element.text)
--- planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree_lxml.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree_lxml.py	(refactored)
@@ -2,10 +2,10 @@
 import warnings
 import re
 
-import _base
+from . import _base
 from html5lib.constants import DataLossWarning
 import html5lib.constants as constants
-import etree as etree_builders
+from . import etree as etree_builders
 from html5lib import ihatexml
 
 try:
@@ -70,7 +70,7 @@
                 while next_element is not None:
                     serializeElement(next_element, indent+2)
                     next_element = next_element.getnext()
-            elif isinstance(element, basestring):
+            elif isinstance(element, str):
                 #Text in a fragment
                 rv.append("|%s\"%s\""%(' '*indent, element))
             else:
@@ -93,7 +93,7 @@
                                      filter.fromXmlName(element.tag)))
 
             if hasattr(element, "attrib"):
-                for name, value in element.attrib.iteritems():
+                for name, value in element.attrib.items():
                     nsmatch = etree_builders.tag_regexp.match(name)
                     if nsmatch:
                         ns = nsmatch.group(1)
@@ -144,7 +144,7 @@
                 rv.append("<%s>"%(element.tag,))
             else:
                 attr = " ".join(["%s=\"%s\""%(name, value) 
-                                 for name, value in element.attrib.iteritems()])
+                                 for name, value in element.attrib.items()])
                 rv.append("<%s %s>"%(element.tag, attr))
             if element.text:
                 rv.append(element.text)
@@ -181,7 +181,7 @@
             def __init__(self, element, value={}):
                 self._element = element
                 dict.__init__(self, value)
-                for key, value in self.iteritems():
+                for key, value in self.items():
                     if isinstance(key, tuple):
                         name = "{%s}%s"%(key[2], filter.coerceAttribute(key[1]))
                     else:
@@ -305,7 +305,7 @@
         try:
             root = etree.fromstring(docStr)
         except etree.XMLSyntaxError:
-            print docStr
+            print(docStr)
             raise
         
         #Append the initial comments:
--- planet.archlinux.org/planet/vendor/html5lib/treebuilders/simpletree.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treebuilders/simpletree.py	(refactored)
@@ -1,4 +1,4 @@
-import _base
+from . import _base
 from html5lib.constants import voidElements, namespaces, prefixes
 from xml.sax.saxutils import escape
 
@@ -25,7 +25,7 @@
         raise NotImplementedError
 
     def printTree(self, indent=0):
-        tree = '\n|%s%s' % (' '* indent, unicode(self))
+        tree = '\n|%s%s' % (' '* indent, str(self))
         for child in self.childNodes:
             tree += child.printTree(indent + 2)
         return tree
@@ -100,7 +100,7 @@
         return result.encode(encoding) + "</pre>"
     
     def printTree(self):
-        tree = unicode(self)
+        tree = str(self)
         for child in self.childNodes:
             tree += child.printTree(2)
         return tree
@@ -131,7 +131,7 @@
                 self.name, publicId, systemId)
                             
         else:
-            return u"<!DOCTYPE %s>" % self.name
+            return "<!DOCTYPE %s>" % self.name
     
 
     toxml = __unicode__
@@ -149,7 +149,7 @@
         self.value = value
 
     def __unicode__(self):
-        return u"\"%s\"" % self.value
+        return "\"%s\"" % self.value
 
     def toxml(self):
         return escape(self.value)
@@ -168,28 +168,28 @@
 
     def __unicode__(self):
         if self.namespace == None:
-            return u"<%s>" % self.name
-        else:
-            return u"<%s %s>"%(prefixes[self.namespace], self.name)
+            return "<%s>" % self.name
+        else:
+            return "<%s %s>"%(prefixes[self.namespace], self.name)
 
     def toxml(self):
         result = '<' + self.name
         if self.attributes:
-            for name,value in self.attributes.iteritems():
-                result += u' %s="%s"' % (name, escape(value,{'"':'&quot;'}))
+            for name,value in self.attributes.items():
+                result += ' %s="%s"' % (name, escape(value,{'"':'&quot;'}))
         if self.childNodes:
             result += '>'
             for child in self.childNodes:
                 result += child.toxml()
-            result += u'</%s>' % self.name
-        else:
-            result += u'/>'
+            result += '</%s>' % self.name
+        else:
+            result += '/>'
         return result
     
     def hilite(self):
         result = '&lt;<code class="markup element-name">%s</code>' % self.name
         if self.attributes:
-            for name, value in self.attributes.iteritems():
+            for name, value in self.attributes.items():
                 result += ' <code class="markup attribute-name">%s</code>=<code class="markup attribute-value">"%s"</code>' % (name, escape(value, {'"':'&quot;'}))
         if self.childNodes:
             result += ">"
@@ -200,10 +200,10 @@
         return result + '&lt;/<code class="markup element-name">%s</code>>' % self.name
 
     def printTree(self, indent):
-        tree = '\n|%s%s' % (' '*indent, unicode(self))
+        tree = '\n|%s%s' % (' '*indent, str(self))
         indent += 2
         if self.attributes:
-            for name, value in self.attributes.iteritems():
+            for name, value in self.attributes.items():
                 if isinstance(name, tuple):
                     name = "%s %s"%(name[0], name[1])
                 tree += '\n|%s%s="%s"' % (' ' * indent, name, value)
@@ -215,7 +215,7 @@
         newNode = Element(self.name)
         if hasattr(self, 'namespace'):
             newNode.namespace = self.namespace
-        for attr, value in self.attributes.iteritems():
+        for attr, value in self.attributes.items():
             newNode.attributes[attr] = value
         return newNode
 
--- planet.archlinux.org/planet/vendor/html5lib/treebuilders/soup.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treebuilders/soup.py	(refactored)
@@ -4,7 +4,7 @@
 
 from BeautifulSoup import BeautifulSoup, Tag, NavigableString, Comment, Declaration
 
-import _base
+from . import _base
 from html5lib.constants import namespaces, DataLossWarning
 
 class AttrList(object):
@@ -12,18 +12,18 @@
         self.element = element
         self.attrs = dict(self.element.attrs)
     def __iter__(self):
-        return self.attrs.items().__iter__()
+        return list(self.attrs.items()).__iter__()
     def __setitem__(self, name, value):
         "set attr", name, value
         self.element[name] = value
     def items(self):
-        return self.attrs.items()RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treebuilders/soup.py to planet.archlinux.org3/planet/vendor/html5lib/treebuilders/soup.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/__init__.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/_base.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/_base.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/_base.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/dom.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/dom.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/dom.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/etree.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/etree.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/etree.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/genshistream.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/genshistream.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/genshistream.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/lxmletree.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/lxmletree.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/lxmletree.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/pulldom.py

+        return list(self.attrs.items())
     def keys(self):
-        return self.attrs.keys()
+        return list(self.attrs.keys())
     def __getitem__(self, name):
         return self.attrs[name]
     def __contains__(self, name):
-        return name in self.attrs.keys()
+        return name in list(self.attrs.keys())
 
 
 class Element(_base.Node):
@@ -65,7 +65,7 @@
 
     def setAttributes(self, attributes):
         if attributes:
-            for name, value in attributes.items():
+            for name, value in list(attributes.items()):
                 self.element[name] =  value
 
     attributes = property(getAttributes, setAttributes)
@@ -212,7 +212,7 @@
 
         elif isinstance(element, Comment):
             rv.append("|%s<!-- %s -->"%(' '*indent, element.string))
-        elif isinstance(element, unicode):
+        elif isinstance(element, str):
             rv.append("|%s\"%s\"" %(' '*indent, element))
         else:
             rv.append("|%s<%s>"%(' '*indent, element.name))
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/__init__.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/__init__.py	(refactored)
@@ -37,16 +37,16 @@
             mod = __import__(treeType, globals())
             treeWalkerCache[treeType] = mod.TreeWalker
         elif treeType == "genshi":
-            import genshistream
+            from . import genshistream
             treeWalkerCache[treeType] = genshistream.TreeWalker
         elif treeType == "beautifulsoup":
-            import soup
+            from . import soup
             treeWalkerCache[treeType] = soup.TreeWalker
         elif treeType == "lxml":
-            import lxmletree
+            from . import lxmletree
             treeWalkerCache[treeType] = lxmletree.TreeWalker
         elif treeType == "etree":
-            import etree
+            from . import etree
             # XXX: NEVER cache here, caching is done in the etree submodule
             return etree.getETreeModule(implementation, **kwargs).TreeWalker
     return treeWalkerCache.get(treeType)
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/_base.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/_base.py	(refactored)
@@ -2,7 +2,7 @@
 _ = gettext.gettext
 
 from html5lib.constants import voidElements, spaceCharacters
-spaceCharacters = u"".join(spaceCharacters)
+spaceCharacters = "".join(spaceCharacters)
 
 class TreeWalker(object):
     def __init__(self, tree):
@@ -18,30 +18,30 @@
         if not attrs:
             attrs = []
         elif hasattr(attrs, 'items'):
-            attrs = attrs.items()
-        return [(unicode(name),unicode(value)) for name,value in attrs]
+            attrs = list(attrs.items())
+        return [(str(name),str(value)) for name,value in attrs]
 
     def emptyTag(self, namespace, name, attrs, hasChildren=False):
-        yield {"type": "EmptyTag", "name": unicode(name), 
-               "namespace":unicode(namespace),
+        yield {"type": "EmptyTag", "name": str(name), 
+               "namespace":str(namespace),
                "data": self.normalizeAttrs(attrs)}
         if hasChildren:
             yield self.error(_("Void element has children"))
 
     def startTag(self, namespace, name, attrs):
         return {"type": "StartTag", 
-                "name": unicode(name),
-                "namespace":unicode(namespace),
+                "name": str(name),
+                "namespace":str(namespace),
                 "data": self.normalizeAttrs(attrs)}
 
     def endTag(self, namespace, name):
         return {"type": "EndTag", 
-                "name": unicode(name),
-                "namespace":unicode(namespace),
+                "name": str(name),
+                "namespace":str(namespace),
                 "data": []}
 
     def text(self, data):
-        data = unicode(data)
+        data = str(data)
         middle = data.lstrip(spaceCharacters)
         left = data[:len(data)-len(middle)]
         if left:
@@ -55,17 +55,17 @@
             yield {"type": "SpaceCharacters", "data": right}
 
     def comment(self, data):
-        return {"type": "Comment", "data": unicode(data)}
+        return {"type": "Comment", "data": str(data)}
 
     def doctype(self, name, publicId=None, systemId=None, correct=True):
         return {"type": "Doctype",
-                "name": name is not None and unicode(name) or u"",
+                "name": name is not None and str(name) or "",
                 "publicId": publicId,
                 "systemId": systemId,
                 "correct": correct}
 
     def entity(self, name):
-        return {"type": "Entity", "name": unicode(name)}
+        return {"type": "Entity", "name": str(name)}
 
     def unknown(self, nodeType):
         return self.error(_("Unknown node type: ") + nodeType)
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/dom.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/dom.py	(refactored)
@@ -3,7 +3,7 @@
 import gettext
 _ = gettext.gettext
 
-import _base
+from . import _base
 from html5lib.constants import voidElements
 
 class TreeWalker(_base.NonRecursiveTreeWalker):
@@ -16,7 +16,7 @@
 
         elif node.nodeType == Node.ELEMENT_NODE:
             return (_base.ELEMENT, node.namespaceURI, node.nodeName, 
-                    node.attributes.items(), node.hasChildNodes)
+                    list(node.attributes.items()), node.hasChildNodes)
 
         elif node.nodeType == Node.COMMENT_NODE:
             return _base.COMMENT, node.nodeValue
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/etree.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/etree.py	(refactored)
@@ -5,7 +5,7 @@
 import copy
 import re
 
-import _base
+from . import _base
 from html5lib.constants import voidElements
 
 tag_regexp = re.compile("{([^}]*)}(.*)")
@@ -70,7 +70,7 @@
                     namespace = None
                     tag = node.tag
                 return (_base.ELEMENT, namespace, tag, 
-                        node.attrib.items(), len(node) or node.text)
+                        list(node.attrib.items()), len(node) or node.text)
     
         def getFirstChild(self, node):
             if isinstance(node, tuple):
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/genshistream.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/genshistream.py	(refactored)
@@ -2,7 +2,7 @@
 from genshi.core  import  START_NS, END_NS, START_CDATA, END_CDATA, PI, COMMENT
 from genshi.output import NamespaceFlattener
 
-import _base
+from . import _base
 
 from html5lib.constants import voidElements
 
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/lxmletree.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/lxmletree.py	(refactored)
@@ -4,7 +4,7 @@
 from gettext import gettext
 _ = gettext
 
-import _base
+from . import _base
 
 from html5lib.constants import voidElements
 from html5lib import ihatexml
@@ -71,7 +71,7 @@
             self.tail = self.obj.tail
         else:
             self.tail = None
-        self.isstring = isinstance(obj, basestring)
+        self.isstring = isinstance(obj, str)
         
     def __getattr__(self, name):
         return getattr(self.obj, name)
@@ -87,7 +87,7 @@
     def __getitem__(self, key):
         return self.obj[key]
 
-    def __nonzero__(self):
+    def __bool__(self):
         return bool(self.obj)
 
     def getparent(self):
@@ -97,7 +97,7 @@
         return str(self.obj)
 
     def __unicode__(self):
-        return unicode(self.obj)
+        return str(self.obj)
 
     def __len__(self):
         return len(self.obj)
@@ -142,7 +142,7 @@
                 tag = node.tag
             return (_base.ELEMENT, namespace, self.filter.fromXmlName(tag), 
                     [(self.filter.fromXmlName(name), value) for 
-                     name,value in node.attrib.iteritems()], 
+                     name,value in node.attrib.items()], 
                      len(node) > 0 or node.text)
 
     def getFirstChild(self, node):
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/pulldom.py	(original)RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/pulldom.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/pulldom.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/simpletree.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/simpletree.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/simpletree.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/html5lib/treewalkers/soup.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/html5lib/treewalkers/soup.py to planet.archlinux.org3/planet/vendor/html5lib/treewalkers/soup.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/httplib2/__init__.py

+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/pulldom.py	(refactored)
@@ -1,7 +1,7 @@
 from xml.dom.pulldom import START_ELEMENT, END_ELEMENT, \
     COMMENT, IGNORABLE_WHITESPACE, CHARACTERS
 
-import _base
+from . import _base
 
 from html5lib.constants import voidElements
 
@@ -33,11 +33,11 @@
             if name in voidElements:
                 for token in self.emptyTag(namespace,
                                            name,
-                                           node.attributes.items(), 
+                                           list(node.attributes.items()), 
                                            not next or next[1] is not node):
                     yield token
             else:
-                yield self.startTag(namespace, name, node.attributes.items())
+                yield self.startTag(namespace, name, list(node.attributes.items()))
 
         elif type == END_ELEMENT:
             name = node.nodeName
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/simpletree.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/simpletree.py	(refactored)
@@ -1,7 +1,7 @@
 import gettext
 _ = gettext.gettext
 
-import _base
+from . import _base
 
 class TreeWalker(_base.NonRecursiveTreeWalker):
     """Given that simpletree has no performant way of getting a node's
@@ -33,7 +33,7 @@
 
         elif node.type == 5: # Element
             return (_base.ELEMENT, node.namespace, node.name, 
-                    node.attributes.items(), node.hasContent())
+                    list(node.attributes.items()), node.hasContent())
 
         elif node.type == 6: # CommentNode
             return _base.COMMENT, node.data
--- planet.archlinux.org/planet/vendor/html5lib/treewalkers/soup.py	(original)
+++ planet.archlinux.org/planet/vendor/html5lib/treewalkers/soup.py	(refactored)
@@ -4,7 +4,7 @@
 
 from BeautifulSoup import BeautifulSoup, Declaration, Comment, Tag
 from html5lib.constants import namespaces
-import _base
+from . import _base
 
 class TreeWalker(_base.NonRecursiveTreeWalker):
     doctype_regexp = re.compile(
@@ -14,7 +14,7 @@
             return (_base.DOCUMENT,)
 
         elif isinstance(node, Declaration): # DocumentType
-            string = unicode(node.string)
+            string = str(node.string)
             #Slice needed to remove markup added during unicode conversion,
             #but only in some versions of BeautifulSoup/Python
             if string.startswith('<!') and string.endswith('>'):
@@ -36,17 +36,17 @@
             return _base.DOCTYPE, name, publicId or "", systemId or ""
 
         elif isinstance(node, Comment):
-            string = unicode(node.string)
+            string = str(node.string)
             if string.startswith('<!--') and string.endswith('-->'):
                 string = string[4:-3]
             return _base.COMMENT, string
 
-        elif isinstance(node, unicode): # TextNode
+        elif isinstance(node, str): # TextNode
             return _base.TEXT, node
 
         elif isinstance(node, Tag): # Element
             return (_base.ELEMENT, namespaces["html"], node.name,
-                    dict(node.attrs).items(), node.contents)
+                    list(dict(node.attrs).items()), node.contents)
         else:
             return _base.UNKNOWN, node.__class__.__name__
 
--- planet.archlinux.org/planet/vendor/httplib2/__init__.py	(original)
+++ planet.archlinux.org/planet/vendor/httplib2/__init__.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import generators
+
 """
 httplib2
 
@@ -30,11 +30,11 @@
 import email.Utils
 import email.Message
 import email.FeedParser
-import StringIO
+import io
 import gzip
 import zlib
-import httplib
-import urlparse
+import http.client
+import urllib.parse
 import base64
 import os
 import copy
@@ -65,11 +65,11 @@
 except ImportError:
     def _ssl_wrap_socket(sock, key_file, cert_file):
         ssl_sock = socket.ssl(sock, key_file, cert_file)
-        return httplib.FakeSocket(sock, ssl_sock)
+        return http.client.FakeSocket(sock, ssl_sock)
 
 
 if sys.version_info >= (2,3):
-    from iri2uri import iri2uri
+    from .iri2uri import iri2uri
 else:
     def iri2uri(uri):
         return uri
@@ -99,11 +99,11 @@
 def HTTPResponse__getheaders(self):
     """Return list of (header, value) tuples."""
     if self.msg is None:
-        raise httplib.ResponseNotReady()
-    return self.msg.items()
-
-if not hasattr(httplib.HTTPResponse, 'getheaders'):
-    httplib.HTTPResponse.getheaders = HTTPResponse__getheaders
+        raise http.client.ResponseNotReady()
+    return list(self.msg.items())
+
+if not hasattr(http.client.HTTPResponse, 'getheaders'):
+    http.client.HTTPResponse.getheaders = HTTPResponse__getheaders
 
 # All exceptions raised here derive from HttpLib2Error
 class HttpLib2Error(Exception): pass
@@ -153,7 +153,7 @@
 def _get_end2end_headers(response):
     hopbyhop = list(HOP_BY_HOP)
     hopbyhop.extend([x.strip() for x in response.get('connection', '').split(',')])
-    return [header for header in response.keys() if header not in hopbyhop]
+    return [header for header in list(response.keys()) if header not in hopbyhop]
 
 URI = re.compile(r"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\?([^#]*))?(#(.*))?")
 
@@ -201,7 +201,7 @@
                 filename = filename.encode('idna')
     except UnicodeError:
         pass
-    if isinstance(filename,unicode):
+    if isinstance(filename,str):
         filename=filename.encode('utf-8')
     filemd5 = _md5(filename).hexdigest()
     filename = re_url_scheme.sub("", filename)
@@ -214,11 +214,11 @@
 
 NORMALIZE_SPACE = re.compile(r'(?:\r\n)?[ \t]+')
 def _normalize_headers(headers):
-    return dict([ (key.lower(), NORMALIZE_SPACE.sub(value, ' ').strip())  for (key, value) in headers.iteritems()])
+    return dict([ (key.lower(), NORMALIZE_SPACE.sub(value, ' ').strip())  for (key, value) in headers.items()])
 
 def _parse_cache_control(headers):
     retval = {}
-    if headers.has_key('cache-control'):
+    if 'cache-control' in headers:
         parts =  headers['cache-control'].split(',')
         parts_with_args = [tuple([x.strip().lower() for x in part.split("=", 1)]) for part in parts if -1 != part.find("=")]
         parts_wo_args = [(name.strip().lower(), 1) for name in parts if -1 == name.find("=")]
@@ -243,7 +243,7 @@
     """Returns a dictionary of dictionaries, one dict
     per auth_scheme."""
     retval = {}
-    if headers.has_key(headername):
+    if headername in headers:
         authenticate = headers[headername].strip()
         www_auth = USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED
         while authenticate:
@@ -298,26 +298,26 @@
     cc = _parse_cache_control(request_headers)
     cc_response = _parse_cache_control(response_headers)
 
-    if request_headers.has_key('pragma') and request_headers['pragma'].lower().find('no-cache') != -1:
+    if 'pragma' in request_headers and request_headers['pragma'].lower().find('no-cache') != -1:
         retval = "TRANSPARENT"
         if 'cache-control' not in request_headers:
             request_headers['cache-control'] = 'no-cache'
-    elif cc.has_key('no-cache'):
+    elif 'no-cache' in cc:
         retval = "TRANSPARENT"
-    elif cc_response.has_key('no-cache'):
+    elif 'no-cache' in cc_response:
         retval = "STALE"
-    elif cc.has_key('only-if-cached'):
+    elif 'only-if-cached' in cc:
         retval = "FRESH"
-    elif response_headers.has_key('date'):
+    elif 'date' in response_headers:
         date = calendar.timegm(email.Utils.parsedate_tz(response_headers['date']))
         now = time.time()
         current_age = max(0, now - date)
-        if cc_response.has_key('max-age'):
+        if 'max-age' in cc_response:
             try:
                 freshness_lifetime = int(cc_response['max-age'])
             except ValueError:
                 freshness_lifetime = 0
-        elif response_headers.has_key('expires'):
+        elif 'expires' in response_headers:
             expires = email.Utils.parsedate_tz(response_headers['expires'])
             if None == expires:
                 freshness_lifetime = 0
@@ -325,12 +325,12 @@
                 freshness_lifetime = max(0, calendar.timegm(expires) - date)
         else:
             freshness_lifetime = 0
-        if cc.has_key('max-age'):
+        if 'max-age' in cc:
             try:
                 freshness_lifetime = int(cc['max-age'])
             except ValueError:
                 freshness_lifetime = 0
-        if cc.has_key('min-fresh'):
+        if 'min-fresh' in cc:
             try:
                 min_fresh = int(cc['min-fresh'])
             except ValueError:
@@ -346,14 +346,14 @@
         encoding = response.get('content-encoding', None)
         if encoding in ['gzip', 'deflate']:
             if encoding == 'gzip':
-                content = gzip.GzipFile(fileobj=StringIO.StringIO(new_content)).read()
+                content = gzip.GzipFile(fileobj=io.StringIO(new_content)).read()
             if encoding == 'deflate':
                 content = zlib.decompress(content)
             response['content-length'] = str(len(content))
             # Record the historical presence of the encoding in a way the won't interfere.
             response['-content-encoding'] = response['content-encoding']
             del response['content-encoding']
-    except (IOError, zlib.error), e:
+    except (IOError, zlib.error) as e:
         content = ""
         raise FailedToDecompressContent(_("Content purported to be compressed with %s but failed to decompress.") % response.get('content-encoding'), response, content)
     return content
@@ -362,11 +362,11 @@
     if cachekey:
         cc = _parse_cache_control(request_headers)
         cc_response = _parse_cache_control(response_headers)
-        if cc.has_key('no-store') or cc_response.has_key('no-store'):
+        if 'no-store' in cc or 'no-store' in cc_response:
             cache.delete(cachekey)
         else:
             info = email.Message.Message()
-            for key, value in response_headers.iteritems():
+            for key, value in response_headers.items():
                 if key not in ['status','content-encoding','transfer-encoding']:
                     info[key] = value
 
@@ -485,7 +485,7 @@
         self.challenge['nc'] += 1
 
     def response(self, response, content):
-        if not response.has_key('authentication-info'):
+        if 'authentication-info' not in response:
             challenge = _parse_www_authenticate(response, 'www-authenticate').get('digest', {})
             if 'true' == challenge.get('stale'):
                 self.challenge['nonce'] = challenge['nonce']
@@ -494,7 +494,7 @@
         else:
             updated_challenge = _parse_www_authenticate(response, 'authentication-info').get('digest', {})
 
-            if updated_challenge.has_key('nextnonce'):
+            if 'nextnonce' in updated_challenge:
                 self.challenge['nonce'] = updated_challenge['nextnonce']
                 self.challenge['nc'] = 1 
         return False
@@ -588,7 +588,7 @@
 
 class GoogleLoginAuthentication(Authentication):
     def __init__(self, credentials, host, request_uri, headers, response, content, http):
-        from urllib import urlencode
+        from urllib.parse import urlencode
         Authentication.__init__(self, credentials, host, request_uri, headers, response, content, http)
         challenge = _parse_www_authenticate(response, 'www-authenticate')
         service = challenge['googlelogin'].get('service', 'xapi')
@@ -697,11 +697,11 @@
     return socks and (self.proxy_host != None) and (self.proxy_port != None)
 
 
-class HTTPConnectionWithTimeout(httplib.HTTPConnection):
+class HTTPConnectionWithTimeout(http.client.HTTPConnection):
     """HTTPConnection subclass that supports timeouts"""
 
     def __init__(self, host, port=None, strict=None, timeout=None, proxy_info=None):
-        httplib.HTTPConnection.__init__(self, host, port, strict)
+        http.client.HTTPConnection.__init__(self, host, port, strict)
         self.timeout = timeout
         self.proxy_info = proxy_info
 
@@ -723,26 +723,26 @@
                     self.sock.settimeout(self.timeout)
                     # End of difference from httplib.
                 if self.debuglevel > 0:
-                    print "connect: (%s, %s)" % (self.host, self.port)
+                    print("connect: (%s, %s)" % (self.host, self.port))
 
                 self.sock.connect(sa)
-            except socket.error, msg:
+            except socket.error as msg:
                 if self.debuglevel > 0:
-                    print 'connect fail:', (self.host, self.port)
+                    print('connect fail:', (self.host, self.port))
                 if self.sock:
                     self.sock.close()
                 self.sock = None
                 continue
             break
         if not self.sock:
-            raise socket.error, msg
-
-class HTTPSConnectionWithTimeout(httplib.HTTPSConnection):
+            raise socket.error(msg)
+
+class HTTPSConnectionWithTimeout(http.client.HTTPSConnection):
     "This class allows communication via SSL."
 
     def __init__(self, host, port=None, key_file=None, cert_file=None,
                  strict=None, timeout=None, proxy_info=None):
-        httplib.HTTPSConnection.__init__(self, host, port=port, key_file=key_file,
+        http.client.HTTPSConnection.__init__(self, host, port=port, key_file=key_file,
                 cert_file=cert_file, strict=strict)
         self.timeout = timeout
         self.proxy_info = proxy_info
@@ -825,7 +825,7 @@
         challenges = _parse_www_authenticate(response, 'www-authenticate')
         for cred in self.credentials.iter(host):
             for scheme in AUTH_SCHEME_ORDER:
-                if challenges.has_key(scheme):
+                if scheme in challenges:
                     yield AUTH_SCHEME_CLASSES[scheme](cred, host, request_uri, headers, response, content, self)
 
     def add_credentials(self, name, password, domain=""):
@@ -851,13 +851,13 @@
             except socket.gaierror:
                 conn.close()
                 raise ServerNotFoundError("Unable to find the server at %s" % conn.host)
-            except (socket.error, httplib.HTTPException):
+            except (socket.error, http.client.HTTPException):
                 # Just because the server closed the connection doesn't apparently mean
                 # that the server didn't send a response.
                 pass
             try:
                 response = conn.getresponse()
-            except (socket.error, httplib.HTTPException):
+            except (socket.error, http.client.HTTPException):
                 if i == 0:
                     conn.close()
                     conn.connect()
@@ -907,27 +907,27 @@
                 # Pick out the location header and basically start from the beginning
                 # remembering first to strip the ETag header and decrement our 'depth'
                 if redirections:
-                    if not response.has_key('location') and response.status != 300:
+                    if 'location' not in response and response.status != 300:
                         raise RedirectMissingLocation( _("Redirected but the response is missing a Location: header."), response, content)
                     # Fix-up relative redirects (which violate an RFC 2616 MUST)
-                    if response.has_key('location'):
+                    if 'location' in response:
                         location = response['location']
                         (scheme, authority, path, query, fragment) = parse_uri(location)
                         if authority == None:
-                            response['location'] = urlparse.urljoin(absolute_uri, location)
+                            response['location'] = urllib.parse.urljoin(absolute_uri, location)
                     if response.status == 301 and method in ["GET", "HEAD"]:
                         response['-x-permanent-redirect-url'] = response['location']
-                        if not response.has_key('content-location'):
+                        if 'content-location' not in response:
                             response['content-location'] = absolute_uri 
                         _updateCache(headers, response, content, self.cache, cachekey)RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/httplib2/__init__.py to planet.archlinux.org3/planet/vendor/httplib2/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/httplib2/iri2uri.py

-                    if headers.has_key('if-none-match'):
+                    if 'if-none-match' in headers:
                         del headers['if-none-match']
-                    if headers.has_key('if-modified-since'):
+                    if 'if-modified-since' in headers:
                         del headers['if-modified-since']
-                    if response.has_key('location'):
+                    if 'location' in response:
                         location = response['location']
                         old_response = copy.deepcopy(response)
-                        if not old_response.has_key('content-location'):
+                        if 'content-location' not in old_response:
                             old_response['content-location'] = absolute_uri 
                         redirect_method = ((response.status == 303) and (method not in ["GET", "HEAD"])) and "GET" or method
                         (response, content) = self.request(location, redirect_method, body=body, headers = headers, redirections = redirections - 1)
@@ -936,7 +936,7 @@
                     raise RedirectLimit( _("Redirected more times than rediection_limit allows."), response, content)
             elif response.status in [200, 203] and method == "GET":
                 # Don't cache 206's since we aren't going to handle byte range requests
-                if not response.has_key('content-location'):
+                if 'content-location' not in response:
                     response['content-location'] = absolute_uri 
                 _updateCache(headers, response, content, self.cache, cachekey)
 
@@ -975,7 +975,7 @@
             else:
                 headers = _normalize_headers(headers)
 
-            if not headers.has_key('user-agent'):
+            if 'user-agent' not in headers:
                 headers['user-agent'] = "Python-httplib2/%s" % __version__
 
             uri = iri2uri(uri)
@@ -1027,7 +1027,7 @@
             else:
                 cachekey = None
 
-            if method in self.optimistic_concurrency_methods and self.cache and info.has_key('etag') and not self.ignore_etag and 'if-match' not in headers:
+            if method in self.optimistic_concurrency_methods and self.cache and 'etag' in info and not self.ignore_etag and 'if-match' not in headers:
                 # http://www.w3.org/1999/04/Editing/
                 headers['if-match'] = info['etag']
 
@@ -1036,7 +1036,7 @@
                 self.cache.delete(cachekey)
 
             if cached_value and method in ["GET", "HEAD"] and self.cache and 'range' not in headers:
-                if info.has_key('-x-permanent-redirect-url'):
+                if '-x-permanent-redirect-url' in info:
                     # Should cached permanent redirects be counted in our redirection count? For now, yes.
                     (response, new_content) = self.request(info['-x-permanent-redirect-url'], "GET", headers = headers, redirections = redirections - 1)
                     response.previous = Response(info)
@@ -1062,9 +1062,9 @@
                         return (response, content)
 
                     if entry_disposition == "STALE":
-                        if info.has_key('etag') and not self.ignore_etag and not 'if-none-match' in headers:
+                        if 'etag' in info and not self.ignore_etag and not 'if-none-match' in headers:
                             headers['if-none-match'] = info['etag']
-                        if info.has_key('last-modified') and not 'last-modified' in headers:
+                        if 'last-modified' in info and not 'last-modified' in headers:
                             headers['if-modified-since'] = info['last-modified']
                     elif entry_disposition == "TRANSPARENT":
                         pass
@@ -1094,13 +1094,13 @@
                     content = new_content 
             else: 
                 cc = _parse_cache_control(headers)
-                if cc.has_key('only-if-cached'):
+                if 'only-if-cached' in cc:
                     info['status'] = '504'
                     response = Response(info)
                     content = ""
                 else:
                     (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey)
-        except Exception, e:
+        except Exception as e:
             if self.force_exception_to_status_code:
                 if isinstance(e, HttpLib2ErrorWithResponse):
                     response = e.response
@@ -1151,7 +1151,7 @@
     def __init__(self, info):
         # info is either an email.Message or 
         # an httplib.HTTPResponse object.
-        if isinstance(info, httplib.HTTPResponse):
+        if isinstance(info, http.client.HTTPResponse):
             for key, value in info.getheaders(): 
                 self[key.lower()] = value 
             self.status = info.status
@@ -1159,11 +1159,11 @@
             self.reason = info.reason
             self.version = info.version
         elif isinstance(info, email.Message.Message):
-            for key, value in info.items(): 
+            for key, value in list(info.items()): 
                 self[key] = value 
             self.status = int(self['status'])
         else:
-            for key, value in info.iteritems(): 
+            for key, value in info.items(): 
                 self[key] = value 
             self.status = int(self.get('status', self.status))
 
@@ -1172,4 +1172,4 @@
         if name == 'dict':
             return self 
         else:  
-            raise AttributeError, name 
+            raise AttributeError(name) 
--- planet.archlinux.org/planet/vendor/httplib2/iri2uri.py	(original)
+++ planet.archlinux.org/planet/vendor/httplib2/iri2uri.py	(refactored)
@@ -12,7 +12,7 @@
 __history__ = """
 """
 
-import urlparse
+import urllib.parse
 
 
 # Convert an IRI to a URI following the rules in RFC 3987
@@ -66,13 +66,13 @@
     """Convert an IRI to a URI. Note that IRIs must be 
     passed in a unicode strings. That is, do not utf-8 encode
     the IRI before passing it into the function.""" 
-    if isinstance(uri ,unicode):
-        (scheme, authority, path, query, fragment) = urlparse.urlsplit(uri)
+    if isinstance(uri ,str):
+        (scheme, authority, path, query, fragment) = urllib.parse.urlsplit(uri)
         authority = authority.encode('idna')
         # For each character in 'ucschar' or 'iprivate'
         #  1. encode as utf-8
         #  2. then %-encode each octet of that utf-8 
-        uri = urlparse.urlunsplit((scheme, authority, path, query, fragment))
+        uri = urllib.parse.urlunsplit((scheme, authority, path, query, fragment))
         uri = "".join([encode(c) for c in uri])
     return uri
         
@@ -84,26 +84,26 @@
         def test_uris(self):
             """Test that URIs are invariant under the transformation."""
             invariant = [ 
-                u"ftp://ftp.is.co.za/rfc/rfc1808.txt",
-                u"http://www.ietf.org/rfc/rfc2396.txt",
-                u"ldap://[2001:db8::7]/c=GB?objectClass?one",
-                u"mailto:John.Doe@example.com",
-                u"news:comp.infosystems.www.servers.unix",
-                u"tel:+1-816-555-1212",
-                u"telnet://192.0.2.16:80/",
-                u"urn:oasis:names:specification:docbook:dtd:xml:4.1.2" ]
+                "ftp://ftp.is.co.za/rfc/rfc1808.txt",
+                "http://www.ietf.org/rfc/rfc2396.txt",
+                "ldap://[2001:db8::7]/c=GB?objectClass?one",
+                "mailto:John.Doe@example.com",
+                "news:comp.infosystems.www.servers.unix",
+                "tel:+1-816-555-1212",
+                "telnet://192.0.2.16:80/",
+                "urn:oasis:names:specification:docbook:dtd:xml:4.1.2" ]
             for uri in invariant:
                 self.assertEqual(uri, iri2uri(uri))
             
         def test_iri(self):
             """ Test that the right type of escaping is done for each part of the URI."""
-            self.assertEqual("http://xn--o3h.com/%E2%98%84", iri2uri(u"http://\N{COMET}.com/\N{COMET}"))
-            self.assertEqual("http://bitworking.org/?fred=%E2%98%84", iri2uri(u"http://bitworking.org/?fred=\N{COMET}"))RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/httplib2/iri2uri.py to planet.archlinux.org3/planet/vendor/httplib2/iri2uri.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/__init__.py to planet.archlinux.org3/planet/vendor/pubsubhubbub_publisher/__init__.py.
RefactoringTool: Refactored planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/pubsubhubbub_publish.py
RefactoringTool: Writing converted planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/pubsubhubbub_publish.py to planet.archlinux.org3/planet/vendor/pubsubhubbub_publisher/pubsubhubbub_publish.py.
RefactoringTool: No changes to planet.archlinux.org/tests/__init__.py
RefactoringTool: Writing converted planet.archlinux.org/tests/__init__.py to planet.archlinux.org3/tests/__init__.py.
RefactoringTool: No changes to planet.archlinux.org/tests/capture.py
RefactoringTool: Writing converted planet.archlinux.org/tests/capture.py to planet.archlinux.org3/tests/capture.py.
RefactoringTool: Refactored planet.archlinux.org/tests/reconstitute.py
RefactoringTool: Writing converted planet.archlinux.org/tests/reconstitute.py to planet.archlinux.org3/tests/reconstitute.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_apply.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_apply.py to planet.archlinux.org3/tests/test_apply.py.
RefactoringTool: No changes to planet.archlinux.org/tests/test_config.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_config.py to planet.archlinux.org3/tests/test_config.py.
RefactoringTool: No changes to planet.archlinux.org/tests/test_config_csv.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_config_csv.py to planet.archlinux.org3/tests/test_config_csv.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_docs.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_docs.py to planet.archlinux.org3/tests/test_docs.py.
RefactoringTool: No changes to planet.archlinux.org/tests/test_expunge.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_expunge.py to planet.archlinux.org3/tests/test_expunge.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_filter_django.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_filter_django.py to planet.archlinux.org3/tests/test_filter_django.py.
RefactoringTool: No changes to planet.archlinux.org/tests/test_filter_genshi.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_filter_genshi.py to planet.archlinux.org3/tests/test_filter_genshi.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_filter_tmpl.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_filter_tmpl.py to planet.archlinux.org3/tests/test_filter_tmpl.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_filter_xslt.py

-            self.assertEqual("http://bitworking.org/#%E2%98%84", iri2uri(u"http://bitworking.org/#\N{COMET}"))
-            self.assertEqual("#%E2%98%84", iri2uri(u"#\N{COMET}"))
-            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}"))
-            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}")))
-            self.assertNotEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}".encode('utf-8')))
+            self.assertEqual("http://xn--o3h.com/%E2%98%84", iri2uri("http://\N{COMET}.com/\N{COMET}"))
+            self.assertEqual("http://bitworking.org/?fred=%E2%98%84", iri2uri("http://bitworking.org/?fred=\N{COMET}"))
+            self.assertEqual("http://bitworking.org/#%E2%98%84", iri2uri("http://bitworking.org/#\N{COMET}"))
+            self.assertEqual("#%E2%98%84", iri2uri("#\N{COMET}"))
+            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri("/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}"))
+            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(iri2uri("/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}")))
+            self.assertNotEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri("/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}".encode('utf-8')))
 
     unittest.main()
 
--- planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/__init__.py	(original)
+++ planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/__init__.py	(refactored)
@@ -1,2 +1,2 @@
-from pubsubhubbub_publish import *
+from .pubsubhubbub_publish import *
 
--- planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/pubsubhubbub_publish.py	(original)
+++ planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/pubsubhubbub_publish.py	(refactored)
@@ -34,8 +34,8 @@
 
 __author__ = 'bslatkin@gmail.com (Brett Slatkin)'
 
-import urllib
-import urllib2
+import urllib.request, urllib.parse, urllib.error
+import urllib.request, urllib.error, urllib.parse
 
 
 class PublishError(Exception):
@@ -59,16 +59,16 @@
   Raises:
     PublishError if anything went wrong during publishing.
   """
-  if len(urls) == 1 and not isinstance(urls[0], basestring):
+  if len(urls) == 1 and not isinstance(urls[0], str):
     urls = list(urls[0])
 
-  for i in xrange(0, len(urls), URL_BATCH_SIZE):
+  for i in range(0, len(urls), URL_BATCH_SIZE):
     chunk = urls[i:i+URL_BATCH_SIZE]
-    data = urllib.urlencode(
+    data = urllib.parse.urlencode(
         {'hub.url': chunk, 'hub.mode': 'publish'}, doseq=True)
     try:
-      response = urllib2.urlopen(hub, data)
-    except (IOError, urllib2.HTTPError), e:
+      response = urllib.request.urlopen(hub, data)
+    except (IOError, urllib.error.HTTPError) as e:
       if hasattr(e, 'code') and e.code == 204:
         continue
       error = ''
--- planet.archlinux.org/tests/reconstitute.py	(original)
+++ planet.archlinux.org/tests/reconstitute.py	(refactored)
@@ -1,5 +1,6 @@
 #!/usr/bin/env python
-import os, sys, ConfigParser, shutil, glob
+import os, sys, configparser, shutil, glob
+from functools import reduce
 venus_base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
 sys.path.insert(0,venus_base)
 
@@ -18,7 +19,7 @@
         else:
             break
 
-    parser = ConfigParser.ConfigParser()
+    parser = configparser.ConfigParser()
     parser.add_section('Planet')
     parser.add_section(sys.argv[1])
     work = reduce(os.path.join, ['tests','work','reconsititute'], venus_base)
@@ -44,14 +45,14 @@
     import feedparser
     for source in glob.glob(os.path.join(work, 'sources/*')):
         feed = feedparser.parse(source).feed
-        if feed.has_key('title'):
+        if 'title' in feed:
             config.parser.set('Planet','name',feed.title_detail.value)
-        if feed.has_key('link'):
+        if 'link' in feed:
             config.parser.set('Planet','link',feed.link)
-        if feed.has_key('author_detail'):
-            if feed.author_detail.has_key('name'):
+        if 'author_detail' in feed:
+            if 'name' in feed.author_detail:
                 config.parser.set('Planet','owner_name',feed.author_detail.name)
-            if feed.author_detail.has_key('email'):
+            if 'email' in feed.author_detail:
                 config.parser.set('Planet','owner_email',feed.author_detail.email)
 
     from planet import splice
@@ -85,4 +86,4 @@
     shutil.rmtree(work)
     os.removedirs(os.path.dirname(work))
 
-    print atom
+    print(atom)
--- planet.archlinux.org/tests/test_apply.py	(original)
+++ planet.archlinux.org/tests/test_apply.py	(refactored)
@@ -103,7 +103,7 @@
         html = open(os.path.join(workdir, 'index.html')).read()
         self.assertTrue(html.find(' href="http://example.com/default.css"')>=0)
 
-import test_filter_genshi
+from . import test_filter_genshi
 for method in dir(test_filter_genshi.GenshiFilterTests):
     if method.startswith('test_'): break
 else:
@@ -125,8 +125,8 @@
         except IOError:
             exitcode = -1
     except:
-        import commands
-        (exitstatus,output) = commands.getstatusoutput('xsltproc -V')
+        import subprocess
+        (exitstatus,output) = subprocess.getstatusoutput('xsltproc -V')
         exitcode = ((exitstatus>>8) & 0xFF)
 
     if exitcode:
--- planet.archlinux.org/tests/test_docs.py	(original)
+++ planet.archlinux.org/tests/test_docs.py	(refactored)
@@ -3,7 +3,7 @@
 import unittest, os, re
 from xml.dom import minidom
 from glob import glob
-from htmlentitydefs import name2codepoint as n2cp
+from html.entities import name2codepoint as n2cp
 
 class DocsTest(unittest.TestCase):
 
--- planet.archlinux.org/tests/test_filter_django.py	(original)
+++ planet.archlinux.org/tests/test_filter_django.py	(refactored)
@@ -25,7 +25,7 @@
         results = dj.run(
             os.path.realpath('tests/data/filter/django/title.html.dj'), input)
         self.assertEqual(results, 
-          u"\xa1Atom-Powered &lt;b&gt;Robots&lt;/b&gt; Run Amok!\n")
+          "\xa1Atom-Powered &lt;b&gt;Robots&lt;/b&gt; Run Amok!\n")
 
     def test_django_entry_title_autoescape_off(self):
         config.load('tests/data/filter/django/test.ini')
@@ -34,7 +34,7 @@
         input = feed.read(); feed.close()
         results = dj.run(
             os.path.realpath('tests/data/filter/django/title.html.dj'), input)
-        self.assertEqual(results, u"\xa1Atom-Powered <b>Robots</b> Run Amok!\n")
+        self.assertEqual(results, "\xa1Atom-Powered <b>Robots</b> Run Amok!\n")
 
     def test_django_config_context(self):
         config.load('tests/data/filter/django/test.ini')
--- planet.archlinux.org/tests/test_filter_tmpl.py	(original)
+++ planet.archlinux.org/tests/test_filter_tmpl.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 
-import unittest, os, sys, glob, new, re, StringIO, time
+import unittest, os, sys, glob, new, re, io, time
 from planet import config
 from planet.shell import tmpl
 
@@ -19,7 +19,7 @@
             description, expect = self.desc_feed_re.search(data).groups()
             testcase.close()
         except:
-            raise RuntimeError, "can't parse %s" % name
+            raise RuntimeError("can't parse %s" % name)
 
         # map to template info
         results = tmpl.template_info(data)
@@ -39,7 +39,7 @@
             description, expect = self.desc_config_re.search(data).groups()
             testcase.close()
         except:
-            raise RuntimeError, "can't parse %s" % name
+            raise RuntimeError("can't parse %s" % name)
 
         # map to template info
         config.load(testfiles % (name,'ini'))
--- planet.archlinux.org/tests/test_filter_xslt.py	(original)
+++ planet.archlinux.org/tests/test_filter_xslt.py	(refactored)
@@ -30,8 +30,8 @@
     try:
         try:
             # Python 2.5 bug 1704790 workaround (alas, Unix only)
-            import commands
-            if commands.getstatusoutput('xsltproc --version')[0] != 0:
+            import subprocess
+            if subprocess.getstatusoutput('xsltproc --version')[0] != 0:
                 raise ImportErrorRefactoringTool: Writing converted planet.archlinux.org/tests/test_filter_xslt.py to planet.archlinux.org3/tests/test_filter_xslt.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_filters.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_filters.py to planet.archlinux.org3/tests/test_filters.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_foaf.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_foaf.py to planet.archlinux.org3/tests/test_foaf.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_idindex.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_idindex.py to planet.archlinux.org3/tests/test_idindex.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_opml.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_opml.py to planet.archlinux.org3/tests/test_opml.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_reconstitute.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_reconstitute.py to planet.archlinux.org3/tests/test_reconstitute.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_rlists.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_rlists.py to planet.archlinux.org3/tests/test_rlists.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_scrub.py

         except:
             from subprocess import Popen, PIPE
--- planet.archlinux.org/tests/test_filters.py	(original)
+++ planet.archlinux.org/tests/test_filters.py	(refactored)
@@ -50,9 +50,9 @@
 
         dom = xml.dom.minidom.parseString(output)
         excerpt = dom.getElementsByTagName('planet:excerpt')[0]
-        self.assertEqual(u'Lorem ipsum dolor sit amet, consectetuer ' +
-            u'adipiscing elit. Nullam velit. Vivamus tincidunt, erat ' +
-            u'in \u2026', excerpt.firstChild.firstChild.nodeValue)
+        self.assertEqual('Lorem ipsum dolor sit amet, consectetuer ' +
+            'adipiscing elit. Nullam velit. Vivamus tincidunt, erat ' +
+            'in \u2026', excerpt.firstChild.firstChild.nodeValue)
 
     def test_excerpt_lorem_ipsum_summary(self):
         testfile = 'tests/data/filter/excerpt-lorem-ipsum.xml'
@@ -65,9 +65,9 @@
 
         dom = xml.dom.minidom.parseString(output)
         excerpt = dom.getElementsByTagName('summary')[0]
-        self.assertEqual(u'Lorem ipsum dolor sit amet, consectetuer ' +
-            u'adipiscing elit. Nullam velit. Vivamus tincidunt, erat ' +
-            u'in \u2026', excerpt.firstChild.firstChild.nodeValue)
+        self.assertEqual('Lorem ipsum dolor sit amet, consectetuer ' +
+            'adipiscing elit. Nullam velit. Vivamus tincidunt, erat ' +
+            'in \u2026', excerpt.firstChild.firstChild.nodeValue)
 
     def test_stripAd_yahoo(self):
         testfile = 'tests/data/filter/stripAd-yahoo.xml'
@@ -79,7 +79,7 @@
 
         dom = xml.dom.minidom.parseString(output)
         excerpt = dom.getElementsByTagName('content')[0]
-        self.assertEqual(u'before--after',
+        self.assertEqual('before--after',
             excerpt.firstChild.firstChild.nodeValue)
 
     def test_xpath_filter1(self):
@@ -159,8 +159,8 @@
     if _no_sed:
         try:
             # Python 2.5 bug 1704790 workaround (alas, Unix only)
-            import commands
-            if commands.getstatusoutput('sed --version')[0]==0: _no_sed = False 
+            import subprocess
+            if subprocess.getstatusoutput('sed --version')[0]==0: _no_sed = False 
         except:
             pass
 
--- planet.archlinux.org/tests/test_foaf.py	(original)
+++ planet.archlinux.org/tests/test_foaf.py	(refactored)
@@ -2,7 +2,7 @@
 
 import unittest, os, shutil
 from planet.foaf import foaf2config
-from ConfigParser import ConfigParser
+from configparser import ConfigParser
 from planet import config, logger
 
 workdir = 'tests/work/config/cache'
@@ -120,7 +120,7 @@
     import RDF
 except:
     logger.warn("Redland RDF is not available => can't test FOAF reading lists")
-    for key in FoafTest.__dict__.keys():
+    for key in list(FoafTest.__dict__.keys()):
         if key.startswith('test_'): delattr(FoafTest, key)
 
 if __name__ == '__main__':
--- planet.archlinux.org/tests/test_idindex.py	(original)
+++ planet.archlinux.org/tests/test_idindex.py	(refactored)
@@ -20,11 +20,11 @@
         iri = 'http://www.\xe8\xa9\xb9\xe5\xa7\x86\xe6\x96\xaf.com/'
         index[filename('', iri)] = 'data'
         index[filename('', iri.decode('utf-8'))] = 'data'
-        index[filename('', u'1234')] = 'data'
+        index[filename('', '1234')] = 'data'
         index.close()
         
     def test_index_spider(self):
-        import test_spider
+        from . import test_spider
         config.load(test_spider.configfile)
 
         index = idindex.create()
@@ -46,7 +46,7 @@
             os.removedirs(os.path.split(test_spider.workdir)[0])
 
     def test_index_splice(self):
-        import test_splice
+        from . import test_splice
         config.load(test_splice.configfile)
         index = idindex.create()
 
@@ -54,7 +54,7 @@
         self.assertEqual('tag:planet.intertwingly.net,2006:testfeed1', index['planet.intertwingly.net,2006,testfeed1,1'])
         self.assertEqual('http://intertwingly.net/code/venus/tests/data/spider/testfeed3.rss', index['planet.intertwingly.net,2006,testfeed3,1'])
 
-        for key in index.keys():
+        for key in list(index.keys()):
             value = index[key]
             if value.find('testfeed2')>0: index[key] = value.swapcase()
         index.close()
--- planet.archlinux.org/tests/test_opml.py	(original)
+++ planet.archlinux.org/tests/test_opml.py	(refactored)
@@ -2,7 +2,7 @@
 
 import unittest
 from planet.opml import opml2config
-from ConfigParser import ConfigParser
+from configparser import ConfigParser
 
 class OpmlTest(unittest.TestCase):
     """
--- planet.archlinux.org/tests/test_reconstitute.py	(original)
+++ planet.archlinux.org/tests/test_reconstitute.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 
-import unittest, os, sys, glob, new, re, StringIO, time
+import unittest, os, sys, glob, new, re, io, time
 from planet import feedparser
 from planet.reconstitute import reconstitute
 from planet.scrub import scrub
@@ -19,10 +19,10 @@
             description, expect = self.desc_re.search(data).groups()
             testcase.close()
         except:
-            raise RuntimeError, "can't parse %s" % name
+            raise RuntimeError("can't parse %s" % name)
 
         # parse and reconstitute to a string
-        work = StringIO.StringIO()
+        work = io.StringIO()
         results = feedparser.parse(data)
         scrub(testfiles%name, results)
         reconstitute(results, results.entries[0]).writexml(work)
--- planet.archlinux.org/tests/test_rlists.py	(original)
+++ planet.archlinux.org/tests/test_rlists.py	(refactored)
@@ -4,7 +4,7 @@
 from planet import config, opml
 from os.path import split
 from glob import glob
-from ConfigParser import ConfigParser
+from configparser import ConfigParser
 
 workdir = os.path.join('tests', 'work', 'config', 'cache')
 
--- planet.archlinux.org/tests/test_scrub.py	(original)
+++ planet.archlinux.org/tests/test_scrub.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 
-import unittest, StringIO, time
+import unittest, io, time
 from copy import deepcopy
 from planet.scrub import scrub
 from planet import feedparser, config
@@ -40,32 +40,32 @@
     def test_scrub_ignore(self):
         base = feedparser.parse(feed)
 
-        self.assertTrue(base.entries[0].has_key('author'))
-        self.assertTrue(base.entries[0].has_key('author_detail'))
-        self.assertTrue(base.entries[0].has_key('id'))
-        self.assertTrue(base.entries[0].has_key('updated'))
-        self.assertTrue(base.entries[0].has_key('updated_parsed'))
-        self.assertTrue(base.entries[0].summary_detail.has_key('language'))
+        self.assertTrue('author' in base.entries[0])
+        self.assertTrue('author_detail' in base.entries[0])
+        self.assertTrue('id' in base.entries[0])
+        self.assertTrue('updated' in base.entries[0])
+        self.assertTrue('updated_parsed' in base.entries[0])
+        self.assertTrue('language' in base.entries[0].summary_detail)
 
-        config.parser.readfp(StringIO.StringIO(configData))
+        config.parser.readfp(io.StringIO(configData))
         config.parser.set('testfeed', 'ignore_in_feed',
           'author id updated xml:lang')
         data = deepcopy(base)
         scrub('testfeed', data)
 
-        self.assertFalse(data.entries[0].has_key('author'))
-        self.assertFalse(data.entries[0].has_key('author_detail'))
-        self.assertFalse(data.entries[0].has_key('id'))
-        self.assertFalse(data.entries[0].has_key('updated'))
-        self.assertFalse(data.entries[0].has_key('updated_parsed'))
-        self.assertFalse(data.entries[0].summary_detail.has_key('language'))
+        self.assertFalse('author' in data.entries[0])
+        self.assertFalse('author_detail' in data.entries[0])
+        self.assertFalse('id' in data.entries[0])
+        self.assertFalse('updated' in data.entries[0])
+        self.assertFalse('updated_parsed' in data.entries[0])
+        self.assertFalse('language' in data.entries[0].summary_detail)
 
     def test_scrub_type(self):
         base = feedparser.parse(feed)
 
         self.assertEqual('F&ouml;o', base.feed.author_detail.name)
 
-        config.parser.readfp(StringIO.StringIO(configData))
+        config.parser.readfp(io.StringIO(configData))RefactoringTool: Writing converted planet.archlinux.org/tests/test_scrub.py to planet.archlinux.org3/tests/test_scrub.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_spider.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_spider.py to planet.archlinux.org3/tests/test_spider.py.
RefactoringTool: No changes to planet.archlinux.org/tests/test_splice.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_splice.py to planet.archlinux.org3/tests/test_splice.py.
RefactoringTool: Refactored planet.archlinux.org/tests/test_subconfig.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_subconfig.py to planet.archlinux.org3/tests/test_subconfig.py.
RefactoringTool: No changes to planet.archlinux.org/tests/test_themes.py
RefactoringTool: Writing converted planet.archlinux.org/tests/test_themes.py to planet.archlinux.org3/tests/test_themes.py.
RefactoringTool: Refactored planet.archlinux.org/tests/data/apply/rebase.py
RefactoringTool: Writing converted planet.archlinux.org/tests/data/apply/rebase.py to planet.archlinux.org3/tests/data/apply/rebase.py.
RefactoringTool: Files that were modified:
RefactoringTool: planet.archlinux.org/admin_cb.py
RefactoringTool: planet.archlinux.org/expunge.py
RefactoringTool: planet.archlinux.org/favicon.py
RefactoringTool: planet.archlinux.org/planet.py
RefactoringTool: planet.archlinux.org/publish.py
RefactoringTool: planet.archlinux.org/runtests.py
RefactoringTool: planet.archlinux.org/spider.py
RefactoringTool: planet.archlinux.org/splice.py
RefactoringTool: planet.archlinux.org/examples/filters/guess-language/guess-language.py
RefactoringTool: planet.archlinux.org/examples/filters/guess-language/learn-language.py
RefactoringTool: planet.archlinux.org/examples/filters/guess-language/trigram.py
RefactoringTool: planet.archlinux.org/filters/coral_cdn_filter.py
RefactoringTool: planet.archlinux.org/filters/excerpt.py
RefactoringTool: planet.archlinux.org/filters/minhead.py
RefactoringTool: planet.archlinux.org/filters/notweets.py
RefactoringTool: planet.archlinux.org/filters/regexp_sifter.py
RefactoringTool: planet.archlinux.org/filters/xpath_sifter.py
RefactoringTool: planet.archlinux.org/planet/__init__.py
RefactoringTool: planet.archlinux.org/planet/config.py
RefactoringTool: planet.archlinux.org/planet/csv_config.py
RefactoringTool: planet.archlinux.org/planet/expunge.py
RefactoringTool: planet.archlinux.org/planet/foaf.py
RefactoringTool: planet.archlinux.org/planet/idindex.py
RefactoringTool: planet.archlinux.org/planet/opml.py
RefactoringTool: planet.archlinux.org/planet/publish.py
RefactoringTool: planet.archlinux.org/planet/reconstitute.py
RefactoringTool: planet.archlinux.org/planet/scrub.py
RefactoringTool: planet.archlinux.org/planet/spider.py
RefactoringTool: planet.archlinux.org/planet/splice.py
RefactoringTool: planet.archlinux.org/planet/shell/__init__.py
RefactoringTool: planet.archlinux.org/planet/shell/_genshi.py
RefactoringTool: planet.archlinux.org/planet/shell/dj.py
RefactoringTool: planet.archlinux.org/planet/shell/plugin.py
RefactoringTool: planet.archlinux.org/planet/shell/py.py
RefactoringTool: planet.archlinux.org/planet/shell/sed.py
RefactoringTool: planet.archlinux.org/planet/shell/tmpl.py
RefactoringTool: planet.archlinux.org/planet/shell/xslt.py
RefactoringTool: planet.archlinux.org/planet/vendor/feedparser.py
RefactoringTool: planet.archlinux.org/planet/vendor/htmltmpl.py
RefactoringTool: planet.archlinux.org/planet/vendor/portalocker.py
RefactoringTool: planet.archlinux.org/planet/vendor/timeoutsocket.py
RefactoringTool: planet.archlinux.org/planet/vendor/compat_logging/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/compat_logging/config.py
RefactoringTool: planet.archlinux.org/planet/vendor/compat_logging/handlers.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/constants.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/html5parser.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/ihatexml.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/inputstream.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/sanitizer.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/tokenizer.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/utils.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/_base.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/formfiller.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/inject_meta_charset.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/lint.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/optionaltags.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/sanitizer.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/filters/whitespace.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/serializer/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/serializer/htmlserializer.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/serializer/xhtmlserializer.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treebuilders/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treebuilders/_base.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treebuilders/dom.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treebuilders/etree_lxml.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treebuilders/simpletree.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treebuilders/soup.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/_base.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/dom.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/etree.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/genshistream.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/lxmletree.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/pulldom.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/simpletree.py
RefactoringTool: planet.archlinux.org/planet/vendor/html5lib/treewalkers/soup.py
RefactoringTool: planet.archlinux.org/planet/vendor/httplib2/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/httplib2/iri2uri.py
RefactoringTool: planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/__init__.py
RefactoringTool: planet.archlinux.org/planet/vendor/pubsubhubbub_publisher/pubsubhubbub_publish.py
RefactoringTool: planet.archlinux.org/tests/__init__.py
RefactoringTool: planet.archlinux.org/tests/capture.py
RefactoringTool: planet.archlinux.org/tests/reconstitute.py
RefactoringTool: planet.archlinux.org/tests/test_apply.py
RefactoringTool: planet.archlinux.org/tests/test_config.py
RefactoringTool: planet.archlinux.org/tests/test_config_csv.py
RefactoringTool: planet.archlinux.org/tests/test_docs.py
RefactoringTool: planet.archlinux.org/tests/test_expunge.py
RefactoringTool: planet.archlinux.org/tests/test_filter_django.py
RefactoringTool: planet.archlinux.org/tests/test_filter_genshi.py
RefactoringTool: planet.archlinux.org/tests/test_filter_tmpl.py
RefactoringTool: planet.archlinux.org/tests/test_filter_xslt.py
RefactoringTool: planet.archlinux.org/tests/test_filters.py
RefactoringTool: planet.archlinux.org/tests/test_foaf.py
RefactoringTool: planet.archlinux.org/tests/test_idindex.py
RefactoringTool: planet.archlinux.org/tests/test_opml.py
RefactoringTool: planet.archlinux.org/tests/test_reconstitute.py
RefactoringTool: planet.archlinux.org/tests/test_rlists.py
RefactoringTool: planet.archlinux.org/tests/test_scrub.py
RefactoringTool: planet.archlinux.org/tests/test_spider.py
RefactoringTool: planet.archlinux.org/tests/test_splice.py
RefactoringTool: planet.archlinux.org/tests/test_subconfig.py
RefactoringTool: planet.archlinux.org/tests/test_themes.py
RefactoringTool: planet.archlinux.org/tests/data/apply/rebase.py
RefactoringTool: Warnings/messages while refactoring:
RefactoringTool: ### In file planet.archlinux.org/planet/config.py ###
RefactoringTool: Line 143: absolute and local imports together
RefactoringTool: ### In file planet.archlinux.org/planet/expunge.py ###
RefactoringTool: Line 2: absolute and local imports together
RefactoringTool: ### In file planet.archlinux.org/planet/reconstitute.py ###
RefactoringTool: Line 21: absolute and local imports together
RefactoringTool: ### In file planet.archlinux.org/planet/scrub.py ###
RefactoringTool: Line 8: absolute and local imports together
RefactoringTool: ### In file planet.archlinux.org/planet/spider.py ###
RefactoringTool: Line 10: absolute and local imports together
RefactoringTool: ### In file planet.archlinux.org/planet/splice.py ###
RefactoringTool: Line 4: absolute and local imports together
RefactoringTool: ### In file planet.archlinux.org/planet/vendor/feedparser.py ###
RefactoringTool: Line 3184: You should use a for loop here
RefactoringTool: ### In file planet.archlinux.org/tests/reconstitute.py ###
RefactoringTool: Line 66: Calls to builtin next() possibly shadowed by global binding

         data = deepcopy(base)
         scrub('testfeed', data)
 
@@ -80,13 +80,13 @@
     def test_scrub_future(self):
         base = feedparser.parse(feed)
         self.assertEqual(1, len(base.entries))
-        self.assertTrue(base.entries[0].has_key('updated'))
+        self.assertTrue('updated' in base.entries[0])
 
-        config.parser.readfp(StringIO.StringIO(configData))
+        config.parser.readfp(io.StringIO(configData))
         config.parser.set('testfeed', 'future_dates', 'ignore_date')
         data = deepcopy(base)
         scrub('testfeed', data)
-        self.assertFalse(data.entries[0].has_key('updated'))
+        self.assertFalse('updated' in data.entries[0])
 
         config.parser.set('testfeed', 'future_dates', 'ignore_entry')
         data = deepcopy(base)
@@ -98,7 +98,7 @@
         self.assertEqual('http://example.com/',
              base.entries[0].title_detail.base)
 
-        config.parser.readfp(StringIO.StringIO(configData))
+        config.parser.readfp(io.StringIO(configData))
         config.parser.set('testfeed', 'xml_base', 'feed_alternate')
         data = deepcopy(base)
         scrub('testfeed', data)
--- planet.archlinux.org/tests/test_spider.py	(original)
+++ planet.archlinux.org/tests/test_spider.py	(refactored)
@@ -31,10 +31,10 @@
             filename('.', 'http://example.com/index.html'))
         self.assertEqual(os.path.join('.',
             'planet.intertwingly.net,2006,testfeed1,1'),
-            filename('.', u'tag:planet.intertwingly.net,2006:testfeed1,1'))
+            filename('.', 'tag:planet.intertwingly.net,2006:testfeed1,1'))
         self.assertEqual(os.path.join('.',
             '00000000-0000-0000-0000-000000000000'),
-            filename('.', u'urn:uuid:00000000-0000-0000-0000-000000000000'))
+            filename('.', 'urn:uuid:00000000-0000-0000-0000-000000000000'))
 
         # Requires Python 2.3
         try:
@@ -42,7 +42,7 @@
         except:
             return
         self.assertEqual(os.path.join('.', 'xn--8ws00zhy3a.com'),
-            filename('.', u'http://www.\u8a79\u59c6\u65af.com/'))
+            filename('.', 'http://www.\u8a79\u59c6\u65af.com/'))
 
     def spiderFeed(self, feed_uri):
         feed_info = feedparser.parse('<feed/>')
@@ -115,7 +115,7 @@
         self.assertEqual(2, len(glob.glob(workdir+"/*")))
         data = feedparser.parse(workdir + 
             '/planet.intertwingly.net,2006,testfeed4')
-        self.assertEqual(u'three', data.entries[0].content[0].value)
+        self.assertEqual('three', data.entries[0].content[0].value)
 
     def verify_spiderPlanet(self):
         files = glob.glob(workdir+"/*")
@@ -146,7 +146,7 @@
         _PORT = config.parser.getint('Planet','test_port')
 
         log = []
-        from SimpleHTTPServer import SimpleHTTPRequestHandler
+        from http.server import SimpleHTTPRequestHandler
         class TestRequestHandler(SimpleHTTPRequestHandler):
             def log_message(self, format, *args):
                 log.append(args)
@@ -158,7 +158,7 @@
               self.done = 0
               Thread.__init__(self)
           def run(self):
-              from BaseHTTPServer import HTTPServer
+              from http.server import HTTPServer
               httpd = HTTPServer(('',_PORT), TestRequestHandler)
               self.ready = 1
               while not self.done:
@@ -173,8 +173,8 @@
             spiderPlanet()
         finally:
             httpd.done = 1
-            import urllib
-            urllib.urlopen('http://127.0.0.1:%d/' % _PORT).read()
+            import urllib.request, urllib.parse, urllib.error
+            urllib.request.urlopen('http://127.0.0.1:%d/' % _PORT).read()
 
         status = [int(rec[1]) for rec in log if str(rec[0]).startswith('GET ')]
         status.sort()
--- planet.archlinux.org/tests/test_subconfig.py	(original)
+++ planet.archlinux.org/tests/test_subconfig.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 
-from test_config_csv import ConfigCsvTest
+from .test_config_csv import ConfigCsvTest
 from planet import config
 
 class SubConfigTest(ConfigCsvTest):
--- planet.archlinux.org/tests/data/apply/rebase.py	(original)
+++ planet.archlinux.org/tests/data/apply/rebase.py	(refactored)
@@ -8,7 +8,7 @@
   sys.exit()
 
 from xml.dom import minidom, Node
-from urlparse import urljoin
+from urllib.parse import urljoin
 
 def rebase(node, newbase):
   if node.hasAttribute('href'):
@@ -21,4 +21,4 @@
 
 doc = minidom.parse(sys.stdin)
 rebase(doc.documentElement, base)
-print doc.toxml('utf-8')
+print(doc.toxml('utf-8'))
